* 
* ==> Audit <==
* |---------|------------------|----------|------------------------|---------|---------------------|---------------------|
| Command |       Args       | Profile  |          User          | Version |     Start Time      |      End Time       |
|---------|------------------|----------|------------------------|---------|---------------------|---------------------|
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 14:29 -05 |                     |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 14:40 -05 |                     |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 14:40 -05 |                     |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 14:47 -05 |                     |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 14:55 -05 |                     |
| start   |                  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 14:55 -05 |                     |
| start   |                  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:05 -05 |                     |
| start   | --no-vtx-check   | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:11 -05 | 10 Oct 23 15:15 -05 |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:17 -05 |                     |
| start   | --no-vtx-check   | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:19 -05 | 10 Oct 23 15:20 -05 |
| service | mongo-service    | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:21 -05 |                     |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:22 -05 |                     |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:24 -05 |                     |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:25 -05 |                     |
| service | mongodb-services | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:26 -05 |                     |
| service | list             | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:37 -05 | 10 Oct 23 15:37 -05 |
| start   | --no-vtx-check   | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:39 -05 |                     |
| start   | --no-vtx-check   | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:45 -05 |                     |
| start   | --no-vtx-check   | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:49 -05 | 10 Oct 23 15:53 -05 |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 15:52 -05 |                     |
| start   | --no-vtx-check   | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 18:30 -05 | 10 Oct 23 18:35 -05 |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 18:37 -05 |                     |
| service | list             | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 18:38 -05 | 10 Oct 23 18:38 -05 |
| start   | --no-vtx-check   | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 18:39 -05 |                     |
| delete  | --all            | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 18:45 -05 |                     |
| start   | --no-vtx-check   | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 18:47 -05 |                     |
| service | list             | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 19:05 -05 |                     |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 19:05 -05 |                     |
| delete  |                  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 19:14 -05 | 10 Oct 23 19:14 -05 |
| start   |                  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 19:15 -05 | 10 Oct 23 19:17 -05 |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 19:32 -05 | 10 Oct 23 19:43 -05 |
| service | mongo-service    | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 19:33 -05 |                     |
| service | list             | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 19:34 -05 | 10 Oct 23 19:34 -05 |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 19:35 -05 | 10 Oct 23 19:44 -05 |
| start   | --no-vtx-check   | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 19:39 -05 | 10 Oct 23 19:41 -05 |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 19:44 -05 | 10 Oct 23 20:19 -05 |
| service | mongo-service    | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 19:44 -05 |                     |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 19:45 -05 | 10 Oct 23 20:48 -05 |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 20:19 -05 | 10 Oct 23 20:27 -05 |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 20:30 -05 | 10 Oct 23 20:48 -05 |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 20:58 -05 | 10 Oct 23 21:58 -05 |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 21:01 -05 | 10 Oct 23 22:00 -05 |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:06 -05 | 10 Oct 23 22:09 -05 |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:12 -05 | 10 Oct 23 22:21 -05 |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:21 -05 |                     |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:24 -05 | 10 Oct 23 22:25 -05 |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:25 -05 | 10 Oct 23 22:26 -05 |
| delete  | all              | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:28 -05 |                     |
| delete  | --all            | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:28 -05 | 10 Oct 23 22:28 -05 |
| start   |                  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:30 -05 | 10 Oct 23 22:33 -05 |
| service | mongo-service    | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:35 -05 |                     |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:35 -05 |                     |
| delete  |                  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:36 -05 | 10 Oct 23 22:36 -05 |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:37 -05 |                     |
| start   |                  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:37 -05 | 10 Oct 23 22:39 -05 |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:40 -05 |                     |
| start   |                  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:47 -05 | 10 Oct 23 22:50 -05 |
| service | mongodb-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:51 -05 |                     |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:53 -05 |                     |
| service | backend-service  | minikube | LAPTOP-BPM7R9UJ\LENOVO | v1.31.2 | 10 Oct 23 22:54 -05 |                     |
|---------|------------------|----------|------------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/10/10 22:47:06
Running on machine: LAPTOP-BPM7R9UJ
Binary: Built with gc go1.20.7 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1010 22:47:06.879808   29740 out.go:296] Setting OutFile to fd 100 ...
I1010 22:47:06.885132   29740 out.go:348] isatty.IsTerminal(100) = true
I1010 22:47:06.885132   29740 out.go:309] Setting ErrFile to fd 104...
I1010 22:47:06.886755   29740 out.go:348] isatty.IsTerminal(104) = true
I1010 22:47:07.449355   29740 out.go:303] Setting JSON to false
I1010 22:47:07.504394   29740 start.go:128] hostinfo: {"hostname":"LAPTOP-BPM7R9UJ","uptime":31047,"bootTime":1696964979,"procs":428,"os":"windows","platform":"Microsoft Windows 11 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.22621.2361 Build 22621.2361","kernelVersion":"10.0.22621.2361 Build 22621.2361","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"59d0adc0-f8b0-4471-966a-f16caff90004"}
W1010 22:47:07.504394   29740 start.go:136] gopshost.Virtualization returned error: not implemented yet
I1010 22:47:07.540704   29740 out.go:177] üòÑ  minikube v1.31.2 on Microsoft Windows 11 Home 10.0.22621.2361 Build 22621.2361
I1010 22:47:07.545590   29740 notify.go:220] Checking for updates...
I1010 22:47:07.649665   29740 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1010 22:47:07.650227   29740 driver.go:373] Setting default libvirt URI to qemu:///system
I1010 22:47:10.128924   29740 docker.go:121] docker version: linux-24.0.6:Docker Desktop 4.24.1 (123237)
I1010 22:47:10.207003   29740 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1010 22:47:30.051152   29740 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (19.844095s)
I1010 22:47:31.446521   29740 info.go:266] docker info: {ID:a11dda2e-aa4d-4b68-a218-310732b17591 Containers:48 ContainersRunning:41 ContainersPaused:0 ContainersStopped:7 Images:29 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:206 OomKillDisable:true NGoroutines:198 SystemTime:2023-10-11 03:47:29.884061808 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:11 KernelVersion:5.10.102.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:6101708800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I1010 22:47:31.633572   29740 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1010 22:47:31.742115   29740 start.go:298] selected driver: docker
I1010 22:47:31.848573   29740 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\LENOVO:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1010 22:47:31.849583   29740 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1010 22:47:31.899454   29740 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1010 22:47:37.092756   29740 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (5.1921796s)
I1010 22:47:37.092756   29740 info.go:266] docker info: {ID:a11dda2e-aa4d-4b68-a218-310732b17591 Containers:45 ContainersRunning:38 ContainersPaused:0 ContainersStopped:7 Images:29 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:205 OomKillDisable:true NGoroutines:199 SystemTime:2023-10-11 03:47:36.847828904 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:5.10.102.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:6101708800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I1010 22:47:37.624512   29740 cni.go:84] Creating CNI manager for ""
I1010 22:47:37.624512   29740 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1010 22:47:37.625680   29740 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\LENOVO:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1010 22:47:37.629011   29740 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I1010 22:47:37.648011   29740 cache.go:122] Beginning downloading kic base image for docker with docker
I1010 22:47:37.648543   29740 out.go:177] üöú  Pulling base image ...
I1010 22:47:37.651269   29740 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I1010 22:47:37.651269   29740 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1010 22:47:37.654869   29740 preload.go:148] Found local preload: C:\Users\LENOVO\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I1010 22:47:37.655180   29740 cache.go:57] Caching tarball of preloaded images
I1010 22:47:37.704571   29740 preload.go:174] Found C:\Users\LENOVO\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1010 22:47:37.704571   29740 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I1010 22:47:37.705624   29740 profile.go:148] Saving config to C:\Users\LENOVO\.minikube\profiles\minikube\config.json ...
I1010 22:47:38.748729   29740 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I1010 22:47:38.748729   29740 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I1010 22:47:38.751507   29740 cache.go:195] Successfully downloaded all kic artifacts
I1010 22:47:38.794771   29740 start.go:365] acquiring machines lock for minikube: {Name:mk1f4826b908553e9bc66fb08644c23b5c53aa71 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1010 22:47:38.795757   29740 start.go:369] acquired machines lock for "minikube" in 0s
I1010 22:47:38.797270   29740 start.go:96] Skipping create...Using existing machine configuration
I1010 22:47:38.798283   29740 fix.go:54] fixHost starting: 
I1010 22:47:38.893155   29740 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1010 22:47:39.563248   29740 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1010 22:47:39.597069   29740 fix.go:128] unexpected machine state, will restart: <nil>
I1010 22:47:39.610976   29740 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I1010 22:47:39.653893   29740 cli_runner.go:164] Run: docker start minikube
W1010 22:47:41.692390   29740 cli_runner.go:211] docker start minikube returned with exit code 1
I1010 22:47:41.692390   29740 cli_runner.go:217] Completed: docker start minikube: (2.0384913s)
I1010 22:47:41.720292   29740 cli_runner.go:164] Run: docker inspect minikube
I1010 22:47:42.729094   29740 cli_runner.go:217] Completed: docker inspect minikube: (1.0087991s)
I1010 22:47:42.760322   29740 errors.go:84] Postmortem inspect ("docker inspect minikube"): -- stdout --
[
    {
        "Id": "078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802",
        "Created": "2023-10-11T03:38:13.24958984Z",
        "Path": "/usr/local/bin/entrypoint",
        "Args": [
            "/sbin/init"
        ],
        "State": {
            "Status": "exited",
            "Running": false,
            "Paused": false,
            "Restarting": false,
            "OOMKilled": false,
            "Dead": false,
            "Pid": 0,
            "ExitCode": 128,
            "Error": "failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write \"a *:* rwm\": write /sys/fs/cgroup/devices/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802/devices.allow: invalid argument: unknown",
            "StartedAt": "2023-10-11T03:38:14.15771216Z",
            "FinishedAt": "2023-10-11T03:46:12.759107016Z"
        },
        "Image": "sha256:c6cc01e6091959400f260dc442708e7c71630b58dab1f7c344cb00926bd84950",
        "ResolvConfPath": "/var/lib/docker/containers/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802/resolv.conf",
        "HostnamePath": "/var/lib/docker/containers/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802/hostname",
        "HostsPath": "/var/lib/docker/containers/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802/hosts",
        "LogPath": "/var/lib/docker/containers/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802-json.log",
        "Name": "/minikube",
        "RestartCount": 0,
        "Driver": "overlay2",
        "Platform": "linux",
        "MountLabel": "",
        "ProcessLabel": "",
        "AppArmorProfile": "",
        "ExecIDs": null,
        "HostConfig": {
            "Binds": [
                "/lib/modules:/lib/modules:ro",
                "minikube:/var"
            ],
            "ContainerIDFile": "",
            "LogConfig": {
                "Type": "json-file",
                "Config": {}
            },
            "NetworkMode": "minikube",
            "PortBindings": {
                "22/tcp": [
                    {
                        "HostIp": "127.0.0.1",
                        "HostPort": "0"
                    }
                ],
                "2376/tcp": [
                    {
                        "HostIp": "127.0.0.1",
                        "HostPort": "0"
                    }
                ],
                "32443/tcp": [
                    {
                        "HostIp": "127.0.0.1",
                        "HostPort": "0"
                    }
                ],
                "5000/tcp": [
                    {
                        "HostIp": "127.0.0.1",
                        "HostPort": "0"
                    }
                ],
                "8443/tcp": [
                    {
                        "HostIp": "127.0.0.1",
                        "HostPort": "0"
                    }
                ]
            },
            "RestartPolicy": {
                "Name": "no",
                "MaximumRetryCount": 0
            },
            "AutoRemove": false,
            "VolumeDriver": "",
            "VolumesFrom": null,
            "ConsoleSize": [
                0,
                0
            ],
            "CapAdd": null,
            "CapDrop": null,
            "CgroupnsMode": "host",
            "Dns": [],
            "DnsOptions": [],
            "DnsSearch": [],
            "ExtraHosts": null,
            "GroupAdd": null,
            "IpcMode": "private",
            "Cgroup": "",
            "Links": null,
            "OomScoreAdj": 0,
            "PidMode": "",
            "Privileged": true,
            "PublishAllPorts": false,
            "ReadonlyRootfs": false,
            "SecurityOpt": [
                "seccomp=unconfined",
                "apparmor=unconfined",
                "label=disable"
            ],
            "Tmpfs": {
                "/run": "",
                "/tmp": ""
            },
            "UTSMode": "",
            "UsernsMode": "",
            "ShmSize": 67108864,
            "Runtime": "runc",
            "Isolation": "",
            "CpuShares": 0,
            "Memory": 3145728000,
            "NanoCpus": 2000000000,
            "CgroupParent": "",
            "BlkioWeight": 0,
            "BlkioWeightDevice": [],
            "BlkioDeviceReadBps": [],
            "BlkioDeviceWriteBps": [],
            "BlkioDeviceReadIOps": [],
            "BlkioDeviceWriteIOps": [],
            "CpuPeriod": 0,
            "CpuQuota": 0,
            "CpuRealtimePeriod": 0,
            "CpuRealtimeRuntime": 0,
            "CpusetCpus": "",
            "CpusetMems": "",
            "Devices": [],
            "DeviceCgroupRules": null,
            "DeviceRequests": null,
            "MemoryReservation": 0,
            "MemorySwap": 3145728000,
            "MemorySwappiness": null,
            "OomKillDisable": false,
            "PidsLimit": null,
            "Ulimits": null,
            "CpuCount": 0,
            "CpuPercent": 0,
            "IOMaximumIOps": 0,
            "IOMaximumBandwidth": 0,
            "MaskedPaths": null,
            "ReadonlyPaths": null
        },
        "GraphDriver": {
            "Data": {
                "LowerDir": "/var/lib/docker/overlay2/bf1c00194ec8a972dfe64132898db16202dab44137db33e9229121d93bbbd409-init/diff:/var/lib/docker/overlay2/a75a25db4452a60896b64b77d901666acdccd3749014901f18b17903979c887a/diff",
                "MergedDir": "/var/lib/docker/overlay2/bf1c00194ec8a972dfe64132898db16202dab44137db33e9229121d93bbbd409/merged",
                "UpperDir": "/var/lib/docker/overlay2/bf1c00194ec8a972dfe64132898db16202dab44137db33e9229121d93bbbd409/diff",
                "WorkDir": "/var/lib/docker/overlay2/bf1c00194ec8a972dfe64132898db16202dab44137db33e9229121d93bbbd409/work"
            },
            "Name": "overlay2"
        },
        "Mounts": [
            {
                "Type": "bind",
                "Source": "/lib/modules",
                "Destination": "/lib/modules",
                "Mode": "ro",
                "RW": false,
                "Propagation": "rprivate"
            },
            {
                "Type": "volume",
                "Name": "minikube",
                "Source": "/var/lib/docker/volumes/minikube/_data",
                "Destination": "/var",
                "Driver": "local",
                "Mode": "z",
                "RW": true,
                "Propagation": ""
            }
        ],
        "Config": {
            "Hostname": "minikube",
            "Domainname": "",
            "User": "",
            "AttachStdin": false,
            "AttachStdout": false,
            "AttachStderr": false,
            "ExposedPorts": {
                "22/tcp": {},
                "2376/tcp": {},
                "32443/tcp": {},
                "5000/tcp": {},
                "8443/tcp": {}
            },
            "Tty": true,
            "OpenStdin": false,
            "StdinOnce": false,
            "Env": [
                "container=docker",
                "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
            ],
            "Cmd": null,
            "Image": "gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631",
            "Volumes": null,
            "WorkingDir": "/",
            "Entrypoint": [
                "/usr/local/bin/entrypoint",
                "/sbin/init"
            ],
            "OnBuild": null,
            "Labels": {
                "created_by.minikube.sigs.k8s.io": "true",
                "mode.minikube.sigs.k8s.io": "minikube",
                "name.minikube.sigs.k8s.io": "minikube",
                "role.minikube.sigs.k8s.io": ""
            },
            "StopSignal": "SIGRTMIN+3"
        },
        "NetworkSettings": {
            "Bridge": "",
            "SandboxID": "cbcd5e1cbd404c0ea129937e0eeeedf749a09cd25aa468c6e247f6f081c5c42f",
            "HairpinMode": false,
            "LinkLocalIPv6Address": "",
            "LinkLocalIPv6PrefixLen": 0,
            "Ports": {},
            "SandboxKey": "/var/run/docker/netns/cbcd5e1cbd40",
            "SecondaryIPAddresses": null,
            "SecondaryIPv6Addresses": null,
            "EndpointID": "",
            "Gateway": "",
            "GlobalIPv6Address": "",
            "GlobalIPv6PrefixLen": 0,
            "IPAddress": "",
            "IPPrefixLen": 0,
            "IPv6Gateway": "",
            "MacAddress": "",
            "Networks": {
                "minikube": {
                    "IPAMConfig": {
                        "IPv4Address": "192.168.49.2"
                    },
                    "Links": null,
                    "Aliases": [
                        "078fdaa2e1d1",
                        "minikube"
                    ],
                    "NetworkID": "26d577b4fe054d54ba5558ff6367eb73303c66a07b47a47062ba51445db5f607",
                    "EndpointID": "",
                    "Gateway": "",
                    "IPAddress": "",
                    "IPPrefixLen": 0,
                    "IPv6Gateway": "",
                    "GlobalIPv6Address": "",
                    "GlobalIPv6PrefixLen": 0,
                    "MacAddress": "",
                    "DriverOpts": null
                }
            }
        }
    }
]

-- /stdout --
I1010 22:47:42.795039   29740 cli_runner.go:164] Run: docker logs --timestamps --details minikube
I1010 22:47:44.124708   29740 cli_runner.go:217] Completed: docker logs --timestamps --details minikube: (1.2903903s)
I1010 22:47:44.175064   29740 errors.go:91] Postmortem logs ("docker logs --timestamps --details minikube"): -- stdout --
2023-10-11T03:38:14.151136286Z  + userns=
2023-10-11T03:38:14.151270993Z  + grep -Eqv '0[[:space:]]+0[[:space:]]+4294967295' /proc/self/uid_map
2023-10-11T03:38:14.167808695Z  + validate_userns
2023-10-11T03:38:14.168781175Z  + [[ -z '' ]]
2023-10-11T03:38:14.168814006Z  + return
2023-10-11T03:38:14.168819231Z  + configure_containerd
2023-10-11T03:38:14.168823489Z  + local snapshotter=
2023-10-11T03:38:14.169106975Z  + [[ -n '' ]]
2023-10-11T03:38:14.169129780Z  + [[ -z '' ]]
2023-10-11T03:38:14.169926030Z  ++ stat -f -c %!T(MISSING) /kind
2023-10-11T03:38:14.172495720Z  + container_filesystem=overlayfs
2023-10-11T03:38:14.172531190Z  + [[ overlayfs == \z\f\s ]]
2023-10-11T03:38:14.172536292Z  + [[ -n '' ]]
2023-10-11T03:38:14.172539829Z  + configure_proxy
2023-10-11T03:38:14.172750666Z  + mkdir -p /etc/systemd/system.conf.d/
2023-10-11T03:38:14.176933225Z  + [[ ! -z '' ]]
2023-10-11T03:38:14.176976957Z  + cat
2023-10-11T03:38:14.180050755Z  + fix_mount
2023-10-11T03:38:14.180084996Z  + echo 'INFO: ensuring we can execute mount/umount even with userns-remap'
2023-10-11T03:38:14.180091932Z  INFO: ensuring we can execute mount/umount even with userns-remap
2023-10-11T03:38:14.180918233Z  ++ which mount
2023-10-11T03:38:14.184933156Z  ++ which umount
2023-10-11T03:38:14.186288879Z  + chown root:root /usr/bin/mount /usr/bin/umount
2023-10-11T03:38:14.207451697Z  ++ which mount
2023-10-11T03:38:14.209482357Z  ++ which umount
2023-10-11T03:38:14.211066575Z  + chmod -s /usr/bin/mount /usr/bin/umount
2023-10-11T03:38:14.215789147Z  +++ which mount
2023-10-11T03:38:14.217473742Z  ++ stat -f -c %!T(MISSING) /usr/bin/mount
2023-10-11T03:38:14.218749829Z  + [[ overlayfs == \a\u\f\s ]]
2023-10-11T03:38:14.218776099Z  + echo 'INFO: remounting /sys read-only'
2023-10-11T03:38:14.218781313Z  INFO: remounting /sys read-only
2023-10-11T03:38:14.218785245Z  + mount -o remount,ro /sys
2023-10-11T03:38:14.221409832Z  + echo 'INFO: making mounts shared'
2023-10-11T03:38:14.221444888Z  INFO: making mounts shared
2023-10-11T03:38:14.221449896Z  + mount --make-rshared /
2023-10-11T03:38:14.223863410Z  + retryable_fix_cgroup
2023-10-11T03:38:14.224757192Z  ++ seq 0 10
2023-10-11T03:38:14.228926665Z  + for i in $(seq 0 10)
2023-10-11T03:38:14.228972723Z  + fix_cgroup
2023-10-11T03:38:14.229198500Z  + [[ -f /sys/fs/cgroup/cgroup.controllers ]]
2023-10-11T03:38:14.229222869Z  + echo 'INFO: detected cgroup v1'
2023-10-11T03:38:14.229231390Z  INFO: detected cgroup v1
2023-10-11T03:38:14.229257945Z  + local current_cgroup
2023-10-11T03:38:14.230461660Z  ++ grep -E '^[^:]*:([^:]*,)?cpu(,[^,:]*)?:.*' /proc/self/cgroup
2023-10-11T03:38:14.230504643Z  ++ cut -d: -f3
2023-10-11T03:38:14.234062779Z  + current_cgroup=/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.234109658Z  + '[' /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 = / ']'
2023-10-11T03:38:14.234120257Z  + echo 'WARN: cgroupns not enabled! Please use cgroup v2, or cgroup v1 with cgroupns enabled.'
2023-10-11T03:38:14.234124340Z  WARN: cgroupns not enabled! Please use cgroup v2, or cgroup v1 with cgroupns enabled.
2023-10-11T03:38:14.234127845Z  + echo 'INFO: fix cgroup mounts for all subsystems'
2023-10-11T03:38:14.234131203Z  INFO: fix cgroup mounts for all subsystems
2023-10-11T03:38:14.234135186Z  + local cgroup_subsystems
2023-10-11T03:38:14.235232413Z  ++ findmnt -lun -o source,target -t cgroup
2023-10-11T03:38:14.235267571Z  ++ grep -F /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.235583723Z  ++ awk '{print $2}'
2023-10-11T03:38:14.246870497Z  + cgroup_subsystems='/sys/fs/cgroup/cpuset
2023-10-11T03:38:14.246914202Z  /sys/fs/cgroup/cpu
2023-10-11T03:38:14.246922757Z  /sys/fs/cgroup/cpuacct
2023-10-11T03:38:14.246926809Z  /sys/fs/cgroup/blkio
2023-10-11T03:38:14.246930394Z  /sys/fs/cgroup/memory
2023-10-11T03:38:14.246933974Z  /sys/fs/cgroup/devices
2023-10-11T03:38:14.246937717Z  /sys/fs/cgroup/freezer
2023-10-11T03:38:14.246944741Z  /sys/fs/cgroup/net_cls
2023-10-11T03:38:14.246951315Z  /sys/fs/cgroup/perf_event
2023-10-11T03:38:14.246955186Z  /sys/fs/cgroup/net_prio
2023-10-11T03:38:14.246958953Z  /sys/fs/cgroup/hugetlb
2023-10-11T03:38:14.246961496Z  /sys/fs/cgroup/pids
2023-10-11T03:38:14.246963856Z  /sys/fs/cgroup/rdma
2023-10-11T03:38:14.246966159Z  /sys/fs/cgroup/systemd'
2023-10-11T03:38:14.246968609Z  + local unsupported_cgroups
2023-10-11T03:38:14.247677665Z  ++ findmnt -lun -o source,target -t cgroup
2023-10-11T03:38:14.247836316Z  ++ grep_allow_nomatch -v -F /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.247847675Z  ++ grep -v -F /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.247852265Z  ++ awk '{print $2}'
2023-10-11T03:38:14.250332471Z  ++ [[ 1 == 1 ]]
2023-10-11T03:38:14.251125339Z  + unsupported_cgroups=
2023-10-11T03:38:14.251148621Z  + '[' -n '' ']'
2023-10-11T03:38:14.251156693Z  + local cgroup_mounts
2023-10-11T03:38:14.251833384Z  ++ grep -E -o '/[[:alnum:]].* /sys/fs/cgroup.*.*cgroup' /proc/self/mountinfo
2023-10-11T03:38:14.254009427Z  + cgroup_mounts='/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/cpuset rw,nosuid,nodev,noexec,relatime shared:468 master:67 - cgroup
2023-10-11T03:38:14.254041300Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/cpu rw,nosuid,nodev,noexec,relatime shared:469 master:68 - cgroup
2023-10-11T03:38:14.254046356Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/cpuacct rw,nosuid,nodev,noexec,relatime shared:470 master:69 - cgroup
2023-10-11T03:38:14.254050667Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/blkio rw,nosuid,nodev,noexec,relatime shared:471 master:70 - cgroup
2023-10-11T03:38:14.254055082Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/memory rw,nosuid,nodev,noexec,relatime shared:472 master:71 - cgroup
2023-10-11T03:38:14.254058772Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/devices rw,nosuid,nodev,noexec,relatime shared:473 master:72 - cgroup
2023-10-11T03:38:14.254073557Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/freezer rw,nosuid,nodev,noexec,relatime shared:474 master:73 - cgroup
2023-10-11T03:38:14.254077843Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/net_cls rw,nosuid,nodev,noexec,relatime shared:475 master:74 - cgroup
2023-10-11T03:38:14.254081763Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/perf_event rw,nosuid,nodev,noexec,relatime shared:476 master:75 - cgroup
2023-10-11T03:38:14.254088970Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/net_prio rw,nosuid,nodev,noexec,relatime shared:477 master:76 - cgroup
2023-10-11T03:38:14.254096119Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/hugetlb rw,nosuid,nodev,noexec,relatime shared:478 master:77 - cgroup
2023-10-11T03:38:14.254100409Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/pids rw,nosuid,nodev,noexec,relatime shared:479 master:78 - cgroup
2023-10-11T03:38:14.254104543Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/rdma rw,nosuid,nodev,noexec,relatime shared:480 master:79 - cgroup
2023-10-11T03:38:14.254107288Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/systemd rw,nosuid,nodev,noexec,relatime shared:481 master:80 - cgroup cgroup'
2023-10-11T03:38:14.254110023Z  + [[ -n /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/cpuset rw,nosuid,nodev,noexec,relatime shared:468 master:67 - cgroup
2023-10-11T03:38:14.254112883Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/cpu rw,nosuid,nodev,noexec,relatime shared:469 master:68 - cgroup
2023-10-11T03:38:14.254115634Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/cpuacct rw,nosuid,nodev,noexec,relatime shared:470 master:69 - cgroup
2023-10-11T03:38:14.254118379Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/blkio rw,nosuid,nodev,noexec,relatime shared:471 master:70 - cgroup
2023-10-11T03:38:14.254133379Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/memory rw,nosuid,nodev,noexec,relatime shared:472 master:71 - cgroup
2023-10-11T03:38:14.254136378Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/devices rw,nosuid,nodev,noexec,relatime shared:473 master:72 - cgroup
2023-10-11T03:38:14.254139043Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/freezer rw,nosuid,nodev,noexec,relatime shared:474 master:73 - cgroup
2023-10-11T03:38:14.254141657Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/net_cls rw,nosuid,nodev,noexec,relatime shared:475 master:74 - cgroup
2023-10-11T03:38:14.254145704Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/perf_event rw,nosuid,nodev,noexec,relatime shared:476 master:75 - cgroup
2023-10-11T03:38:14.254150436Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/net_prio rw,nosuid,nodev,noexec,relatime shared:477 master:76 - cgroup
2023-10-11T03:38:14.254154551Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/hugetlb rw,nosuid,nodev,noexec,relatime shared:478 master:77 - cgroup
2023-10-11T03:38:14.254159141Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/pids rw,nosuid,nodev,noexec,relatime shared:479 master:78 - cgroup
2023-10-11T03:38:14.254163540Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/rdma rw,nosuid,nodev,noexec,relatime shared:480 master:79 - cgroup
2023-10-11T03:38:14.254167892Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/systemd rw,nosuid,nodev,noexec,relatime shared:481 master:80 - cgroup cgroup ]]
2023-10-11T03:38:14.254172092Z  + local mount_root
2023-10-11T03:38:14.254968671Z  ++ head -n 1
2023-10-11T03:38:14.255022244Z  ++ cut '-d ' -f1
2023-10-11T03:38:14.257979061Z  + mount_root=/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.259208956Z  ++ echo '/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/cpuset rw,nosuid,nodev,noexec,relatime shared:468 master:67 - cgroup
2023-10-11T03:38:14.259271676Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/cpu rw,nosuid,nodev,noexec,relatime shared:469 master:68 - cgroup
2023-10-11T03:38:14.259303027Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/cpuacct rw,nosuid,nodev,noexec,relatime shared:470 master:69 - cgroup
2023-10-11T03:38:14.259307582Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/blkio rw,nosuid,nodev,noexec,relatime shared:471 master:70 - cgroup
2023-10-11T03:38:14.259312031Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/memory rw,nosuid,nodev,noexec,relatime shared:472 master:71 - cgroup
2023-10-11T03:38:14.259316510Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/devices rw,nosuid,nodev,noexec,relatime shared:473 master:72 - cgroup
2023-10-11T03:38:14.259352660Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/freezer rw,nosuid,nodev,noexec,relatime shared:474 master:73 - cgroup
2023-10-11T03:38:14.259360182Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/net_cls rw,nosuid,nodev,noexec,relatime shared:475 master:74 - cgroup
2023-10-11T03:38:14.259365062Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/perf_event rw,nosuid,nodev,noexec,relatime shared:476 master:75 - cgroup
2023-10-11T03:38:14.259369694Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/net_prio rw,nosuid,nodev,noexec,relatime shared:477 master:76 - cgroup
2023-10-11T03:38:14.259374228Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/hugetlb rw,nosuid,nodev,noexec,relatime shared:478 master:77 - cgroup
2023-10-11T03:38:14.259378925Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/pids rw,nosuid,nodev,noexec,relatime shared:479 master:78 - cgroup
2023-10-11T03:38:14.259385737Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/rdma rw,nosuid,nodev,noexec,relatime shared:480 master:79 - cgroup
2023-10-11T03:38:14.259390230Z  /docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802 /sys/fs/cgroup/systemd rw,nosuid,nodev,noexec,relatime shared:481 master:80 - cgroup cgroup'
2023-10-11T03:38:14.259395186Z  ++ cut '-d ' -f 2
2023-10-11T03:38:14.263671501Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.263722944Z  + local target=/sys/fs/cgroup/cpuset/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.263727702Z  + findmnt /sys/fs/cgroup/cpuset/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.267766164Z  + mkdir -p /sys/fs/cgroup/cpuset/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.342995912Z  + mount --bind /sys/fs/cgroup/cpuset /sys/fs/cgroup/cpuset/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.345005624Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.345038052Z  + local target=/sys/fs/cgroup/cpu/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.345043448Z  + findmnt /sys/fs/cgroup/cpu/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.348097712Z  + mkdir -p /sys/fs/cgroup/cpu/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.349898324Z  + mount --bind /sys/fs/cgroup/cpu /sys/fs/cgroup/cpu/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.352064236Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.352106868Z  + local target=/sys/fs/cgroup/cpuacct/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.352127069Z  + findmnt /sys/fs/cgroup/cpuacct/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.354890625Z  + mkdir -p /sys/fs/cgroup/cpuacct/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.356932002Z  + mount --bind /sys/fs/cgroup/cpuacct /sys/fs/cgroup/cpuacct/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.359238895Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.359327176Z  + local target=/sys/fs/cgroup/blkio/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.359335565Z  + findmnt /sys/fs/cgroup/blkio/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.364593407Z  + mkdir -p /sys/fs/cgroup/blkio/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.366883616Z  + mount --bind /sys/fs/cgroup/blkio /sys/fs/cgroup/blkio/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.369293959Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.369330149Z  + local target=/sys/fs/cgroup/memory/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.369335472Z  + findmnt /sys/fs/cgroup/memory/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.372685104Z  + mkdir -p /sys/fs/cgroup/memory/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.375941175Z  + mount --bind /sys/fs/cgroup/memory /sys/fs/cgroup/memory/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.378642528Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.378693981Z  + local target=/sys/fs/cgroup/devices/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.378703816Z  + findmnt /sys/fs/cgroup/devices/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.397498843Z  + mkdir -p /sys/fs/cgroup/devices/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.399398863Z  + mount --bind /sys/fs/cgroup/devices /sys/fs/cgroup/devices/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.402494484Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.402676169Z  + local target=/sys/fs/cgroup/freezer/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.402687132Z  + findmnt /sys/fs/cgroup/freezer/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.406117581Z  + mkdir -p /sys/fs/cgroup/freezer/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.408288867Z  + mount --bind /sys/fs/cgroup/freezer /sys/fs/cgroup/freezer/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.410766324Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.410812201Z  + local target=/sys/fs/cgroup/net_cls/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.438915849Z  + findmnt /sys/fs/cgroup/net_cls/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.438956818Z  + mkdir -p /sys/fs/cgroup/net_cls/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.438964447Z  + mount --bind /sys/fs/cgroup/net_cls /sys/fs/cgroup/net_cls/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.438968913Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.438978112Z  + local target=/sys/fs/cgroup/perf_event/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.438984388Z  + findmnt /sys/fs/cgroup/perf_event/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.438988709Z  + mkdir -p /sys/fs/cgroup/perf_event/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.438993295Z  + mount --bind /sys/fs/cgroup/perf_event /sys/fs/cgroup/perf_event/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.438999884Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.439007355Z  + local target=/sys/fs/cgroup/net_prio/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.439011563Z  + findmnt /sys/fs/cgroup/net_prio/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.439019763Z  + mkdir -p /sys/fs/cgroup/net_prio/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.441334576Z  + mount --bind /sys/fs/cgroup/net_prio /sys/fs/cgroup/net_prio/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.443665951Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.443701114Z  + local target=/sys/fs/cgroup/hugetlb/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.443706536Z  + findmnt /sys/fs/cgroup/hugetlb/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.446440856Z  + mkdir -p /sys/fs/cgroup/hugetlb/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.448118563Z  + mount --bind /sys/fs/cgroup/hugetlb /sys/fs/cgroup/hugetlb/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.450209275Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.450243307Z  + local target=/sys/fs/cgroup/pids/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.450249000Z  + findmnt /sys/fs/cgroup/pids/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.452801675Z  + mkdir -p /sys/fs/cgroup/pids/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.454609573Z  + mount --bind /sys/fs/cgroup/pids /sys/fs/cgroup/pids/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.465827752Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.465870492Z  + local target=/sys/fs/cgroup/rdma/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.465876225Z  + findmnt /sys/fs/cgroup/rdma/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.468792783Z  + mkdir -p /sys/fs/cgroup/rdma/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.470578288Z  + mount --bind /sys/fs/cgroup/rdma /sys/fs/cgroup/rdma/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.472828805Z  + for mount_point in $(echo "${cgroup_mounts}" | cut -d' ' -f 2)
2023-10-11T03:38:14.472884390Z  + local target=/sys/fs/cgroup/systemd/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.472893556Z  + findmnt /sys/fs/cgroup/systemd/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.475674949Z  + mkdir -p /sys/fs/cgroup/systemd/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.477458327Z  + mount --bind /sys/fs/cgroup/systemd /sys/fs/cgroup/systemd/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802
2023-10-11T03:38:14.479799234Z  + mount --make-rprivate /sys/fs/cgroup
2023-10-11T03:38:14.482200741Z  + echo '/sys/fs/cgroup/cpuset
2023-10-11T03:38:14.482233251Z  /sys/fs/cgroup/cpu
2023-10-11T03:38:14.482237671Z  /sys/fs/cgroup/cpuacct
2023-10-11T03:38:14.482241521Z  /sys/fs/cgroup/blkio
2023-10-11T03:38:14.482245897Z  /sys/fs/cgroup/memory
2023-10-11T03:38:14.482249880Z  /sys/fs/cgroup/devices
2023-10-11T03:38:14.482253750Z  /sys/fs/cgroup/freezer
2023-10-11T03:38:14.482257557Z  /sys/fs/cgroup/net_cls
2023-10-11T03:38:14.482261350Z  /sys/fs/cgroup/perf_event
2023-10-11T03:38:14.482265098Z  /sys/fs/cgroup/net_prio
2023-10-11T03:38:14.482268833Z  /sys/fs/cgroup/hugetlb
2023-10-11T03:38:14.482272532Z  /sys/fs/cgroup/pids
2023-10-11T03:38:14.482276522Z  /sys/fs/cgroup/rdma
2023-10-11T03:38:14.482280543Z  /sys/fs/cgroup/systemd'
2023-10-11T03:38:14.482284672Z  + IFS=
2023-10-11T03:38:14.482288682Z  + read -r subsystem
2023-10-11T03:38:14.482299616Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/cpuset
2023-10-11T03:38:14.482303959Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.482308078Z  + local subsystem=/sys/fs/cgroup/cpuset
2023-10-11T03:38:14.482452914Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.482489056Z  + mkdir -p /sys/fs/cgroup/cpuset//kubelet
2023-10-11T03:38:14.583172683Z  + '[' /sys/fs/cgroup/cpuset == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.583222676Z  + cat /sys/fs/cgroup/cpuset/cpuset.cpus
2023-10-11T03:38:14.586240880Z  + cat /sys/fs/cgroup/cpuset/cpuset.mems
2023-10-11T03:38:14.587804796Z  + mount --bind /sys/fs/cgroup/cpuset//kubelet /sys/fs/cgroup/cpuset//kubelet
2023-10-11T03:38:14.589952131Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/cpuset
2023-10-11T03:38:14.589991906Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.590003274Z  + local subsystem=/sys/fs/cgroup/cpuset
2023-10-11T03:38:14.590010532Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.590014348Z  + mkdir -p /sys/fs/cgroup/cpuset//kubelet.slice
2023-10-11T03:38:14.592380528Z  + '[' /sys/fs/cgroup/cpuset == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.592424560Z  + cat /sys/fs/cgroup/cpuset/cpuset.cpus
2023-10-11T03:38:14.596307587Z  + cat /sys/fs/cgroup/cpuset/cpuset.mems
2023-10-11T03:38:14.597804966Z  + mount --bind /sys/fs/cgroup/cpuset//kubelet.slice /sys/fs/cgroup/cpuset//kubelet.slice
2023-10-11T03:38:14.600367372Z  + IFS=
2023-10-11T03:38:14.600406440Z  + read -r subsystem
2023-10-11T03:38:14.600418232Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/cpu
2023-10-11T03:38:14.600422790Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.600427143Z  + local subsystem=/sys/fs/cgroup/cpu
2023-10-11T03:38:14.600430789Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.600434524Z  + mkdir -p /sys/fs/cgroup/cpu//kubelet
2023-10-11T03:38:14.602409868Z  + '[' /sys/fs/cgroup/cpu == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.602442105Z  + mount --bind /sys/fs/cgroup/cpu//kubelet /sys/fs/cgroup/cpu//kubelet
2023-10-11T03:38:14.604395153Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/cpu
2023-10-11T03:38:14.604429444Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.604438491Z  + local subsystem=/sys/fs/cgroup/cpu
2023-10-11T03:38:14.604442637Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.604447067Z  + mkdir -p /sys/fs/cgroup/cpu//kubelet.slice
2023-10-11T03:38:14.606198610Z  + '[' /sys/fs/cgroup/cpu == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.606232722Z  + mount --bind /sys/fs/cgroup/cpu//kubelet.slice /sys/fs/cgroup/cpu//kubelet.slice
2023-10-11T03:38:14.608507148Z  + IFS=
2023-10-11T03:38:14.608543677Z  + read -r subsystem
2023-10-11T03:38:14.608551460Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/cpuacct
2023-10-11T03:38:14.608558259Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.608565916Z  + local subsystem=/sys/fs/cgroup/cpuacct
2023-10-11T03:38:14.608588731Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.608593077Z  + mkdir -p /sys/fs/cgroup/cpuacct//kubelet
2023-10-11T03:38:14.610630033Z  + '[' /sys/fs/cgroup/cpuacct == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.610665658Z  + mount --bind /sys/fs/cgroup/cpuacct//kubelet /sys/fs/cgroup/cpuacct//kubelet
2023-10-11T03:38:14.612783941Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/cpuacct
2023-10-11T03:38:14.612817523Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.612822531Z  + local subsystem=/sys/fs/cgroup/cpuacct
2023-10-11T03:38:14.612826417Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.612830377Z  + mkdir -p /sys/fs/cgroup/cpuacct//kubelet.slice
2023-10-11T03:38:14.615045102Z  + '[' /sys/fs/cgroup/cpuacct == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.615078955Z  + mount --bind /sys/fs/cgroup/cpuacct//kubelet.slice /sys/fs/cgroup/cpuacct//kubelet.slice
2023-10-11T03:38:14.617322809Z  + IFS=
2023-10-11T03:38:14.617365409Z  + read -r subsystem
2023-10-11T03:38:14.617374026Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/blkio
2023-10-11T03:38:14.617508579Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.617533645Z  + local subsystem=/sys/fs/cgroup/blkio
2023-10-11T03:38:14.617538482Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.617542488Z  + mkdir -p /sys/fs/cgroup/blkio//kubelet
2023-10-11T03:38:14.620047083Z  + '[' /sys/fs/cgroup/blkio == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.620086348Z  + mount --bind /sys/fs/cgroup/blkio//kubelet /sys/fs/cgroup/blkio//kubelet
2023-10-11T03:38:14.622300370Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/blkio
2023-10-11T03:38:14.622344923Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.622353104Z  + local subsystem=/sys/fs/cgroup/blkio
2023-10-11T03:38:14.622357025Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.622361381Z  + mkdir -p /sys/fs/cgroup/blkio//kubelet.slice
2023-10-11T03:38:14.624298586Z  + '[' /sys/fs/cgroup/blkio == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.624338626Z  + mount --bind /sys/fs/cgroup/blkio//kubelet.slice /sys/fs/cgroup/blkio//kubelet.slice
2023-10-11T03:38:14.628216444Z  + IFS=
2023-10-11T03:38:14.628273416Z  + read -r subsystem
2023-10-11T03:38:14.628283336Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/memory
2023-10-11T03:38:14.628287921Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.628291473Z  + local subsystem=/sys/fs/cgroup/memory
2023-10-11T03:38:14.628295065Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.628303145Z  + mkdir -p /sys/fs/cgroup/memory//kubelet
2023-10-11T03:38:14.631022512Z  + '[' /sys/fs/cgroup/memory == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.631068876Z  + mount --bind /sys/fs/cgroup/memory//kubelet /sys/fs/cgroup/memory//kubelet
2023-10-11T03:38:14.633600261Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/memory
2023-10-11T03:38:14.633637760Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.633642517Z  + local subsystem=/sys/fs/cgroup/memory
2023-10-11T03:38:14.633646318Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.633650131Z  + mkdir -p /sys/fs/cgroup/memory//kubelet.slice
2023-10-11T03:38:14.635591389Z  + '[' /sys/fs/cgroup/memory == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.635628500Z  + mount --bind /sys/fs/cgroup/memory//kubelet.slice /sys/fs/cgroup/memory//kubelet.slice
2023-10-11T03:38:14.638114254Z  + IFS=
2023-10-11T03:38:14.638148641Z  + read -r subsystem
2023-10-11T03:38:14.638153530Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/devices
2023-10-11T03:38:14.638157872Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.638161518Z  + local subsystem=/sys/fs/cgroup/devices
2023-10-11T03:38:14.638165212Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.638168974Z  + mkdir -p /sys/fs/cgroup/devices//kubelet
2023-10-11T03:38:14.643091320Z  + '[' /sys/fs/cgroup/devices == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.643141735Z  + mount --bind /sys/fs/cgroup/devices//kubelet /sys/fs/cgroup/devices//kubelet
2023-10-11T03:38:14.645479551Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/devices
2023-10-11T03:38:14.645521589Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.645526665Z  + local subsystem=/sys/fs/cgroup/devices
2023-10-11T03:38:14.645530227Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.645534337Z  + mkdir -p /sys/fs/cgroup/devices//kubelet.slice
2023-10-11T03:38:14.647754849Z  + '[' /sys/fs/cgroup/devices == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.647789725Z  + mount --bind /sys/fs/cgroup/devices//kubelet.slice /sys/fs/cgroup/devices//kubelet.slice
2023-10-11T03:38:14.650126435Z  + IFS=
2023-10-11T03:38:14.650171752Z  + read -r subsystem
2023-10-11T03:38:14.650180943Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/freezer
2023-10-11T03:38:14.650185840Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.650189946Z  + local subsystem=/sys/fs/cgroup/freezer
2023-10-11T03:38:14.650337635Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.650361780Z  + mkdir -p /sys/fs/cgroup/freezer//kubelet
2023-10-11T03:38:14.652395275Z  + '[' /sys/fs/cgroup/freezer == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.652439264Z  + mount --bind /sys/fs/cgroup/freezer//kubelet /sys/fs/cgroup/freezer//kubelet
2023-10-11T03:38:14.655857485Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/freezer
2023-10-11T03:38:14.655891967Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.655917674Z  + local subsystem=/sys/fs/cgroup/freezer
2023-10-11T03:38:14.655921944Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.655925582Z  + mkdir -p /sys/fs/cgroup/freezer//kubelet.slice
2023-10-11T03:38:14.659141376Z  + '[' /sys/fs/cgroup/freezer == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.659210627Z  + mount --bind /sys/fs/cgroup/freezer//kubelet.slice /sys/fs/cgroup/freezer//kubelet.slice
2023-10-11T03:38:14.669443492Z  + IFS=
2023-10-11T03:38:14.669480455Z  + read -r subsystem
2023-10-11T03:38:14.669485514Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/net_cls
2023-10-11T03:38:14.669489462Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.669493148Z  + local subsystem=/sys/fs/cgroup/net_cls
2023-10-11T03:38:14.669496655Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.669501210Z  + mkdir -p /sys/fs/cgroup/net_cls//kubelet
2023-10-11T03:38:14.671436813Z  + '[' /sys/fs/cgroup/net_cls == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.671481404Z  + mount --bind /sys/fs/cgroup/net_cls//kubelet /sys/fs/cgroup/net_cls//kubelet
2023-10-11T03:38:14.673934596Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/net_cls
2023-10-11T03:38:14.673997224Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.674004265Z  + local subsystem=/sys/fs/cgroup/net_cls
2023-10-11T03:38:14.674008687Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.674012683Z  + mkdir -p /sys/fs/cgroup/net_cls//kubelet.slice
2023-10-11T03:38:14.676218011Z  + '[' /sys/fs/cgroup/net_cls == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.676269169Z  + mount --bind /sys/fs/cgroup/net_cls//kubelet.slice /sys/fs/cgroup/net_cls//kubelet.slice
2023-10-11T03:38:14.679147295Z  + IFS=
2023-10-11T03:38:14.679184485Z  + read -r subsystem
2023-10-11T03:38:14.679190009Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/perf_event
2023-10-11T03:38:14.679194330Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.679198392Z  + local subsystem=/sys/fs/cgroup/perf_event
2023-10-11T03:38:14.679202222Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.679205946Z  + mkdir -p /sys/fs/cgroup/perf_event//kubelet
2023-10-11T03:38:14.681327887Z  + '[' /sys/fs/cgroup/perf_event == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.681363544Z  + mount --bind /sys/fs/cgroup/perf_event//kubelet /sys/fs/cgroup/perf_event//kubelet
2023-10-11T03:38:14.683971242Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/perf_event
2023-10-11T03:38:14.684021310Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.684028676Z  + local subsystem=/sys/fs/cgroup/perf_event
2023-10-11T03:38:14.684032848Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.684036678Z  + mkdir -p /sys/fs/cgroup/perf_event//kubelet.slice
2023-10-11T03:38:14.686127722Z  + '[' /sys/fs/cgroup/perf_event == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.686164736Z  + mount --bind /sys/fs/cgroup/perf_event//kubelet.slice /sys/fs/cgroup/perf_event//kubelet.slice
2023-10-11T03:38:14.705857944Z  + IFS=
2023-10-11T03:38:14.705909888Z  + read -r subsystem
2023-10-11T03:38:14.705915220Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/net_prio
2023-10-11T03:38:14.705919439Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.705923421Z  + local subsystem=/sys/fs/cgroup/net_prio
2023-10-11T03:38:14.705927615Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.705932840Z  + mkdir -p /sys/fs/cgroup/net_prio//kubelet
2023-10-11T03:38:14.705937148Z  + '[' /sys/fs/cgroup/net_prio == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.705941218Z  + mount --bind /sys/fs/cgroup/net_prio//kubelet /sys/fs/cgroup/net_prio//kubelet
2023-10-11T03:38:14.705945502Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/net_prio
2023-10-11T03:38:14.705949691Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.705953600Z  + local subsystem=/sys/fs/cgroup/net_prio
2023-10-11T03:38:14.705957516Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.705961344Z  + mkdir -p /sys/fs/cgroup/net_prio//kubelet.slice
2023-10-11T03:38:14.710494246Z  + '[' /sys/fs/cgroup/net_prio == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.710538962Z  + mount --bind /sys/fs/cgroup/net_prio//kubelet.slice /sys/fs/cgroup/net_prio//kubelet.slice
2023-10-11T03:38:14.712704838Z  + IFS=
2023-10-11T03:38:14.712745431Z  + read -r subsystem
2023-10-11T03:38:14.712755799Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/hugetlb
2023-10-11T03:38:14.712761114Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.712765149Z  + local subsystem=/sys/fs/cgroup/hugetlb
2023-10-11T03:38:14.712769064Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.712773155Z  + mkdir -p /sys/fs/cgroup/hugetlb//kubelet
2023-10-11T03:38:14.715199962Z  + '[' /sys/fs/cgroup/hugetlb == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.715232995Z  + mount --bind /sys/fs/cgroup/hugetlb//kubelet /sys/fs/cgroup/hugetlb//kubelet
2023-10-11T03:38:14.725676369Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/hugetlb
2023-10-11T03:38:14.725718244Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.725723732Z  + local subsystem=/sys/fs/cgroup/hugetlb
2023-10-11T03:38:14.725727846Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.725731612Z  + mkdir -p /sys/fs/cgroup/hugetlb//kubelet.slice
2023-10-11T03:38:14.725735356Z  + '[' /sys/fs/cgroup/hugetlb == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.725739305Z  + mount --bind /sys/fs/cgroup/hugetlb//kubelet.slice /sys/fs/cgroup/hugetlb//kubelet.slice
2023-10-11T03:38:14.725760382Z  + IFS=
2023-10-11T03:38:14.725764610Z  + read -r subsystem
2023-10-11T03:38:14.725768833Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/pids
2023-10-11T03:38:14.725772975Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.725776899Z  + local subsystem=/sys/fs/cgroup/pids
2023-10-11T03:38:14.725780734Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.725784369Z  + mkdir -p /sys/fs/cgroup/pids//kubelet
2023-10-11T03:38:14.727200607Z  + '[' /sys/fs/cgroup/pids == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.727314784Z  + mount --bind /sys/fs/cgroup/pids//kubelet /sys/fs/cgroup/pids//kubelet
2023-10-11T03:38:14.727321455Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/pids
2023-10-11T03:38:14.727325899Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.727330291Z  + local subsystem=/sys/fs/cgroup/pids
2023-10-11T03:38:14.727334815Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.727338368Z  + mkdir -p /sys/fs/cgroup/pids//kubelet.slice
2023-10-11T03:38:14.732203473Z  + '[' /sys/fs/cgroup/pids == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.732244325Z  + mount --bind /sys/fs/cgroup/pids//kubelet.slice /sys/fs/cgroup/pids//kubelet.slice
2023-10-11T03:38:14.739521254Z  + IFS=
2023-10-11T03:38:14.739557139Z  + read -r subsystem
2023-10-11T03:38:14.746567772Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/rdma
2023-10-11T03:38:14.746606934Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.746612003Z  + local subsystem=/sys/fs/cgroup/rdma
2023-10-11T03:38:14.746615706Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.746619976Z  + mkdir -p /sys/fs/cgroup/rdma//kubelet
2023-10-11T03:38:14.746623631Z  + '[' /sys/fs/cgroup/rdma == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.746627520Z  + mount --bind /sys/fs/cgroup/rdma//kubelet /sys/fs/cgroup/rdma//kubelet
2023-10-11T03:38:14.746631759Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/rdma
2023-10-11T03:38:14.746635942Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.746640398Z  + local subsystem=/sys/fs/cgroup/rdma
2023-10-11T03:38:14.746644204Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.746650511Z  + mkdir -p /sys/fs/cgroup/rdma//kubelet.slice
2023-10-11T03:38:14.746658527Z  + '[' /sys/fs/cgroup/rdma == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.746662348Z  + mount --bind /sys/fs/cgroup/rdma//kubelet.slice /sys/fs/cgroup/rdma//kubelet.slice
2023-10-11T03:38:14.746666319Z  + IFS=
2023-10-11T03:38:14.746670617Z  + read -r subsystem
2023-10-11T03:38:14.746674510Z  + mount_kubelet_cgroup_root /kubelet /sys/fs/cgroup/systemd
2023-10-11T03:38:14.746678605Z  + local cgroup_root=/kubelet
2023-10-11T03:38:14.746700641Z  + local subsystem=/sys/fs/cgroup/systemd
2023-10-11T03:38:14.746707777Z  + '[' -z /kubelet ']'
2023-10-11T03:38:14.746712152Z  + mkdir -p /sys/fs/cgroup/systemd//kubelet
2023-10-11T03:38:14.746716068Z  + '[' /sys/fs/cgroup/systemd == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.746720072Z  + mount --bind /sys/fs/cgroup/systemd//kubelet /sys/fs/cgroup/systemd//kubelet
2023-10-11T03:38:14.748619752Z  + mount_kubelet_cgroup_root /kubelet.slice /sys/fs/cgroup/systemd
2023-10-11T03:38:14.748658606Z  + local cgroup_root=/kubelet.slice
2023-10-11T03:38:14.748663378Z  + local subsystem=/sys/fs/cgroup/systemd
2023-10-11T03:38:14.748667173Z  + '[' -z /kubelet.slice ']'
2023-10-11T03:38:14.748671883Z  + mkdir -p /sys/fs/cgroup/systemd//kubelet.slice
2023-10-11T03:38:14.750807157Z  + '[' /sys/fs/cgroup/systemd == /sys/fs/cgroup/cpuset ']'
2023-10-11T03:38:14.750841041Z  + mount --bind /sys/fs/cgroup/systemd//kubelet.slice /sys/fs/cgroup/systemd//kubelet.slice
2023-10-11T03:38:14.752975817Z  + IFS=
2023-10-11T03:38:14.753013921Z  + read -r subsystem
2023-10-11T03:38:14.753406692Z  + [[ ! /sys/fs/cgroup/cpuset
2023-10-11T03:38:14.753428700Z  /sys/fs/cgroup/cpu
2023-10-11T03:38:14.753436510Z  /sys/fs/cgroup/cpuacct
2023-10-11T03:38:14.753440194Z  /sys/fs/cgroup/blkio
2023-10-11T03:38:14.753444014Z  /sys/fs/cgroup/memory
2023-10-11T03:38:14.753451372Z  /sys/fs/cgroup/devices
2023-10-11T03:38:14.753460152Z  /sys/fs/cgroup/freezer
2023-10-11T03:38:14.753465953Z  /sys/fs/cgroup/net_cls
2023-10-11T03:38:14.753470081Z  /sys/fs/cgroup/perf_event
2023-10-11T03:38:14.753474203Z  /sys/fs/cgroup/net_prio
2023-10-11T03:38:14.753480794Z  /sys/fs/cgroup/hugetlb
2023-10-11T03:38:14.753488431Z  /sys/fs/cgroup/pids
2023-10-11T03:38:14.753492477Z  /sys/fs/cgroup/rdma
2023-10-11T03:38:14.753496153Z  /sys/fs/cgroup/systemd = */sys/fs/cgroup/systemd* ]]
2023-10-11T03:38:14.753499863Z  + return
2023-10-11T03:38:14.753503139Z  + fix_machine_id
2023-10-11T03:38:14.753507328Z  + echo 'INFO: clearing and regenerating /etc/machine-id'
2023-10-11T03:38:14.753514015Z  INFO: clearing and regenerating /etc/machine-id
2023-10-11T03:38:14.753521916Z  + rm -f /etc/machine-id
2023-10-11T03:38:14.757281833Z  + systemd-machine-id-setup
2023-10-11T03:38:14.765197341Z  Initializing machine ID from random generator.
2023-10-11T03:38:14.768964233Z  + fix_product_name
2023-10-11T03:38:14.769005676Z  + [[ -f /sys/class/dmi/id/product_name ]]
2023-10-11T03:38:14.769010913Z  + fix_product_uuid
2023-10-11T03:38:14.769014774Z  + [[ ! -f /kind/product_uuid ]]
2023-10-11T03:38:14.769273747Z  + cat /proc/sys/kernel/random/uuid
2023-10-11T03:38:14.771028843Z  + [[ -f /sys/class/dmi/id/product_uuid ]]
2023-10-11T03:38:14.771054991Z  + [[ -f /sys/devices/virtual/dmi/id/product_uuid ]]
2023-10-11T03:38:14.771059701Z  + select_iptables
2023-10-11T03:38:14.771063925Z  + local mode num_legacy_lines num_nft_lines
2023-10-11T03:38:14.772473881Z  ++ grep -c '^-'
2023-10-11T03:38:14.779628828Z  + num_legacy_lines=6
2023-10-11T03:38:14.780876521Z  ++ grep -c '^-'
2023-10-11T03:38:14.792465506Z  ++ true
2023-10-11T03:38:14.792906189Z  + num_nft_lines=0
2023-10-11T03:38:14.792940243Z  + '[' 6 -ge 0 ']'
2023-10-11T03:38:14.792947254Z  + mode=legacy
2023-10-11T03:38:14.792951598Z  + echo 'INFO: setting iptables to detected mode: legacy'
2023-10-11T03:38:14.792977849Z  INFO: setting iptables to detected mode: legacy
2023-10-11T03:38:14.792985448Z  + update-alternatives --set iptables /usr/sbin/iptables-legacy
2023-10-11T03:38:14.793183293Z  + echo 'retryable update-alternatives: --set iptables /usr/sbin/iptables-legacy'
2023-10-11T03:38:14.793266180Z  + local 'args=--set iptables /usr/sbin/iptables-legacy'
2023-10-11T03:38:14.796059973Z  ++ seq 0 15
2023-10-11T03:38:14.802381280Z  + for i in $(seq 0 15)
2023-10-11T03:38:14.802428085Z  + /usr/bin/update-alternatives --set iptables /usr/sbin/iptables-legacy
2023-10-11T03:38:14.811798442Z  + return
2023-10-11T03:38:14.811831934Z  + update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy
2023-10-11T03:38:14.811837261Z  + echo 'retryable update-alternatives: --set ip6tables /usr/sbin/ip6tables-legacy'
2023-10-11T03:38:14.811841821Z  + local 'args=--set ip6tables /usr/sbin/ip6tables-legacy'
2023-10-11T03:38:14.812656673Z  ++ seq 0 15
2023-10-11T03:38:14.814140467Z  + for i in $(seq 0 15)
2023-10-11T03:38:14.814176188Z  + /usr/bin/update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy
2023-10-11T03:38:14.818900612Z  + return
2023-10-11T03:38:14.818928476Z  + enable_network_magic
2023-10-11T03:38:14.818939666Z  + local docker_embedded_dns_ip=127.0.0.11
2023-10-11T03:38:14.818944411Z  + local docker_host_ip
2023-10-11T03:38:14.820547332Z  ++ cut '-d ' -f1
2023-10-11T03:38:14.820580784Z  ++ head -n1 /dev/fd/63
2023-10-11T03:38:14.820588063Z  +++ timeout 5 getent ahostsv4 host.docker.internal
2023-10-11T03:38:14.839902542Z  + docker_host_ip=192.168.65.254
2023-10-11T03:38:14.839941610Z  + [[ -z 192.168.65.254 ]]
2023-10-11T03:38:14.839947309Z  + [[ 192.168.65.254 =~ ^127\.[0-9]+\.[0-9]+\.[0-9]+$ ]]
2023-10-11T03:38:14.840251433Z  + iptables-save
2023-10-11T03:38:14.840496500Z  + iptables-restore
2023-10-11T03:38:14.844251624Z  + sed -e 's/-d 127.0.0.11/-d 192.168.65.254/g' -e 's/-A OUTPUT \(.*\) -j DOCKER_OUTPUT/\0\n-A PREROUTING \1 -j DOCKER_OUTPUT/' -e 's/--to-source :53/--to-source 192.168.65.254:53/g' -e 's/p -j DNAT --to-destination 127.0.0.11/p --dport 53 -j DNAT --to-destination 127.0.0.11/g'
2023-10-11T03:38:14.850275272Z  + cp /etc/resolv.conf /etc/resolv.conf.original
2023-10-11T03:38:14.855838454Z  ++ sed -e s/127.0.0.11/192.168.65.254/g /etc/resolv.conf.original
2023-10-11T03:38:14.857387740Z  + replaced='nameserver 192.168.65.254
2023-10-11T03:38:14.857417035Z  options ndots:0'
2023-10-11T03:38:14.857424824Z  + [[ '' == '' ]]
2023-10-11T03:38:14.857428894Z  + echo 'nameserver 192.168.65.254
2023-10-11T03:38:14.857432748Z  options ndots:0'
2023-10-11T03:38:14.857557495Z  + files_to_update=('/etc/kubernetes/manifests/etcd.yaml' '/etc/kubernetes/manifests/kube-apiserver.yaml' '/etc/kubernetes/manifests/kube-controller-manager.yaml' '/etc/kubernetes/manifests/kube-scheduler.yaml' '/etc/kubernetes/controller-manager.conf' '/etc/kubernetes/scheduler.conf' '/kind/kubeadm.conf' '/var/lib/kubelet/kubeadm-flags.env')
2023-10-11T03:38:14.858738894Z  + local files_to_update
2023-10-11T03:38:14.858798044Z  + local should_fix_certificate=false
2023-10-11T03:38:14.861803576Z  ++ cut '-d ' -f1
2023-10-11T03:38:14.861840578Z  ++ head -n1 /dev/fd/63
2023-10-11T03:38:14.862671183Z  ++++ hostname
2023-10-11T03:38:14.865804973Z  +++ timeout 5 getent ahostsv4 minikube
2023-10-11T03:38:14.870644932Z  + curr_ipv4=192.168.49.2
2023-10-11T03:38:14.870675026Z  + echo 'INFO: Detected IPv4 address: 192.168.49.2'
2023-10-11T03:38:14.870680424Z  INFO: Detected IPv4 address: 192.168.49.2
2023-10-11T03:38:14.870684401Z  + '[' -f /kind/old-ipv4 ']'
2023-10-11T03:38:14.870688194Z  + [[ -n 192.168.49.2 ]]
2023-10-11T03:38:14.870692392Z  + echo -n 192.168.49.2
2023-10-11T03:38:14.872592868Z  ++ cut '-d ' -f1
2023-10-11T03:38:14.872629023Z  ++ head -n1 /dev/fd/63
2023-10-11T03:38:14.873199611Z  ++++ hostname
2023-10-11T03:38:14.874510251Z  +++ timeout 5 getent ahostsv6 minikube
2023-10-11T03:38:14.878921736Z  + curr_ipv6=
2023-10-11T03:38:14.878965016Z  + echo 'INFO: Detected IPv6 address: '
2023-10-11T03:38:14.878970146Z  INFO: Detected IPv6 address: 
2023-10-11T03:38:14.878974736Z  + '[' -f /kind/old-ipv6 ']'
2023-10-11T03:38:14.878978775Z  + [[ -n '' ]]
2023-10-11T03:38:14.878982485Z  + false
2023-10-11T03:38:14.879706321Z  ++ uname -a
2023-10-11T03:38:14.881755400Z  + echo 'entrypoint completed: Linux minikube 5.10.102.1-microsoft-standard-WSL2 #1 SMP Wed Mar 2 00:30:59 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux'
2023-10-11T03:38:14.881793711Z  entrypoint completed: Linux minikube 5.10.102.1-microsoft-standard-WSL2 #1 SMP Wed Mar 2 00:30:59 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
2023-10-11T03:38:14.881818606Z  + exec /sbin/init
2023-10-11T03:38:14.896328483Z  systemd 249.11-0ubuntu3.9 running in system mode (+PAM +AUDIT +SELINUX +APPARMOR +IMA +SMACK +SECCOMP +GCRYPT +GNUTLS +OPENSSL +ACL +BLKID +CURL +ELFUTILS +FIDO2 +IDN2 -IDN +IPTC +KMOD +LIBCRYPTSETUP +LIBFDISK +PCRE2 -PWQUALITY -P11KIT -QRENCODE +BZIP2 +LZ4 +XZ +ZLIB +ZSTD -XKBCOMMON +UTMP +SYSVINIT default-hierarchy=unified)
2023-10-11T03:38:14.896367108Z  Detected virtualization wsl.
2023-10-11T03:38:14.896373929Z  Detected architecture x86-64.
2023-10-11T03:38:14.901728637Z  [0;1;31mFailed to create symlink /sys/fs/cgroup/cpuacct: File exists[0m
2023-10-11T03:38:14.901770724Z  [0;1;31mFailed to create symlink /sys/fs/cgroup/cpu: File exists[0m
2023-10-11T03:38:14.901776257Z  [0;1;31mFailed to create symlink /sys/fs/cgroup/net_cls: File exists[0m
2023-10-11T03:38:14.901780503Z  [0;1;31mFailed to create symlink /sys/fs/cgroup/net_prio: File exists[0m
2023-10-11T03:38:14.902350312Z  
2023-10-11T03:38:14.902374390Z  Welcome to [1mUbuntu 22.04.2 LTS[0m!
2023-10-11T03:38:14.902380207Z  
2023-10-11T03:38:15.012084027Z  Queued start job for default target Graphical Interface.
2023-10-11T03:38:15.056444343Z  [[0;32m  OK  [0m] Created slice [0;1;39mSlice /system/modprobe[0m.
2023-10-11T03:38:15.056514807Z  [[0;32m  OK  [0m] Started [0;1;39mDispatch Password ‚Ä¶ts to Console Directory Watch[0m.
2023-10-11T03:38:15.057505357Z  [[0;32m  OK  [0m] Set up automount [0;1;39mArbitrary‚Ä¶s File System Automount Point[0m.
2023-10-11T03:38:15.057532585Z  [[0;32m  OK  [0m] Reached target [0;1;39mLocal Encrypted Volumes[0m.
2023-10-11T03:38:15.057541256Z  [[0;32m  OK  [0m] Reached target [0;1;39mNetwork is Online[0m.
2023-10-11T03:38:15.057547444Z  [[0;32m  OK  [0m] Reached target [0;1;39mPath Units[0m.
2023-10-11T03:38:15.057930133Z  [[0;32m  OK  [0m] Reached target [0;1;39mSlice Units[0m.
2023-10-11T03:38:15.057951248Z  [[0;32m  OK  [0m] Reached target [0;1;39mSwaps[0m.
2023-10-11T03:38:15.057957281Z  [[0;32m  OK  [0m] Reached target [0;1;39mLocal Verity Protected Volumes[0m.
2023-10-11T03:38:15.058963115Z  [[0;32m  OK  [0m] Listening on [0;1;39mJournal Socket (/dev/log)[0m.
2023-10-11T03:38:15.059115376Z  [[0;32m  OK  [0m] Listening on [0;1;39mJournal Socket[0m.
2023-10-11T03:38:15.064022848Z           Mounting [0;1;39mHuge Pages File System[0m...
2023-10-11T03:38:15.066461064Z           Mounting [0;1;39mKernel Debug File System[0m...
2023-10-11T03:38:15.079171616Z           Mounting [0;1;39mKernel Trace File System[0m...
2023-10-11T03:38:15.105011470Z           Starting [0;1;39mJournal Service[0m...
2023-10-11T03:38:15.114542477Z           Starting [0;1;39mLoad Kernel Module configfs[0m...
2023-10-11T03:38:15.122363603Z           Starting [0;1;39mLoad Kernel Module fuse[0m...
2023-10-11T03:38:15.144461537Z           Starting [0;1;39mRemount Root and Kernel File Systems[0m...
2023-10-11T03:38:15.179165069Z           Starting [0;1;39mApply Kernel Variables[0m...
2023-10-11T03:38:15.182084768Z  [[0;32m  OK  [0m] Mounted [0;1;39mHuge Pages File System[0m.
2023-10-11T03:38:15.182122859Z  [[0;32m  OK  [0m] Mounted [0;1;39mKernel Debug File System[0m.
2023-10-11T03:38:15.182453076Z  [[0;32m  OK  [0m] Mounted [0;1;39mKernel Trace File System[0m.
2023-10-11T03:38:15.182684378Z  modprobe@configfs.service: Deactivated successfully.
2023-10-11T03:38:15.183371851Z  [[0;32m  OK  [0m] Finished [0;1;39mLoad Kernel Module configfs[0m.
2023-10-11T03:38:15.184197957Z  modprobe@fuse.service: Deactivated successfully.
2023-10-11T03:38:15.184863816Z  [[0;32m  OK  [0m] Finished [0;1;39mLoad Kernel Module fuse[0m.
2023-10-11T03:38:15.187396140Z           Mounting [0;1;39mFUSE Control File System[0m...
2023-10-11T03:38:15.189009449Z  [[0;32m  OK  [0m] Finished [0;1;39mRemount Root and Kernel File Systems[0m.
2023-10-11T03:38:15.222002559Z           Starting [0;1;39mCreate System Users[0m...
2023-10-11T03:38:15.278965782Z           Starting [0;1;39mRecord System Boot/Shutdown in UTMP[0m...
2023-10-11T03:38:15.282973636Z  [[0;32m  OK  [0m] Finished [0;1;39mApply Kernel Variables[0m.
2023-10-11T03:38:15.283014757Z  [[0;32m  OK  [0m] Mounted [0;1;39mFUSE Control File System[0m.
2023-10-11T03:38:15.299825681Z  [[0;32m  OK  [0m] Finished [0;1;39mRecord System Boot/Shutdown in UTMP[0m.
2023-10-11T03:38:15.299867966Z  [[0;32m  OK  [0m] Finished [0;1;39mCreate System Users[0m.
2023-10-11T03:38:15.317983282Z           Starting [0;1;39mCreate Static Device Nodes in /dev[0m...
2023-10-11T03:38:15.329027765Z  [[0;32m  OK  [0m] Started [0;1;39mJournal Service[0m.
2023-10-11T03:38:15.334328482Z           Starting [0;1;39mFlush Journal to Persistent Storage[0m...
2023-10-11T03:38:15.350879886Z  [[0;32m  OK  [0m] Finished [0;1;39mCreate Static Device Nodes in /dev[0m.
2023-10-11T03:38:15.350961758Z  [[0;32m  OK  [0m] Finished [0;1;39mFlush Journal to Persistent Storage[0m.
2023-10-11T03:38:15.351264468Z  [[0;32m  OK  [0m] Reached target [0;1;39mPreparation for Local File Systems[0m.
2023-10-11T03:38:15.351612550Z  [[0;32m  OK  [0m] Reached target [0;1;39mLocal File Systems[0m.
2023-10-11T03:38:15.351680738Z  [[0;32m  OK  [0m] Reached target [0;1;39mSystem Initialization[0m.
2023-10-11T03:38:15.351696380Z  [[0;32m  OK  [0m] Started [0;1;39mPodman auto-update timer[0m.
2023-10-11T03:38:15.351701725Z  [[0;32m  OK  [0m] Started [0;1;39mDaily Cleanup of Temporary Directories[0m.
2023-10-11T03:38:15.351706410Z  [[0;32m  OK  [0m] Reached target [0;1;39mTimer Units[0m.
2023-10-11T03:38:15.351710676Z  [[0;32m  OK  [0m] Listening on [0;1;39mBuildKit[0m.
2023-10-11T03:38:15.354282229Z           Starting [0;1;39mDocker Socket for the API[0m...
2023-10-11T03:38:15.372739478Z           Starting [0;1;39mPodman API Socket[0m...
2023-10-11T03:38:15.373561473Z  [[0;32m  OK  [0m] Listening on [0;1;39mDocker Socket for the API[0m.
2023-10-11T03:38:15.373601340Z  [[0;32m  OK  [0m] Listening on [0;1;39mPodman API Socket[0m.
2023-10-11T03:38:15.373940135Z  [[0;32m  OK  [0m] Reached target [0;1;39mSocket Units[0m.
2023-10-11T03:38:15.373956687Z  [[0;32m  OK  [0m] Reached target [0;1;39mBasic System[0m.
2023-10-11T03:38:15.377732514Z           Starting [0;1;39mcontainerd container runtime[0m...
2023-10-11T03:38:15.380598481Z           Starting [0;1;39mminikube automount[0m...
2023-10-11T03:38:15.397225514Z           Starting [0;1;39mPodman auto-update service[0m...
2023-10-11T03:38:15.411075936Z           Starting [0;1;39mPodman Start All ‚Ä¶estart Policy Set To Always[0m...
2023-10-11T03:38:15.416475481Z           Starting [0;1;39mPodman API Service[0m...
2023-10-11T03:38:15.440803650Z           Starting [0;1;39mOpenBSD Secure Shell server[0m...
2023-10-11T03:38:15.447166884Z  [[0;32m  OK  [0m] Finished [0;1;39mminikube automount[0m.
2023-10-11T03:38:15.447783135Z  [[0;32m  OK  [0m] Started [0;1;39mPodman API Service[0m.
2023-10-11T03:38:15.525970442Z  [[0;32m  OK  [0m] Started [0;1;39mOpenBSD Secure Shell server[0m.
2023-10-11T03:38:16.231819654Z  [[0;32m  OK  [0m] Started [0;1;39mcontainerd container runtime[0m.
2023-10-11T03:38:16.231857775Z           Starting [0;1;39mDocker Application Container Engine[0m...
2023-10-11T03:38:16.487002089Z  [[0;32m  OK  [0m] Finished [0;1;39mPodman Start All ‚Ä¶ Restart Policy Set To Always[0m.
2023-10-11T03:38:16.678971747Z  [[0;32m  OK  [0m] Finished [0;1;39mPodman auto-update service[0m.
2023-10-11T03:38:17.376610478Z  [[0;32m  OK  [0m] Started [0;1;39mDocker Application Container Engine[0m.
2023-10-11T03:38:17.377213350Z  [[0;32m  OK  [0m] Reached target [0;1;39mMulti-User System[0m.
2023-10-11T03:38:17.377241475Z  [[0;32m  OK  [0m] Reached target [0;1;39mGraphical Interface[0m.
2023-10-11T03:38:17.380124422Z           Starting [0;1;39mRecord Runlevel Change in UTMP[0m...
2023-10-11T03:38:17.390398712Z  [[0;32m  OK  [0m] Finished [0;1;39mRecord Runlevel Change in UTMP[0m.

-- /stdout --
I1010 22:47:44.241109   29740 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1010 22:49:05.747043   29740 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1m21.5057136s)
I1010 22:49:05.747555   29740 info.go:266] docker info: {ID:a11dda2e-aa4d-4b68-a218-310732b17591 Containers:45 ContainersRunning:38 ContainersPaused:0 ContainersStopped:7 Images:29 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:207 OomKillDisable:true NGoroutines:200 SystemTime:2023-10-11 03:49:05.416159933 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.10.102.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:6101708800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I1010 22:49:05.747680   29740 errors.go:98] postmortem docker info: {ID:a11dda2e-aa4d-4b68-a218-310732b17591 Containers:45 ContainersRunning:38 ContainersPaused:0 ContainersStopped:7 Images:29 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:207 OomKillDisable:true NGoroutines:200 SystemTime:2023-10-11 03:49:05.416159933 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.10.102.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:6101708800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I1010 22:49:05.999580   29740 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I1010 22:49:05.999580   29740 cli_runner.go:164] Run: docker network inspect minikube
I1010 22:49:07.061198   29740 cli_runner.go:217] Completed: docker network inspect minikube: (1.0616146s)
I1010 22:49:07.061198   29740 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[
    {
        "Name": "minikube",
        "Id": "26d577b4fe054d54ba5558ff6367eb73303c66a07b47a47062ba51445db5f607",
        "Created": "2023-10-11T03:37:33.624629145Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "192.168.49.0/24",
                    "Gateway": "192.168.49.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {},
        "Options": {
            "--icc": "",
            "--ip-masq": "",
            "com.docker.network.driver.mtu": "1500"
        },
        "Labels": {
            "created_by.minikube.sigs.k8s.io": "true",
            "name.minikube.sigs.k8s.io": "minikube"
        }
    }
]

-- /stdout --
I1010 22:49:07.110284   29740 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1010 22:49:12.646690   29740 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (5.5363907s)
I1010 22:49:12.646690   29740 info.go:266] docker info: {ID:a11dda2e-aa4d-4b68-a218-310732b17591 Containers:45 ContainersRunning:38 ContainersPaused:0 ContainersStopped:7 Images:29 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:205 OomKillDisable:true NGoroutines:196 SystemTime:2023-10-11 03:49:12.324538794 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:11 KernelVersion:5.10.102.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:6101708800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I1010 22:49:13.029232   29740 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1010 22:49:13.086865   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1010 22:49:14.600078   29740 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
I1010 22:49:14.600078   29740 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.5132091s)
I1010 22:49:14.669323   29740 retry.go:31] will retry after 272.111367ms: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
I1010 22:49:15.021846   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1010 22:49:16.139292   29740 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
I1010 22:49:16.139292   29740 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.1174431s)
W1010 22:49:16.166467   29740 start.go:275] error running df -h /var: NewSession: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
W1010 22:49:16.167134   29740 start.go:242] error getting percentage of /var that is free: NewSession: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
I1010 22:49:16.269241   29740 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1010 22:49:16.337720   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1010 22:49:17.550016   29740 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
I1010 22:49:17.550016   29740 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.2122926s)
I1010 22:49:17.550016   29740 retry.go:31] will retry after 214.848112ms: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
I1010 22:49:17.820075   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1010 22:49:18.890050   29740 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube returned with exit code 1
I1010 22:49:18.890050   29740 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.0699727s)
W1010 22:49:18.890050   29740 start.go:290] error running df -BG /var: NewSession: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
W1010 22:49:18.890050   29740 start.go:247] error getting GiB of /var that is available: NewSession: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: unable to inspect a not running container to get SSH port
I1010 22:49:18.893268   29740 fix.go:56] fixHost completed within 1m40.0914976s
I1010 22:49:18.893268   29740 start.go:83] releasing machines lock for "minikube", held for 1m40.0972412s
W1010 22:49:18.895177   29740 start.go:672] error starting host: driver start: start: docker start minikube: exit status 1
stdout:

stderr:
Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write "a *:* rwm": write /sys/fs/cgroup/devices/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802/devices.allow: invalid argument: unknown
Error: failed to start containers: minikube
W1010 22:49:18.896255   29740 out.go:239] ü§¶  StartHost failed, but will try again: driver start: start: docker start minikube: exit status 1
stdout:

stderr:
Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write "a *:* rwm": write /sys/fs/cgroup/devices/docker/078fdaa2e1d11aed85164f4b2ce4cacce73fbc4fae50dc5a3af305568898f802/devices.allow: invalid argument: unknown
Error: failed to start containers: minikube

I1010 22:49:18.899169   29740 start.go:687] Will try again in 5 seconds ...
I1010 22:49:23.911014   29740 start.go:365] acquiring machines lock for minikube: {Name:mk1f4826b908553e9bc66fb08644c23b5c53aa71 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1010 22:49:23.911215   29740 start.go:369] acquired machines lock for "minikube" in 200.8¬µs
I1010 22:49:24.174979   29740 start.go:96] Skipping create...Using existing machine configuration
I1010 22:49:24.175704   29740 fix.go:54] fixHost starting: 
I1010 22:49:24.339389   29740 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1010 22:49:25.175969   29740 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1010 22:49:25.175969   29740 fix.go:128] unexpected machine state, will restart: <nil>
I1010 22:49:25.176971   29740 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I1010 22:49:25.226808   29740 cli_runner.go:164] Run: docker start minikube
I1010 22:49:26.749199   29740 cli_runner.go:217] Completed: docker start minikube: (1.522387s)
I1010 22:49:26.807271   29740 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1010 22:49:28.093388   29740 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.2861138s)
I1010 22:49:28.098903   29740 kic.go:426] container "minikube" state is running.
I1010 22:49:28.239193   29740 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1010 22:49:29.896261   29740 cli_runner.go:217] Completed: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube: (1.6570638s)
I1010 22:49:29.969330   29740 profile.go:148] Saving config to C:\Users\LENOVO\.minikube\profiles\minikube\config.json ...
I1010 22:49:30.100552   29740 machine.go:88] provisioning docker machine ...
I1010 22:49:30.134221   29740 ubuntu.go:169] provisioning hostname "minikube"
I1010 22:49:30.249676   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1010 22:49:31.468927   29740 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.2192482s)
I1010 22:49:31.595867   29740 main.go:141] libmachine: Using SSH client type: native
I1010 22:49:31.850129   29740 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa69400] 0xa6c2a0 <nil>  [] 0s} 127.0.0.1 58043 <nil> <nil>}
I1010 22:49:31.850129   29740 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1010 22:49:32.301247   29740 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1010 22:49:32.333284   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1010 22:49:32.940047   29740 main.go:141] libmachine: Using SSH client type: native
I1010 22:49:32.941121   29740 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa69400] 0xa6c2a0 <nil>  [] 0s} 127.0.0.1 58043 <nil> <nil>}
I1010 22:49:32.941121   29740 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1010 22:49:33.138892   29740 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1010 22:49:33.140011   29740 ubuntu.go:175] set auth options {CertDir:C:\Users\LENOVO\.minikube CaCertPath:C:\Users\LENOVO\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\LENOVO\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\LENOVO\.minikube\machines\server.pem ServerKeyPath:C:\Users\LENOVO\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\LENOVO\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\LENOVO\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\LENOVO\.minikube}
I1010 22:49:33.140011   29740 ubuntu.go:177] setting up certificates
I1010 22:49:33.140011   29740 provision.go:83] configureAuth start
I1010 22:49:33.183328   29740 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1010 22:49:33.859076   29740 provision.go:138] copyHostCerts
I1010 22:49:33.879477   29740 exec_runner.go:144] found C:\Users\LENOVO\.minikube/ca.pem, removing ...
I1010 22:49:33.879983   29740 exec_runner.go:203] rm: C:\Users\LENOVO\.minikube\ca.pem
I1010 22:49:33.880511   29740 exec_runner.go:151] cp: C:\Users\LENOVO\.minikube\certs\ca.pem --> C:\Users\LENOVO\.minikube/ca.pem (1078 bytes)
I1010 22:49:33.923475   29740 exec_runner.go:144] found C:\Users\LENOVO\.minikube/cert.pem, removing ...
I1010 22:49:33.923475   29740 exec_runner.go:203] rm: C:\Users\LENOVO\.minikube\cert.pem
I1010 22:49:33.924041   29740 exec_runner.go:151] cp: C:\Users\LENOVO\.minikube\certs\cert.pem --> C:\Users\LENOVO\.minikube/cert.pem (1123 bytes)
I1010 22:49:33.954285   29740 exec_runner.go:144] found C:\Users\LENOVO\.minikube/key.pem, removing ...
I1010 22:49:33.954285   29740 exec_runner.go:203] rm: C:\Users\LENOVO\.minikube\key.pem
I1010 22:49:33.955334   29740 exec_runner.go:151] cp: C:\Users\LENOVO\.minikube\certs\key.pem --> C:\Users\LENOVO\.minikube/key.pem (1675 bytes)
I1010 22:49:33.960008   29740 provision.go:112] generating server cert: C:\Users\LENOVO\.minikube\machines\server.pem ca-key=C:\Users\LENOVO\.minikube\certs\ca.pem private-key=C:\Users\LENOVO\.minikube\certs\ca-key.pem org=LENOVO.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1010 22:49:34.165598   29740 provision.go:172] copyRemoteCerts
I1010 22:49:34.256819   29740 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1010 22:49:34.308100   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1010 22:49:34.962877   29740 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58043 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I1010 22:49:35.076745   29740 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1010 22:49:35.124816   29740 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\machines\server.pem --> /etc/docker/server.pem (1200 bytes)
I1010 22:49:35.168016   29740 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1010 22:49:35.202844   29740 provision.go:86] duration metric: configureAuth took 2.0628273s
I1010 22:49:35.202844   29740 ubuntu.go:193] setting minikube options for container-runtime
I1010 22:49:35.204431   29740 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1010 22:49:35.241491   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1010 22:49:35.830314   29740 main.go:141] libmachine: Using SSH client type: native
I1010 22:49:35.831383   29740 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa69400] 0xa6c2a0 <nil>  [] 0s} 127.0.0.1 58043 <nil> <nil>}
I1010 22:49:35.831383   29740 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1010 22:49:36.019776   29740 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1010 22:49:36.019776   29740 ubuntu.go:71] root file system type: overlay
I1010 22:49:36.021412   29740 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1010 22:49:36.069818   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1010 22:49:36.772054   29740 main.go:141] libmachine: Using SSH client type: native
I1010 22:49:36.773133   29740 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa69400] 0xa6c2a0 <nil>  [] 0s} 127.0.0.1 58043 <nil> <nil>}
I1010 22:49:36.773133   29740 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1010 22:49:36.982776   29740 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1010 22:49:37.034540   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1010 22:49:37.869587   29740 main.go:141] libmachine: Using SSH client type: native
I1010 22:49:37.870676   29740 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa69400] 0xa6c2a0 <nil>  [] 0s} 127.0.0.1 58043 <nil> <nil>}
I1010 22:49:37.870676   29740 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1010 22:49:38.061847   29740 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1010 22:49:38.061847   29740 machine.go:91] provisioned docker machine in 7.9612741s
I1010 22:49:38.063548   29740 start.go:300] post-start starting for "minikube" (driver="docker")
I1010 22:49:38.076299   29740 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1010 22:49:38.150940   29740 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1010 22:49:38.200627   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1010 22:49:39.091514   29740 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58043 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I1010 22:49:39.168200   29740 ssh_runner.go:235] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (1.0172579s)
I1010 22:49:39.234783   29740 ssh_runner.go:195] Run: cat /etc/os-release
I1010 22:49:39.261380   29740 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1010 22:49:39.261380   29740 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1010 22:49:39.261380   29740 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1010 22:49:39.261380   29740 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I1010 22:49:39.261892   29740 filesync.go:126] Scanning C:\Users\LENOVO\.minikube\addons for local assets ...
I1010 22:49:39.263598   29740 filesync.go:126] Scanning C:\Users\LENOVO\.minikube\files for local assets ...
I1010 22:49:39.264156   29740 start.go:303] post-start completed in 1.2006044s
I1010 22:49:39.318496   29740 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1010 22:49:39.354078   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1010 22:49:40.064794   29740 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58043 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I1010 22:49:40.231382   29740 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1010 22:49:40.240610   29740 fix.go:56] fixHost completed within 16.0648633s
I1010 22:49:40.240610   29740 start.go:83] releasing machines lock for "minikube", held for 16.3293515s
I1010 22:49:40.274488   29740 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1010 22:49:40.900489   29740 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1010 22:49:40.940795   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1010 22:49:40.966059   29740 ssh_runner.go:195] Run: cat /version.json
I1010 22:49:41.004151   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1010 22:49:41.559601   29740 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58043 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I1010 22:49:41.944536   29740 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58043 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I1010 22:49:43.761495   29740 ssh_runner.go:235] Completed: cat /version.json: (2.7954283s)
I1010 22:49:43.764248   29740 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.8637513s)
W1010 22:49:43.780750   29740 start.go:815] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Operation timed out after 2001 milliseconds with 0 bytes received
W1010 22:49:43.781264   29740 out.go:239] ‚ùó  This container is having trouble accessing https://registry.k8s.io
W1010 22:49:43.781511   29740 out.go:239] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1010 22:49:43.883281   29740 ssh_runner.go:195] Run: systemctl --version
I1010 22:49:43.950836   29740 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1010 22:49:44.017087   29740 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1010 22:49:44.035597   29740 start.go:410] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1010 22:49:44.097435   29740 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1010 22:49:44.114963   29740 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1010 22:49:44.133778   29740 start.go:466] detecting cgroup driver to use...
I1010 22:49:44.133778   29740 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1010 22:49:44.166553   29740 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1010 22:49:44.248582   29740 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1010 22:49:44.343708   29740 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1010 22:49:44.362376   29740 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1010 22:49:44.412386   29740 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1010 22:49:44.480796   29740 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1010 22:49:44.555272   29740 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1010 22:49:44.633743   29740 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1010 22:49:44.708268   29740 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1010 22:49:44.787579   29740 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1010 22:49:44.887359   29740 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1010 22:49:44.964504   29740 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1010 22:49:45.029858   29740 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1010 22:49:45.201091   29740 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1010 22:49:45.372643   29740 start.go:466] detecting cgroup driver to use...
I1010 22:49:45.372643   29740 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1010 22:49:45.442322   29740 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1010 22:49:45.479896   29740 cruntime.go:276] skipping containerd shutdown because we are bound to it
I1010 22:49:45.535445   29740 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1010 22:49:45.563404   29740 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1010 22:49:45.644849   29740 ssh_runner.go:195] Run: which cri-dockerd
I1010 22:49:45.702506   29740 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1010 22:49:45.716787   29740 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1010 22:49:45.801964   29740 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1010 22:49:46.033454   29740 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1010 22:49:46.159787   29740 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I1010 22:49:46.159787   29740 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I1010 22:49:46.239480   29740 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1010 22:49:46.428055   29740 ssh_runner.go:195] Run: sudo systemctl restart docker
I1010 22:49:46.943792   29740 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1010 22:49:47.166670   29740 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1010 22:49:47.419159   29740 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1010 22:49:47.674105   29740 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1010 22:49:47.905677   29740 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1010 22:49:48.004504   29740 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1010 22:49:48.285861   29740 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1010 22:49:48.750697   29740 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1010 22:49:48.835978   29740 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1010 22:49:48.848831   29740 start.go:534] Will wait 60s for crictl version
I1010 22:49:48.943480   29740 ssh_runner.go:195] Run: which crictl
I1010 22:49:49.034219   29740 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1010 22:49:49.305316   29740 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I1010 22:49:49.363828   29740 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1010 22:49:49.575743   29740 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1010 22:49:49.689314   29740 out.go:204] üê≥  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I1010 22:49:49.750689   29740 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1010 22:49:51.092891   29740 cli_runner.go:217] Completed: docker exec -t minikube dig +short host.docker.internal: (1.3421979s)
I1010 22:49:51.093380   29740 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1010 22:49:51.157523   29740 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1010 22:49:51.172045   29740 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1010 22:49:51.258798   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1010 22:49:52.018238   29740 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1010 22:49:52.050366   29740 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1010 22:49:52.084086   29740 docker.go:636] Got preloaded images: -- stdout --
mongo:latest
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1010 22:49:52.084086   29740 docker.go:566] Images already preloaded, skipping extraction
I1010 22:49:52.131926   29740 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1010 22:49:52.179594   29740 docker.go:636] Got preloaded images: -- stdout --
mongo:latest
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1010 22:49:52.233442   29740 cache_images.go:84] Images are preloaded, skipping loading
I1010 22:49:52.347301   29740 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1010 22:49:52.640558   29740 cni.go:84] Creating CNI manager for ""
I1010 22:49:52.640558   29740 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1010 22:49:52.641086   29740 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1010 22:49:52.641628   29740 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1010 22:49:52.667463   29740 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1010 22:49:52.684358   29740 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1010 22:49:52.746890   29740 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I1010 22:49:52.775027   29740 binaries.go:44] Found k8s binaries, skipping transfer
I1010 22:49:52.833718   29740 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1010 22:49:52.851846   29740 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1010 22:49:52.886633   29740 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1010 22:49:52.921485   29740 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1010 22:49:53.011655   29740 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1010 22:49:53.017814   29740 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1010 22:49:53.065020   29740 certs.go:56] Setting up C:\Users\LENOVO\.minikube\profiles\minikube for IP: 192.168.49.2
I1010 22:49:53.065020   29740 certs.go:190] acquiring lock for shared ca certs: {Name:mke9df2fc6a4f59b06fbb05c903dcdb65dec22b7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1010 22:49:53.067008   29740 certs.go:199] skipping minikubeCA CA generation: C:\Users\LENOVO\.minikube\ca.key
I1010 22:49:53.068642   29740 certs.go:199] skipping proxyClientCA CA generation: C:\Users\LENOVO\.minikube\proxy-client-ca.key
I1010 22:49:53.070275   29740 certs.go:315] skipping minikube-user signed cert generation: C:\Users\LENOVO\.minikube\profiles\minikube\client.key
I1010 22:49:53.071285   29740 certs.go:315] skipping minikube signed cert generation: C:\Users\LENOVO\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I1010 22:49:53.072613   29740 certs.go:315] skipping aggregator signed cert generation: C:\Users\LENOVO\.minikube\profiles\minikube\proxy-client.key
I1010 22:49:53.100806   29740 certs.go:437] found cert: C:\Users\LENOVO\.minikube\certs\C:\Users\LENOVO\.minikube\certs\ca-key.pem (1679 bytes)
I1010 22:49:53.102316   29740 certs.go:437] found cert: C:\Users\LENOVO\.minikube\certs\C:\Users\LENOVO\.minikube\certs\ca.pem (1078 bytes)
I1010 22:49:53.102316   29740 certs.go:437] found cert: C:\Users\LENOVO\.minikube\certs\C:\Users\LENOVO\.minikube\certs\cert.pem (1123 bytes)
I1010 22:49:53.103328   29740 certs.go:437] found cert: C:\Users\LENOVO\.minikube\certs\C:\Users\LENOVO\.minikube\certs\key.pem (1675 bytes)
I1010 22:49:53.124929   29740 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1010 22:49:53.178164   29740 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1010 22:49:53.233004   29740 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1010 22:49:53.279299   29740 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1010 22:49:53.345517   29740 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1010 22:49:53.479419   29740 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1010 22:49:53.603997   29740 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1010 22:49:53.701617   29740 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1010 22:49:53.783579   29740 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1010 22:49:53.857148   29740 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1010 22:49:54.065622   29740 ssh_runner.go:195] Run: openssl version
I1010 22:49:54.212657   29740 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1010 22:49:54.347097   29740 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1010 22:49:54.356342   29740 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Oct 10 04:00 /usr/share/ca-certificates/minikubeCA.pem
I1010 22:49:54.491218   29740 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1010 22:49:54.634856   29740 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1010 22:49:54.820906   29740 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1010 22:49:54.948022   29740 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1010 22:49:55.104135   29740 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1010 22:49:55.211894   29740 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1010 22:49:55.344850   29740 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1010 22:49:55.437469   29740 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1010 22:49:55.569103   29740 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1010 22:49:55.594037   29740 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\LENOVO:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1010 22:49:55.644990   29740 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1010 22:49:55.789364   29740 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1010 22:49:55.813628   29740 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1010 22:49:55.814140   29740 kubeadm.go:636] restartCluster start
I1010 22:49:55.890015   29740 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1010 22:49:55.908167   29740 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1010 22:49:55.942987   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1010 22:49:56.705174   29740 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:54542"
I1010 22:49:56.705708   29740 kubeconfig.go:135] verify returned: got: 127.0.0.1:54542, want: 127.0.0.1:58048
I1010 22:49:56.708421   29740 lock.go:35] WriteFile acquiring C:\Users\LENOVO\.kube\config: {Name:mkbb0d5237f4fb5206829a6920ada974ed50112f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1010 22:49:56.825976   29740 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1010 22:49:56.855527   29740 api_server.go:166] Checking apiserver status ...
I1010 22:49:56.922178   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:49:56.955279   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:49:56.955279   29740 api_server.go:166] Checking apiserver status ...
I1010 22:49:57.026299   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:49:57.049538   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:49:57.550039   29740 api_server.go:166] Checking apiserver status ...
I1010 22:49:57.623172   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:49:57.642365   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:49:58.061517   29740 api_server.go:166] Checking apiserver status ...
I1010 22:49:58.178822   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:49:58.211751   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:49:58.555592   29740 api_server.go:166] Checking apiserver status ...
I1010 22:49:58.647668   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:49:58.672875   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:49:59.064178   29740 api_server.go:166] Checking apiserver status ...
I1010 22:49:59.179859   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:49:59.210020   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:49:59.555537   29740 api_server.go:166] Checking apiserver status ...
I1010 22:49:59.649385   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:49:59.687128   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:00.063165   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:00.148915   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:00.184195   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:00.553782   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:00.735600   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:00.801780   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:01.069612   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:01.193026   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:01.252773   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:01.552870   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:01.669544   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:01.709246   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:02.061019   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:02.152171   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:02.178005   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:02.565014   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:02.683239   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:02.710853   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:03.057409   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:03.194034   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:03.229002   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:03.553093   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:03.641517   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:03.671867   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:04.062518   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:04.134111   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:04.155294   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:04.562402   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:04.718812   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:04.764671   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:05.065061   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:05.211578   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:05.232533   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:05.556834   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:05.650106   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:05.669709   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:06.053756   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:06.110981   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:06.128153   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:06.563782   29740 api_server.go:166] Checking apiserver status ...
I1010 22:50:06.619380   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1010 22:50:06.638475   29740 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1010 22:50:06.856890   29740 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I1010 22:50:06.857503   29740 kubeadm.go:1128] stopping kube-system containers ...
I1010 22:50:06.885502   29740 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1010 22:50:06.917374   29740 docker.go:462] Stopping containers: [b6fe8ea8d568 8d56242589c6 4afca4381570 8af8c2c2a7f0 b8632a921bc3 3b1da451fa09 f17eccf524bb 07c7a87e577f 11ed290a628c 669c8be5e863 528bef7d2777 a23130948184 335931d53376 badc2b349185 2613118c2fbf]
I1010 22:50:07.439043   29740 ssh_runner.go:195] Run: docker stop b6fe8ea8d568 8d56242589c6 4afca4381570 8af8c2c2a7f0 b8632a921bc3 3b1da451fa09 f17eccf524bb 07c7a87e577f 11ed290a628c 669c8be5e863 528bef7d2777 a23130948184 335931d53376 badc2b349185 2613118c2fbf
I1010 22:50:07.509192   29740 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1010 22:50:07.563259   29740 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1010 22:50:08.644057   29740 ssh_runner.go:235] Completed: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: (1.080795s)
I1010 22:50:08.644057   29740 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Oct 11 03:39 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Oct 11 03:39 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Oct 11 03:39 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Oct 11 03:39 /etc/kubernetes/scheduler.conf

I1010 22:50:08.711369   29740 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1010 22:50:08.809827   29740 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1010 22:50:08.904707   29740 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1010 22:50:08.919470   29740 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1010 22:50:08.982850   29740 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1010 22:50:09.079299   29740 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1010 22:50:09.102152   29740 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1010 22:50:09.191230   29740 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1010 22:50:09.287913   29740 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1010 22:50:09.311080   29740 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1010 22:50:09.311080   29740 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1010 22:50:09.587802   29740 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1010 22:50:10.585143   29740 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1010 22:50:10.890720   29740 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1010 22:50:11.019494   29740 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1010 22:50:11.109848   29740 api_server.go:52] waiting for apiserver process to appear ...
I1010 22:50:11.167293   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1010 22:50:11.257347   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1010 22:50:11.900250   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1010 22:50:12.403756   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1010 22:50:12.934585   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1010 22:50:13.386455   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1010 22:50:13.484462   29740 api_server.go:72] duration metric: took 2.374368s to wait for apiserver process to appear ...
I1010 22:50:13.484462   29740 api_server.go:88] waiting for apiserver healthz status ...
I1010 22:50:13.503628   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:13.566850   29740 api_server.go:269] stopped: https://127.0.0.1:58048/healthz: Get "https://127.0.0.1:58048/healthz": EOF
I1010 22:50:13.567361   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:13.571178   29740 api_server.go:269] stopped: https://127.0.0.1:58048/healthz: Get "https://127.0.0.1:58048/healthz": EOF
I1010 22:50:14.071789   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:14.075785   29740 api_server.go:269] stopped: https://127.0.0.1:58048/healthz: Get "https://127.0.0.1:58048/healthz": EOF
I1010 22:50:14.576940   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:14.584850   29740 api_server.go:269] stopped: https://127.0.0.1:58048/healthz: Get "https://127.0.0.1:58048/healthz": EOF
I1010 22:50:15.085931   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:20.097095   29740 api_server.go:269] stopped: https://127.0.0.1:58048/healthz: Get "https://127.0.0.1:58048/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1010 22:50:20.097649   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:21.615882   29740 api_server.go:279] https://127.0.0.1:58048/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1010 22:50:21.629445   29740 api_server.go:103] status: https://127.0.0.1:58048/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1010 22:50:21.629445   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:21.701583   29740 api_server.go:279] https://127.0.0.1:58048/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1010 22:50:21.701583   29740 api_server.go:103] status: https://127.0.0.1:58048/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1010 22:50:22.075875   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:22.183041   29740 api_server.go:279] https://127.0.0.1:58048/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1010 22:50:22.183041   29740 api_server.go:103] status: https://127.0.0.1:58048/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1010 22:50:22.571889   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:22.680168   29740 api_server.go:279] https://127.0.0.1:58048/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1010 22:50:22.680168   29740 api_server.go:103] status: https://127.0.0.1:58048/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1010 22:50:23.084701   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:23.187164   29740 api_server.go:279] https://127.0.0.1:58048/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1010 22:50:23.187164   29740 api_server.go:103] status: https://127.0.0.1:58048/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1010 22:50:23.579486   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:23.602032   29740 api_server.go:279] https://127.0.0.1:58048/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1010 22:50:23.623742   29740 api_server.go:103] status: https://127.0.0.1:58048/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1010 22:50:24.072293   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:24.089057   29740 api_server.go:279] https://127.0.0.1:58048/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1010 22:50:24.089576   29740 api_server.go:103] status: https://127.0.0.1:58048/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1010 22:50:24.582884   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:24.609503   29740 api_server.go:279] https://127.0.0.1:58048/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1010 22:50:24.609503   29740 api_server.go:103] status: https://127.0.0.1:58048/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1010 22:50:25.080775   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:25.099643   29740 api_server.go:279] https://127.0.0.1:58048/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1010 22:50:25.099643   29740 api_server.go:103] status: https://127.0.0.1:58048/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1010 22:50:25.578506   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:25.596832   29740 api_server.go:279] https://127.0.0.1:58048/healthz returned 200:
ok
I1010 22:50:25.842649   29740 api_server.go:141] control plane version: v1.27.4
I1010 22:50:25.842649   29740 api_server.go:131] duration metric: took 12.3581538s to wait for apiserver health ...
I1010 22:50:25.842649   29740 cni.go:84] Creating CNI manager for ""
I1010 22:50:25.842649   29740 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1010 22:50:25.843653   29740 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I1010 22:50:25.908793   29740 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1010 22:50:26.200126   29740 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1010 22:50:26.425363   29740 system_pods.go:43] waiting for kube-system pods to appear ...
I1010 22:50:26.952634   29740 system_pods.go:59] 7 kube-system pods found
I1010 22:50:26.952634   29740 system_pods.go:61] "coredns-5d78c9869d-rgt2n" [8e77c0eb-fb1f-465d-b114-962e7a21dcf5] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1010 22:50:26.952634   29740 system_pods.go:61] "etcd-minikube" [7e4e9cc9-b3dd-43ad-87f0-c11e62c78c5d] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1010 22:50:26.952634   29740 system_pods.go:61] "kube-apiserver-minikube" [d87b85d5-ad7a-4e7c-83e2-d73ad9ee58a5] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1010 22:50:26.952634   29740 system_pods.go:61] "kube-controller-manager-minikube" [df229e42-4a88-4183-9696-5c4b0d9d86c5] Running
I1010 22:50:26.952634   29740 system_pods.go:61] "kube-proxy-mkmvg" [709ec71d-6020-49b1-82e7-be053892893a] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1010 22:50:26.952634   29740 system_pods.go:61] "kube-scheduler-minikube" [c436a8a4-cef3-456e-afe9-f52db56b1efe] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1010 22:50:26.952634   29740 system_pods.go:61] "storage-provisioner" [f509436d-32fe-4c91-8f5a-58fab04f4055] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1010 22:50:26.952634   29740 system_pods.go:74] duration metric: took 527.2692ms to wait for pod list to return data ...
I1010 22:50:26.952634   29740 node_conditions.go:102] verifying NodePressure condition ...
I1010 22:50:27.063250   29740 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I1010 22:50:27.063250   29740 node_conditions.go:123] node cpu capacity is 8
I1010 22:50:27.063758   29740 node_conditions.go:105] duration metric: took 111.1242ms to run NodePressure ...
I1010 22:50:27.064770   29740 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1010 22:50:28.201652   29740 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.1368785s)
I1010 22:50:28.201652   29740 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1010 22:50:28.216795   29740 ops.go:34] apiserver oom_adj: -16
I1010 22:50:28.216795   29740 kubeadm.go:640] restartCluster took 32.4025674s
I1010 22:50:28.216795   29740 kubeadm.go:406] StartCluster complete in 32.6232394s
I1010 22:50:28.231374   29740 settings.go:142] acquiring lock: {Name:mk7cc598f23abbc1dd8bb11a8c1e633e312702c8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1010 22:50:28.231374   29740 settings.go:150] Updating kubeconfig:  C:\Users\LENOVO\.kube\config
I1010 22:50:28.235231   29740 lock.go:35] WriteFile acquiring C:\Users\LENOVO\.kube\config: {Name:mkbb0d5237f4fb5206829a6920ada974ed50112f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1010 22:50:28.238458   29740 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1010 22:50:28.239558   29740 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1010 22:50:28.241168   29740 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I1010 22:50:28.262978   29740 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1010 22:50:28.262978   29740 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W1010 22:50:28.262978   29740 addons.go:240] addon storage-provisioner should already be in state true
I1010 22:50:28.262978   29740 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1010 22:50:28.262978   29740 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1010 22:50:28.264342   29740 host.go:66] Checking if "minikube" exists ...
I1010 22:50:28.308359   29740 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1010 22:50:28.354525   29740 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1010 22:50:28.355502   29740 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1010 22:50:28.355502   29740 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I1010 22:50:28.358739   29740 out.go:177] üîé  Verifying Kubernetes components...
I1010 22:50:28.512008   29740 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1010 22:50:30.071741   29740 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.833278s)
I1010 22:50:30.075161   29740 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1.5626039s)
I1010 22:50:30.093210   29740 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1010 22:50:30.187535   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
W1010 22:50:30.462727   29740 out.go:239] ‚ùó  Executing "docker container inspect minikube --format={{.State.Status}}" took an unusually long time: 2.0723068s
W1010 22:50:30.464405   29740 out.go:239] üí°  Restarting the docker service may improve performance.
I1010 22:50:30.474101   29740 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (2.0723068s)
I1010 22:50:30.681130   29740 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (2.3246221s)
I1010 22:50:30.745462   29740 addons.go:231] Setting addon default-storageclass=true in "minikube"
W1010 22:50:30.745462   29740 addons.go:240] addon default-storageclass should already be in state true
I1010 22:50:30.745462   29740 host.go:66] Checking if "minikube" exists ...
I1010 22:50:30.835786   29740 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1010 22:50:30.966862   29740 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1010 22:50:31.121198   29740 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1010 22:50:31.121198   29740 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1010 22:50:31.200321   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1010 22:50:32.503431   29740 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube: (2.3158894s)
I1010 22:50:32.503431   29740 api_server.go:52] waiting for apiserver process to appear ...
I1010 22:50:32.642531   29740 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1010 22:50:33.118452   29740 api_server.go:72] duration metric: took 4.7629371s to wait for apiserver process to appear ...
I1010 22:50:33.118452   29740 api_server.go:88] waiting for apiserver healthz status ...
I1010 22:50:33.118452   29740 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58048/healthz ...
I1010 22:50:33.204333   29740 api_server.go:279] https://127.0.0.1:58048/healthz returned 200:
ok
I1010 22:50:33.241356   29740 api_server.go:141] control plane version: v1.27.4
I1010 22:50:33.241356   29740 api_server.go:131] duration metric: took 122.9035ms to wait for apiserver health ...
I1010 22:50:33.241356   29740 system_pods.go:43] waiting for kube-system pods to appear ...
I1010 22:50:33.308642   29740 system_pods.go:59] 7 kube-system pods found
I1010 22:50:33.308642   29740 system_pods.go:61] "coredns-5d78c9869d-rgt2n" [8e77c0eb-fb1f-465d-b114-962e7a21dcf5] Running
I1010 22:50:33.308642   29740 system_pods.go:61] "etcd-minikube" [7e4e9cc9-b3dd-43ad-87f0-c11e62c78c5d] Running
I1010 22:50:33.308642   29740 system_pods.go:61] "kube-apiserver-minikube" [d87b85d5-ad7a-4e7c-83e2-d73ad9ee58a5] Running
I1010 22:50:33.308642   29740 system_pods.go:61] "kube-controller-manager-minikube" [df229e42-4a88-4183-9696-5c4b0d9d86c5] Running
I1010 22:50:33.308642   29740 system_pods.go:61] "kube-proxy-mkmvg" [709ec71d-6020-49b1-82e7-be053892893a] Running
I1010 22:50:33.308642   29740 system_pods.go:61] "kube-scheduler-minikube" [c436a8a4-cef3-456e-afe9-f52db56b1efe] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1010 22:50:33.308642   29740 system_pods.go:61] "storage-provisioner" [f509436d-32fe-4c91-8f5a-58fab04f4055] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1010 22:50:33.308642   29740 system_pods.go:74] duration metric: took 67.2862ms to wait for pod list to return data ...
I1010 22:50:33.308642   29740 kubeadm.go:581] duration metric: took 4.9531268s to wait for : map[apiserver:true system_pods:true] ...
I1010 22:50:33.308642   29740 node_conditions.go:102] verifying NodePressure condition ...
I1010 22:50:33.320031   29740 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I1010 22:50:33.320031   29740 node_conditions.go:123] node cpu capacity is 8
I1010 22:50:33.320031   29740 node_conditions.go:105] duration metric: took 11.3884ms to run NodePressure ...
I1010 22:50:33.336323   29740 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (2.3694544s)
I1010 22:50:33.336846   29740 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1010 22:50:33.336846   29740 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1010 22:50:33.370457   29740 start.go:228] waiting for startup goroutines ...
I1010 22:50:33.417780   29740 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1010 22:50:34.318317   29740 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (3.1164764s)
I1010 22:50:34.318317   29740 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58043 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I1010 22:50:35.959915   29740 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1010 22:50:38.923667   29740 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.9637436s)
I1010 22:50:44.337029   29740 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (10.9192196s)
I1010 22:50:44.337629   29740 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58043 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I1010 22:50:44.959049   29740 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1010 22:50:46.665513   29740 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.706136s)
I1010 22:50:46.677251   29740 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I1010 22:50:46.713605   29740 addons.go:502] enable addons completed in 18.4729268s: enabled=[storage-provisioner default-storageclass]
I1010 22:50:46.714203   29740 start.go:233] waiting for cluster config update ...
I1010 22:50:46.714203   29740 start.go:242] writing updated cluster config ...
I1010 22:50:46.877044   29740 ssh_runner.go:195] Run: rm -f paused
I1010 22:50:48.112613   29740 start.go:600] kubectl: 1.27.2, cluster: 1.27.4 (minor skew: 0)
I1010 22:50:48.113667   29740 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Oct 11 03:49:46 minikube dockerd[705]: time="2023-10-11T03:49:46.447258674Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Oct 11 03:49:46 minikube systemd[1]: docker.service: Deactivated successfully.
Oct 11 03:49:46 minikube systemd[1]: Stopped Docker Application Container Engine.
Oct 11 03:49:46 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.521306650Z" level=info msg="Starting up"
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.538201023Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.553811767Z" level=info msg="Loading containers: start."
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.755410432Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.806161167Z" level=info msg="Loading containers: done."
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.821622300Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.821679750Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.821688265Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.821692484Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.821712772Z" level=info msg="Docker daemon" commit=4ffc614 graphdriver=overlay2 version=24.0.4
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.821763263Z" level=info msg="Daemon has completed initialization"
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.869491688Z" level=info msg="API listen on /var/run/docker.sock"
Oct 11 03:49:46 minikube dockerd[923]: time="2023-10-11T03:49:46.869579058Z" level=info msg="API listen on [::]:2376"
Oct 11 03:49:46 minikube systemd[1]: Started Docker Application Container Engine.
Oct 11 03:49:48 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Oct 11 03:49:48 minikube cri-dockerd[1157]: time="2023-10-11T03:49:48Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Oct 11 03:49:48 minikube cri-dockerd[1157]: time="2023-10-11T03:49:48Z" level=info msg="Start docker client with request timeout 0s"
Oct 11 03:49:48 minikube cri-dockerd[1157]: time="2023-10-11T03:49:48Z" level=info msg="Hairpin mode is set to hairpin-veth"
Oct 11 03:49:48 minikube cri-dockerd[1157]: time="2023-10-11T03:49:48Z" level=info msg="Loaded network plugin cni"
Oct 11 03:49:48 minikube cri-dockerd[1157]: time="2023-10-11T03:49:48Z" level=info msg="Docker cri networking managed by network plugin cni"
Oct 11 03:49:48 minikube cri-dockerd[1157]: time="2023-10-11T03:49:48Z" level=info msg="Docker Info: &{ID:60eeee3b-2ae7-4192-9401-21dafbb40652 Containers:17 ContainersRunning:0 ContainersPaused:0 ContainersStopped:17 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:36 SystemTime:2023-10-11T03:49:48.720941393Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:5.10.102.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.2 LTS (containerized) OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0000b97a0 NCPU:8 MemTotal:6101708800 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support]}"
Oct 11 03:49:48 minikube cri-dockerd[1157]: time="2023-10-11T03:49:48Z" level=info msg="Setting cgroupDriver cgroupfs"
Oct 11 03:49:48 minikube cri-dockerd[1157]: time="2023-10-11T03:49:48Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Oct 11 03:49:48 minikube cri-dockerd[1157]: time="2023-10-11T03:49:48Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Oct 11 03:49:48 minikube cri-dockerd[1157]: time="2023-10-11T03:49:48Z" level=info msg="Start cri-dockerd grpc backend"
Oct 11 03:49:48 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Oct 11 03:50:11 minikube cri-dockerd[1157]: time="2023-10-11T03:50:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongodb-7776888f9-mff54_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6c6be2fc0e03df6e4722277d64bc7019d8b52cbc3344bac488c22c45125ee9fd\""
Oct 11 03:50:11 minikube cri-dockerd[1157]: time="2023-10-11T03:50:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-rgt2n_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3b1da451fa09b4a3d9ec7e187a03c29b963ca2fd98bc888ed6677f84c387460b\""
Oct 11 03:50:12 minikube cri-dockerd[1157]: time="2023-10-11T03:50:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5468e9ab9d0cd8ee72eb893ece69304766e605fbf41a950f9ca78d9b93dfd63b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 11 03:50:12 minikube cri-dockerd[1157]: time="2023-10-11T03:50:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/27dd2bf6de9853c0e7daaacf13abe7f43679fda4af22ca15929c09f75a0ec493/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 11 03:50:12 minikube cri-dockerd[1157]: time="2023-10-11T03:50:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6f7f40b24d7411e9007e5292c6cef3094e31b6a1ba78e95dd1061c7525c2c42c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 11 03:50:12 minikube cri-dockerd[1157]: time="2023-10-11T03:50:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e9f9bc3ee419605cc72030c3a580363943f59af30a59a4b4ba9c953c3477e603/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 11 03:50:21 minikube cri-dockerd[1157]: time="2023-10-11T03:50:21Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Oct 11 03:50:24 minikube cri-dockerd[1157]: time="2023-10-11T03:50:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8f64f577b8389f8356355e225c9a4a197326cd06bd3d61f7274f216ef14bbf52/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 11 03:50:24 minikube cri-dockerd[1157]: time="2023-10-11T03:50:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1d07e65b8fdeacb9c71c725d0af847f3dd23cb3863f4ecdda33d3ae293828c24/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 11 03:50:26 minikube cri-dockerd[1157]: time="2023-10-11T03:50:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eb29107dbf034c0bd602f30b96d5a7a6fba2447e265bb59ce7df45171b0d5f1d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 11 03:50:26 minikube cri-dockerd[1157]: time="2023-10-11T03:50:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/445d9ed09ccdd31705b7ed6f8d26766c36460207d5347dac7529b5423ddea94a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 11 03:50:26 minikube dockerd[923]: time="2023-10-11T03:50:26.998956265Z" level=info msg="ignoring event" container=62a15ab473415f8fe425afc137a5d0887b4e24cd9fa64bd6147eee781737d46a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 11 03:50:29 minikube cri-dockerd[1157]: time="2023-10-11T03:50:29Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Oct 11 03:53:22 minikube cri-dockerd[1157]: time="2023-10-11T03:53:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3349947db51f2a6c240a8998f9c352196bc147fea0727ce5ea736458eb49f88c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 11 03:53:35 minikube cri-dockerd[1157]: time="2023-10-11T03:53:35Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: b47a222d28fa: Downloading [=============================>                     ]  14.25MB/24.03MB"
Oct 11 03:53:45 minikube cri-dockerd[1157]: time="2023-10-11T03:53:45Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: debce5f9f3a9: Downloading [==============================>                    ]  38.81MB/64.11MB"
Oct 11 03:53:55 minikube cri-dockerd[1157]: time="2023-10-11T03:53:55Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: c72c41e7648d: Downloading [==============>                                    ]  13.73MB/47.89MB"
Oct 11 03:54:05 minikube cri-dockerd[1157]: time="2023-10-11T03:54:05Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: 31ea2e9c3b5e: Downloading [=============>                                     ]  20.96MB/76.58MB"
Oct 11 03:54:15 minikube cri-dockerd[1157]: time="2023-10-11T03:54:15Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: 167b8a53ca45: Extracting [=======================================>           ]   38.8MB/49.56MB"
Oct 11 03:54:25 minikube cri-dockerd[1157]: time="2023-10-11T03:54:25Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: 1d7ca7cd2e06: Downloading [==========================>                        ]  111.6MB/211MB"
Oct 11 03:54:35 minikube cri-dockerd[1157]: time="2023-10-11T03:54:35Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: 1d7ca7cd2e06: Downloading [=======================================>           ]  165.4MB/211MB"
Oct 11 03:54:45 minikube cri-dockerd[1157]: time="2023-10-11T03:54:45Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: 1d7ca7cd2e06: Downloading [============================================>      ]  189.1MB/211MB"
Oct 11 03:54:49 minikube cri-dockerd[1157]: time="2023-10-11T03:54:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9bd042075e5b1db407e83086a3bad895fff1960997cbf2e1797d9748e8bfdc87/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 11 03:54:49 minikube cri-dockerd[1157]: time="2023-10-11T03:54:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2457fac334334c3694b9d5c2dd0e0f09a570ae4b73b87e87dfb2299ea455abb5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 11 03:54:55 minikube cri-dockerd[1157]: time="2023-10-11T03:54:55Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: debce5f9f3a9: Extracting [=======================>                           ]  30.08MB/64.11MB"
Oct 11 03:55:05 minikube cri-dockerd[1157]: time="2023-10-11T03:55:05Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: debce5f9f3a9: Extracting [==============================================>    ]  60.16MB/64.11MB"
Oct 11 03:55:15 minikube cri-dockerd[1157]: time="2023-10-11T03:55:15Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: 1d7ca7cd2e06: Extracting [======>                                            ]   27.3MB/211MB"
Oct 11 03:55:25 minikube cri-dockerd[1157]: time="2023-10-11T03:55:25Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: 1d7ca7cd2e06: Extracting [==================>                                ]  76.87MB/211MB"
Oct 11 03:55:35 minikube cri-dockerd[1157]: time="2023-10-11T03:55:35Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: 1d7ca7cd2e06: Extracting [==================================>                ]  147.6MB/211MB"
Oct 11 03:55:45 minikube cri-dockerd[1157]: time="2023-10-11T03:55:45Z" level=info msg="Pulling image bloor06/veterinaria:0.0.4: 1d7ca7cd2e06: Extracting [=============================================>     ]  192.7MB/211MB"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
f5aff4c120579       6e38f40d628db                                                                   5 minutes ago       Running             storage-provisioner       4                   1d07e65b8fdea       storage-provisioner
58f044cfc7ea0       mongo@sha256:26549961b7dd23ea104ad9ba0c30abec6689fda873070de9fdd2cad304af671d   5 minutes ago       Running             mongodb                   1                   445d9ed09ccdd       mongodb-7776888f9-mff54
a3f720f264e33       ead0a4a53df89                                                                   5 minutes ago       Running             coredns                   1                   eb29107dbf034       coredns-5d78c9869d-rgt2n
62a15ab473415       6e38f40d628db                                                                   5 minutes ago       Exited              storage-provisioner       3                   1d07e65b8fdea       storage-provisioner
41f5666077bcc       6848d7eda0341                                                                   5 minutes ago       Running             kube-proxy                1                   8f64f577b8389       kube-proxy-mkmvg
88545e8c19722       e7972205b6614                                                                   5 minutes ago       Running             kube-apiserver            1                   e9f9bc3ee4196       kube-apiserver-minikube
7fa051f66a966       f466468864b7a                                                                   5 minutes ago       Running             kube-controller-manager   1                   6f7f40b24d741       kube-controller-manager-minikube
69ca757a10103       86b6af7dd652c                                                                   5 minutes ago       Running             etcd                      1                   27dd2bf6de985       etcd-minikube
180b4f8bcef71       98ef2570f3cde                                                                   5 minutes ago       Running             kube-scheduler            1                   5468e9ab9d0cd       kube-scheduler-minikube
310973b7bcf19       mongo@sha256:26549961b7dd23ea104ad9ba0c30abec6689fda873070de9fdd2cad304af671d   14 minutes ago      Exited              mongodb                   0                   6c6be2fc0e03d       mongodb-7776888f9-mff54
4afca4381570a       6848d7eda0341                                                                   16 minutes ago      Exited              kube-proxy                0                   b8632a921bc3d       kube-proxy-mkmvg
8af8c2c2a7f0d       ead0a4a53df89                                                                   16 minutes ago      Exited              coredns                   0                   3b1da451fa09b       coredns-5d78c9869d-rgt2n
07c7a87e577fe       98ef2570f3cde                                                                   16 minutes ago      Exited              kube-scheduler            0                   a231309481844       kube-scheduler-minikube
11ed290a628c5       e7972205b6614                                                                   16 minutes ago      Exited              kube-apiserver            0                   335931d533766       kube-apiserver-minikube
669c8be5e8631       86b6af7dd652c                                                                   16 minutes ago      Exited              etcd                      0                   badc2b3491855       etcd-minikube
528bef7d2777b       f466468864b7a                                                                   16 minutes ago      Exited              kube-controller-manager   0                   2613118c2fbf9       kube-controller-manager-minikube

* 
* ==> coredns [8af8c2c2a7f0] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:44593 - 11677 "HINFO IN 1870843057867545118.7088113533518051689. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.082075057s
[INFO] 10.244.0.3:35331 - 28916 "A IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.002585252s
[INFO] 10.244.0.3:58341 - 50482 "A IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.00022679s
[INFO] 10.244.0.3:59416 - 21381 "A IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.00019592s
[INFO] 10.244.0.3:40104 - 29057 "A IN compass.mongodb.com. udp 37 false 512" NOERROR qr,rd,ra 306 0.220651786s
[INFO] 10.244.0.3:40531 - 4401 "A IN api.segment.io.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00032901s
[INFO] 10.244.0.3:44200 - 37283 "A IN api.segment.io.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000266117s
[INFO] 10.244.0.3:59869 - 34984 "A IN api.segment.io.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000211663s
[INFO] 10.244.0.3:32972 - 11050 "A IN api.segment.io. udp 32 false 512" NOERROR qr,rd,ra 122 0.078887265s
[INFO] 10.244.0.3:47382 - 59940 "A IN api.segment.io.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000252725s
[INFO] 10.244.0.3:33982 - 4067 "A IN api.segment.io.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000155802s
[INFO] 10.244.0.3:60614 - 56234 "A IN api.segment.io.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000175069s
[INFO] 10.244.0.3:48879 - 45848 "A IN api.segment.io. udp 32 false 512" NOERROR qr,aa,rd,ra 122 0.000224739s
[INFO] 10.244.0.3:53944 - 59600 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000311903s
[INFO] 10.244.0.3:41342 - 20404 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.000189102s
[INFO] 10.244.0.3:37706 - 46303 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0001859s
[INFO] 10.244.0.3:58777 - 37703 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd,ra 207 0.005833582s

* 
* ==> coredns [a3f720f264e3] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:58254 - 17121 "HINFO IN 919615951791383663.1271507834706716164. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.331200489s
[INFO] 10.244.0.5:58324 - 62365 "A IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.002452442s
[INFO] 10.244.0.5:47741 - 50555 "A IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000253431s
[INFO] 10.244.0.5:47985 - 29107 "A IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000189661s
[INFO] 10.244.0.5:43101 - 77 "A IN compass.mongodb.com. udp 37 false 512" NOERROR qr,rd,ra 306 0.294204081s
[INFO] 10.244.0.5:51386 - 947 "A IN api.segment.io.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000305603s
[INFO] 10.244.0.5:41253 - 54703 "A IN api.segment.io.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000235162s
[INFO] 10.244.0.5:43859 - 61201 "A IN api.segment.io.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.00021472s
[INFO] 10.244.0.5:45433 - 61910 "A IN api.segment.io. udp 32 false 512" NOERROR qr,rd,ra 122 0.072755367s
[INFO] 10.244.0.5:37346 - 54570 "A IN api.segment.io.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000279442s
[INFO] 10.244.0.5:48116 - 44713 "A IN api.segment.io.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.00018107s
[INFO] 10.244.0.5:56564 - 34818 "A IN api.segment.io.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000130908s
[INFO] 10.244.0.5:53970 - 58236 "A IN api.segment.io. udp 32 false 512" NOERROR qr,aa,rd,ra 122 0.000186636s
[INFO] 10.244.0.5:33276 - 7912 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000339818s
[INFO] 10.244.0.5:47501 - 19983 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.000201867s
[INFO] 10.244.0.5:57401 - 54216 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.006038696s
[INFO] 10.244.0.5:42722 - 55213 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd,ra 207 0.010219657s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_10_10T22_39_20_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 11 Oct 2023 03:39:15 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 11 Oct 2023 03:55:49 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 11 Oct 2023 03:55:32 +0000   Wed, 11 Oct 2023 03:39:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 11 Oct 2023 03:55:32 +0000   Wed, 11 Oct 2023 03:39:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 11 Oct 2023 03:55:32 +0000   Wed, 11 Oct 2023 03:39:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 11 Oct 2023 03:55:32 +0000   Wed, 11 Oct 2023 03:39:30 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             5958700Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             5958700Ki
  pods:               110
System Info:
  Machine ID:                 1183365012d842f2856f6e60ab30bc4b
  System UUID:                1183365012d842f2856f6e60ab30bc4b
  Boot ID:                    232f299a-f40e-4caf-b657-d814129334bd
  Kernel Version:             5.10.102.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---
  default                     backend-deployment-6d4d68f5b5-s6szv    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m36s
  default                     backend-deployment-6fcbc7759c-8hrhp    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         78s
  default                     mongodb-565b4ccdb4-v22kk               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         78s
  default                     mongodb-7776888f9-mff54                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15m
  kube-system                 coredns-5d78c9869d-rgt2n               100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (2%!)(MISSING)     16m
  kube-system                 etcd-minikube                          100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         16m
  kube-system                 kube-apiserver-minikube                250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16m
  kube-system                 kube-controller-manager-minikube       200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16m
  kube-system                 kube-proxy-mkmvg                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16m
  kube-system                 kube-scheduler-minikube                100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16m
  kube-system                 storage-provisioner                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 16m                    kube-proxy       
  Normal  Starting                 5m29s                  kube-proxy       
  Normal  Starting                 16m                    kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  16m (x8 over 16m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    16m (x8 over 16m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     16m (x7 over 16m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  16m                    kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 16m                    kubelet          Starting kubelet.
  Normal  NodeHasSufficientPID     16m                    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasNoDiskPressure    16m                    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeNotReady             16m                    kubelet          Node minikube status is now: NodeNotReady
  Normal  NodeAllocatableEnforced  16m                    kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  16m                    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeReady                16m                    kubelet          Node minikube status is now: NodeReady
  Normal  RegisteredNode           16m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 5m46s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  5m46s (x8 over 5m46s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    5m46s (x8 over 5m46s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     5m46s (x7 over 5m46s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  5m46s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           5m18s                  node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [  +0.032476] WARNING: /usr/share/zoneinfo/America/Bogota not found. Is the tzdata package installed?
[  +0.272754] init: (8) ERROR: CreateProcessEntryCommon:443: getpwuid(0) failed 2
[  +0.008823] init: (8) ERROR: CreateProcessEntryCommon:446: getpwuid(0) failed 2
[  +3.894627] cgroup: runc (682) created nested cgroup for controller "memory" which has incomplete hierarchy support. Nested cgroups may change behavior in the future.
[  +0.010088] cgroup: "memory" requires setting use_hierarchy to 1 on the root
[Oct10 23:54] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.007087] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Oct11 00:24] hrtimer: interrupt took 2813540 ns
[Oct11 00:35] CPU: 7 PID: 173 Comm: weston Not tainted 5.10.102.1-microsoft-standard-WSL2 #1
[  +0.003316] RIP: 0033:0x7fe4fa6c5115
[  +0.001998] Code: c2 b8 ea 00 00 00 0f 05 48 3d 00 f0 ff ff 77 3d 41 89 c0 41 ba 08 00 00 00 31 d2 4c 89 ce bf 02 00 00 00 b8 0e 00 00 00 0f 05 <48> 8b 84 24 08 01 00 00 64 48 33 04 25 28 00 00 00 75 24 44 89 c0
[  +0.000001] RSP: 002b:00007fffaf9435b0 EFLAGS: 00000246 ORIG_RAX: 000000000000000e
[  +0.000793] RAX: 0000000000000000 RBX: 00007fe4fa853000 RCX: 00007fe4fa6c5115
[  +0.000001] RDX: 0000000000000000 RSI: 00007fffaf9435b0 RDI: 0000000000000002
[  +0.000001] RBP: 00007fe4fa815630 R08: 0000000000000000 R09: 00007fffaf9435b0
[  +0.000001] R10: 0000000000000008 R11: 0000000000000246 R12: 00007fe4f990c430
[  +0.000001] R13: 000000000000047c R14: 00007fe4f990c242 R15: 00007fffaf944218
[  +0.000029] FS:  00007fe4f9e1a900 GS:  0000000000000000
[Oct11 02:55] CPU: 4 PID: 277 Comm: weston Not tainted 5.10.102.1-microsoft-standard-WSL2 #1
[  +0.001474] RIP: 0033:0x7fb16030c115
[  +0.000008] Code: c2 b8 ea 00 00 00 0f 05 48 3d 00 f0 ff ff 77 3d 41 89 c0 41 ba 08 00 00 00 31 d2 4c 89 ce bf 02 00 00 00 b8 0e 00 00 00 0f 05 <48> 8b 84 24 08 01 00 00 64 48 33 04 25 28 00 00 00 75 24 44 89 c0
[  +0.000001] RSP: 002b:00007ffebb119790 EFLAGS: 00000246 ORIG_RAX: 000000000000000e
[  +0.000031] RAX: 0000000000000000 RBX: 00007fb16049a000 RCX: 00007fb16030c115
[  +0.000001] RDX: 0000000000000000 RSI: 00007ffebb119790 RDI: 0000000000000002
[  +0.000000] RBP: 00007fb16045c630 R08: 0000000000000000 R09: 00007ffebb119790
[  +0.000001] R10: 0000000000000008 R11: 0000000000000246 R12: 00007fb15f553430
[  +0.000001] R13: 000000000000047c R14: 00007fb15f553242 R15: 00007ffebb11a3f8
[  +0.000001] FS:  00007fb15fa61900 GS:  0000000000000000
[Oct11 02:58] CPU: 3 PID: 365731 Comm: weston Not tainted 5.10.102.1-microsoft-standard-WSL2 #1
[  +0.000033] RIP: 0033:0x7ff17bcc7115
[  +0.000030] Code: c2 b8 ea 00 00 00 0f 05 48 3d 00 f0 ff ff 77 3d 41 89 c0 41 ba 08 00 00 00 31 d2 4c 89 ce bf 02 00 00 00 b8 0e 00 00 00 0f 05 <48> 8b 84 24 08 01 00 00 64 48 33 04 25 28 00 00 00 75 24 44 89 c0
[  +0.000002] RSP: 002b:00007ffd43a92660 EFLAGS: 00000246 ORIG_RAX: 000000000000000e
[  +0.000002] RAX: 0000000000000000 RBX: 00007ff17be55000 RCX: 00007ff17bcc7115
[  +0.000001] RDX: 0000000000000000 RSI: 00007ffd43a92660 RDI: 0000000000000002
[  +0.000001] RBP: 00007ff17be17630 R08: 0000000000000000 R09: 00007ffd43a92660
[  +0.000001] R10: 0000000000000008 R11: 0000000000000246 R12: 00007ff17af0e430
[  +0.000001] R13: 000000000000047c R14: 00007ff17af0e242 R15: 00007ffd43a932c8
[  +0.000001] FS:  00007ff17b41c900 GS:  0000000000000000
[Oct11 03:07] CPU: 2 PID: 371681 Comm: weston Not tainted 5.10.102.1-microsoft-standard-WSL2 #1
[  +0.019400] RIP: 0033:0x7ffa6b750115
[  +0.006391] Code: c2 b8 ea 00 00 00 0f 05 48 3d 00 f0 ff ff 77 3d 41 89 c0 41 ba 08 00 00 00 31 d2 4c 89 ce bf 02 00 00 00 b8 0e 00 00 00 0f 05 <48> 8b 84 24 08 01 00 00 64 48 33 04 25 28 00 00 00 75 24 44 89 c0
[  +0.000002] RSP: 002b:00007ffd1000b160 EFLAGS: 00000246 ORIG_RAX: 000000000000000e
[  +0.000128] RAX: 0000000000000000 RBX: 00007ffa6b8de000 RCX: 00007ffa6b750115
[  +0.000243] RDX: 0000000000000000 RSI: 00007ffd1000b160 RDI: 0000000000000002
[  +0.000006] RBP: 00007ffa6b8a0630 R08: 0000000000000000 R09: 00007ffd1000b160
[  +0.000001] R10: 0000000000000008 R11: 0000000000000246 R12: 00007ffa6a997430
[  +0.000001] R13: 000000000000047c R14: 00007ffa6a997242 R15: 00007ffd1000bdc8
[  +0.000002] FS:  00007ffa6aea5900 GS:  0000000000000000
[Oct11 03:45] init: (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000009]  failed 2
[  +0.164390] WARNING: /usr/share/zoneinfo/America/Bogota not found. Is the tzdata package installed?
[  +1.579000] init: (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000161]  failed 2
[  +0.044755] init: (2) ERROR: UtilCreateProcessAndWait:702: /bin/mount failed with 2
[  +0.028952] init: (1) ERROR: UtilCreateProcessAndWait:722: /bin/mount failed with status 0x
[  +0.000024] ff00
[  +0.002091] init: (1) ERROR: ConfigMountFsTab:2484: Processing fstab with mount -a failed.
[  +0.036973] WARNING: /usr/share/zoneinfo/America/Bogota not found. Is the tzdata package installed?
[  +0.141883] init: (8) ERROR: CreateProcessEntryCommon:443: getpwuid(0) failed 2
[  +0.005967] init: (8) ERROR: CreateProcessEntryCommon:446: getpwuid(0) failed 2

* 
* ==> etcd [669c8be5e863] <==
* {"level":"info","ts":"2023-10-11T03:40:16.210Z","caller":"traceutil/trace.go:171","msg":"trace[1452419520] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:465; }","duration":"1.816734375s","start":"2023-10-11T03:40:14.393Z","end":"2023-10-11T03:40:16.210Z","steps":["trace[1452419520] 'agreement among raft nodes before linearized reading'  (duration: 1.816574553s)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:16.210Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:14.393Z","time spent":"1.81678783s","remote":"127.0.0.1:55848","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":624,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2023-10-11T03:40:16.210Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.187323169s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:40:16.210Z","caller":"traceutil/trace.go:171","msg":"trace[1018200177] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:465; }","duration":"1.187344858s","start":"2023-10-11T03:40:15.023Z","end":"2023-10-11T03:40:16.210Z","steps":["trace[1018200177] 'agreement among raft nodes before linearized reading'  (duration: 1.18730792s)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:16.210Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:15.023Z","time spent":"1.187387103s","remote":"127.0.0.1:55856","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-10-11T03:40:16.210Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"182.018089ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:40:16.210Z","caller":"traceutil/trace.go:171","msg":"trace[1693399115] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:465; }","duration":"182.037484ms","start":"2023-10-11T03:40:16.028Z","end":"2023-10-11T03:40:16.210Z","steps":["trace[1693399115] 'agreement among raft nodes before linearized reading'  (duration: 181.988222ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:16.210Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"540.782329ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:40:16.210Z","caller":"traceutil/trace.go:171","msg":"trace[1556986638] range","detail":"{range_begin:/registry/networkpolicies/; range_end:/registry/networkpolicies0; response_count:0; response_revision:465; }","duration":"540.80388ms","start":"2023-10-11T03:40:15.669Z","end":"2023-10-11T03:40:16.210Z","steps":["trace[1556986638] 'agreement among raft nodes before linearized reading'  (duration: 540.758513ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:16.212Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"768.955895ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/volumeattachments/\" range_end:\"/registry/volumeattachments0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:40:16.212Z","caller":"traceutil/trace.go:171","msg":"trace[1787104336] range","detail":"{range_begin:/registry/volumeattachments/; range_end:/registry/volumeattachments0; response_count:0; response_revision:465; }","duration":"769.031088ms","start":"2023-10-11T03:40:15.443Z","end":"2023-10-11T03:40:16.212Z","steps":["trace[1787104336] 'agreement among raft nodes before linearized reading'  (duration: 768.88148ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:16.212Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:15.443Z","time spent":"769.135815ms","remote":"127.0.0.1:55928","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":0,"response size":29,"request content":"key:\"/registry/volumeattachments/\" range_end:\"/registry/volumeattachments0\" count_only:true "}
{"level":"warn","ts":"2023-10-11T03:40:16.210Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:15.669Z","time spent":"540.942317ms","remote":"127.0.0.1:55894","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":0,"response size":29,"request content":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true "}
{"level":"info","ts":"2023-10-11T03:40:27.211Z","caller":"traceutil/trace.go:171","msg":"trace[2095093772] transaction","detail":"{read_only:false; response_revision:474; number_of_response:1; }","duration":"674.02216ms","start":"2023-10-11T03:40:26.537Z","end":"2023-10-11T03:40:27.211Z","steps":["trace[2095093772] 'process raft request'  (duration: 673.760036ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:27.211Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:26.537Z","time spent":"674.149847ms","remote":"127.0.0.1:55848","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:473 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-10-11T03:40:28.176Z","caller":"traceutil/trace.go:171","msg":"trace[642882039] transaction","detail":"{read_only:false; response_revision:475; number_of_response:1; }","duration":"463.973568ms","start":"2023-10-11T03:40:27.712Z","end":"2023-10-11T03:40:28.176Z","steps":["trace[642882039] 'process raft request'  (duration: 463.795761ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:28.176Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:27.712Z","time spent":"464.100643ms","remote":"127.0.0.1:55890","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:467 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2023-10-11T03:40:28.176Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"370.981332ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/default\" ","response":"range_response_count:1 size:339"}
{"level":"info","ts":"2023-10-11T03:40:28.176Z","caller":"traceutil/trace.go:171","msg":"trace[389235203] range","detail":"{range_begin:/registry/namespaces/default; range_end:; response_count:1; response_revision:475; }","duration":"371.028289ms","start":"2023-10-11T03:40:27.805Z","end":"2023-10-11T03:40:28.176Z","steps":["trace[389235203] 'agreement among raft nodes before linearized reading'  (duration: 370.912358ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:28.176Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:27.805Z","time spent":"371.087497ms","remote":"127.0.0.1:55846","response type":"/etcdserverpb.KV/Range","request count":0,"request size":30,"response count":1,"response size":363,"request content":"key:\"/registry/namespaces/default\" "}
{"level":"info","ts":"2023-10-11T03:40:28.176Z","caller":"traceutil/trace.go:171","msg":"trace[1287442399] linearizableReadLoop","detail":"{readStateIndex:497; appliedIndex:496; }","duration":"370.708566ms","start":"2023-10-11T03:40:27.805Z","end":"2023-10-11T03:40:28.176Z","steps":["trace[1287442399] 'read index received'  (duration: 370.552981ms)","trace[1287442399] 'applied index is now lower than readState.Index'  (duration: 154.982¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-10-11T03:40:28.178Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"154.863828ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:40:28.178Z","caller":"traceutil/trace.go:171","msg":"trace[1685796124] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:475; }","duration":"154.96741ms","start":"2023-10-11T03:40:28.023Z","end":"2023-10-11T03:40:28.178Z","steps":["trace[1685796124] 'agreement among raft nodes before linearized reading'  (duration: 154.792147ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:28.715Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"332.53509ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128024383380835970 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc8b1cd120ce81>","response":"size:41"}
{"level":"info","ts":"2023-10-11T03:40:28.715Z","caller":"traceutil/trace.go:171","msg":"trace[1766680575] linearizableReadLoop","detail":"{readStateIndex:498; appliedIndex:497; }","duration":"299.782165ms","start":"2023-10-11T03:40:28.416Z","end":"2023-10-11T03:40:28.715Z","steps":["trace[1766680575] 'read index received'  (duration: 183.071¬µs)","trace[1766680575] 'applied index is now lower than readState.Index'  (duration: 299.597188ms)"],"step_count":2}
{"level":"warn","ts":"2023-10-11T03:40:28.715Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:28.182Z","time spent":"533.219812ms","remote":"127.0.0.1:55822","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2023-10-11T03:40:28.716Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"299.914259ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/\" range_end:\"/registry/rolebindings0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-10-11T03:40:28.716Z","caller":"traceutil/trace.go:171","msg":"trace[1680440025] range","detail":"{range_begin:/registry/rolebindings/; range_end:/registry/rolebindings0; response_count:0; response_revision:475; }","duration":"299.961459ms","start":"2023-10-11T03:40:28.416Z","end":"2023-10-11T03:40:28.716Z","steps":["trace[1680440025] 'agreement among raft nodes before linearized reading'  (duration: 299.837436ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:28.716Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:28.416Z","time spent":"300.019184ms","remote":"127.0.0.1:55910","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":12,"response size":31,"request content":"key:\"/registry/rolebindings/\" range_end:\"/registry/rolebindings0\" count_only:true "}
{"level":"warn","ts":"2023-10-11T03:40:34.609Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"131.425457ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128024383380836001 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/default/mongodb-service\" mod_revision:0 > success:<request_put:<key:\"/registry/services/endpoints/default/mongodb-service\" value_size:351 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2023-10-11T03:40:34.609Z","caller":"traceutil/trace.go:171","msg":"trace[478323977] transaction","detail":"{read_only:false; response_revision:485; number_of_response:1; }","duration":"180.940116ms","start":"2023-10-11T03:40:34.428Z","end":"2023-10-11T03:40:34.609Z","steps":["trace[478323977] 'process raft request'  (duration: 49.213801ms)","trace[478323977] 'compare'  (duration: 131.090079ms)"],"step_count":2}
{"level":"warn","ts":"2023-10-11T03:40:47.948Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"141.372088ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/default\" ","response":"range_response_count:1 size:339"}
{"level":"info","ts":"2023-10-11T03:40:47.948Z","caller":"traceutil/trace.go:171","msg":"trace[719885374] range","detail":"{range_begin:/registry/namespaces/default; range_end:; response_count:1; response_revision:495; }","duration":"141.476948ms","start":"2023-10-11T03:40:47.807Z","end":"2023-10-11T03:40:47.948Z","steps":["trace[719885374] 'range keys from in-memory index tree'  (duration: 141.221437ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:49.392Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128024383380836064,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-10-11T03:40:49.893Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128024383380836064,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-10-11T03:40:50.143Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.346388999s","expected-duration":"1s"}
{"level":"info","ts":"2023-10-11T03:40:50.143Z","caller":"traceutil/trace.go:171","msg":"trace[2121910694] linearizableReadLoop","detail":"{readStateIndex:524; appliedIndex:523; }","duration":"1.252309372s","start":"2023-10-11T03:40:48.891Z","end":"2023-10-11T03:40:50.143Z","steps":["trace[2121910694] 'read index received'  (duration: 1.252085296s)","trace[2121910694] 'applied index is now lower than readState.Index'  (duration: 223.32¬µs)"],"step_count":2}
{"level":"info","ts":"2023-10-11T03:40:50.143Z","caller":"traceutil/trace.go:171","msg":"trace[1300635689] transaction","detail":"{read_only:false; response_revision:497; number_of_response:1; }","duration":"1.347006407s","start":"2023-10-11T03:40:48.796Z","end":"2023-10-11T03:40:50.143Z","steps":["trace[1300635689] 'process raft request'  (duration: 1.346704642s)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:50.144Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"880.383412ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-10-11T03:40:50.144Z","caller":"traceutil/trace.go:171","msg":"trace[674468400] range","detail":"{range_begin:/registry/namespaces/; range_end:/registry/namespaces0; response_count:0; response_revision:497; }","duration":"880.442616ms","start":"2023-10-11T03:40:49.263Z","end":"2023-10-11T03:40:50.144Z","steps":["trace[674468400] 'agreement among raft nodes before linearized reading'  (duration: 880.317168ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:50.144Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:48.796Z","time spent":"1.347067278s","remote":"127.0.0.1:55890","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:489 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2023-10-11T03:40:50.144Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:49.263Z","time spent":"880.495641ms","remote":"127.0.0.1:55846","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":4,"response size":31,"request content":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true "}
{"level":"warn","ts":"2023-10-11T03:40:50.144Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"444.497581ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:600"}
{"level":"warn","ts":"2023-10-11T03:40:50.144Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"114.802117ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2023-10-11T03:40:50.144Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.252792853s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/storageclasses/\" range_end:\"/registry/storageclasses0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-10-11T03:40:50.144Z","caller":"traceutil/trace.go:171","msg":"trace[1158856911] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:497; }","duration":"114.877211ms","start":"2023-10-11T03:40:50.029Z","end":"2023-10-11T03:40:50.144Z","steps":["trace[1158856911] 'agreement among raft nodes before linearized reading'  (duration: 114.714943ms)"],"step_count":1}
{"level":"info","ts":"2023-10-11T03:40:50.144Z","caller":"traceutil/trace.go:171","msg":"trace[1355029715] range","detail":"{range_begin:/registry/storageclasses/; range_end:/registry/storageclasses0; response_count:0; response_revision:497; }","duration":"1.252840029s","start":"2023-10-11T03:40:48.891Z","end":"2023-10-11T03:40:50.144Z","steps":["trace[1355029715] 'agreement among raft nodes before linearized reading'  (duration: 1.252760059s)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:50.144Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:48.891Z","time spent":"1.252950373s","remote":"127.0.0.1:55922","response type":"/etcdserverpb.KV/Range","request count":0,"request size":56,"response count":1,"response size":31,"request content":"key:\"/registry/storageclasses/\" range_end:\"/registry/storageclasses0\" count_only:true "}
{"level":"info","ts":"2023-10-11T03:40:50.144Z","caller":"traceutil/trace.go:171","msg":"trace[29076097] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:497; }","duration":"444.551837ms","start":"2023-10-11T03:40:49.699Z","end":"2023-10-11T03:40:50.144Z","steps":["trace[29076097] 'agreement among raft nodes before linearized reading'  (duration: 444.423639ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:40:50.144Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:40:49.699Z","time spent":"444.657491ms","remote":"127.0.0.1:55848","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":624,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2023-10-11T03:41:05.876Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"859.008488ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:41:05.876Z","caller":"traceutil/trace.go:171","msg":"trace[431282125] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:509; }","duration":"859.097875ms","start":"2023-10-11T03:41:05.017Z","end":"2023-10-11T03:41:05.876Z","steps":["trace[431282125] 'range keys from in-memory index tree'  (duration: 858.879276ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:41:05.876Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:41:05.267Z","time spent":"608.910442ms","remote":"127.0.0.1:57490","response type":"/etcdserverpb.Maintenance/Status","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2023-10-11T03:41:05.876Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:41:05.017Z","time spent":"859.151044ms","remote":"127.0.0.1:55856","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2023-10-11T03:41:13.114Z","caller":"traceutil/trace.go:171","msg":"trace[903045897] transaction","detail":"{read_only:false; response_revision:517; number_of_response:1; }","duration":"227.078692ms","start":"2023-10-11T03:41:12.886Z","end":"2023-10-11T03:41:13.113Z","steps":["trace[903045897] 'process raft request'  (duration: 184.013051ms)","trace[903045897] 'compare'  (duration: 42.93938ms)"],"step_count":2}
{"level":"info","ts":"2023-10-11T03:41:14.849Z","caller":"traceutil/trace.go:171","msg":"trace[212274888] transaction","detail":"{read_only:false; response_revision:518; number_of_response:1; }","duration":"160.34549ms","start":"2023-10-11T03:41:14.688Z","end":"2023-10-11T03:41:14.849Z","steps":["trace[212274888] 'process raft request'  (duration: 160.172759ms)"],"step_count":1}
{"level":"info","ts":"2023-10-11T03:41:57.551Z","caller":"traceutil/trace.go:171","msg":"trace[1675232244] transaction","detail":"{read_only:false; response_revision:559; number_of_response:1; }","duration":"331.3958ms","start":"2023-10-11T03:41:57.219Z","end":"2023-10-11T03:41:57.550Z","steps":["trace[1675232244] 'process raft request'  (duration: 330.619262ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:41:57.551Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:41:57.219Z","time spent":"331.948361ms","remote":"127.0.0.1:55848","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:558 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-10-11T03:42:58.674Z","caller":"traceutil/trace.go:171","msg":"trace[1020946353] transaction","detail":"{read_only:false; response_revision:608; number_of_response:1; }","duration":"162.082215ms","start":"2023-10-11T03:42:58.512Z","end":"2023-10-11T03:42:58.674Z","steps":["trace[1020946353] 'process raft request'  (duration: 161.972955ms)"],"step_count":1}
{"level":"info","ts":"2023-10-11T03:43:51.236Z","caller":"traceutil/trace.go:171","msg":"trace[1549310511] transaction","detail":"{read_only:false; response_revision:649; number_of_response:1; }","duration":"114.1994ms","start":"2023-10-11T03:43:51.122Z","end":"2023-10-11T03:43:51.236Z","steps":["trace[1549310511] 'process raft request'  (duration: 111.419559ms)"],"step_count":1}

* 
* ==> etcd [69ca757a1010] <==
* {"level":"warn","ts":"2023-10-11T03:54:45.901Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:54:45.494Z","time spent":"406.243759ms","remote":"127.0.0.1:34766","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":693,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/kube-apiserver-minikube.178cf14ae0cf3759\" mod_revision:1026 > success:<request_put:<key:\"/registry/events/kube-system/kube-apiserver-minikube.178cf14ae0cf3759\" value_size:606 lease:8128024383551025099 >> failure:<request_range:<key:\"/registry/events/kube-system/kube-apiserver-minikube.178cf14ae0cf3759\" > >"}
{"level":"warn","ts":"2023-10-11T03:55:20.983Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128024383551025373,"retry-timeout":"500ms"}
{"level":"info","ts":"2023-10-11T03:55:21.253Z","caller":"traceutil/trace.go:171","msg":"trace[1230119551] transaction","detail":"{read_only:false; response_revision:1096; number_of_response:1; }","duration":"953.070408ms","start":"2023-10-11T03:55:20.300Z","end":"2023-10-11T03:55:21.253Z","steps":["trace[1230119551] 'process raft request'  (duration: 952.978549ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:21.253Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:20.300Z","time spent":"953.164663ms","remote":"127.0.0.1:34786","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1093 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-10-11T03:55:21.729Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"399.27403ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128024383551025375 > lease_revoke:<id:70cc8b1cdb45b09e>","response":"size:29"}
{"level":"info","ts":"2023-10-11T03:55:21.729Z","caller":"traceutil/trace.go:171","msg":"trace[1193988383] linearizableReadLoop","detail":"{readStateIndex:1261; appliedIndex:1259; }","duration":"1.246273093s","start":"2023-10-11T03:55:20.483Z","end":"2023-10-11T03:55:21.729Z","steps":["trace[1193988383] 'read index received'  (duration: 769.797343ms)","trace[1193988383] 'applied index is now lower than readState.Index'  (duration: 476.474276ms)"],"step_count":2}
{"level":"warn","ts":"2023-10-11T03:55:21.729Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.246395889s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:55:21.729Z","caller":"traceutil/trace.go:171","msg":"trace[103383422] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1096; }","duration":"1.246426237s","start":"2023-10-11T03:55:20.483Z","end":"2023-10-11T03:55:21.729Z","steps":["trace[103383422] 'agreement among raft nodes before linearized reading'  (duration: 1.246348528s)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:21.729Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:20.483Z","time spent":"1.2464704s","remote":"127.0.0.1:34822","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-10-11T03:55:21.729Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"337.059292ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:55:21.729Z","caller":"traceutil/trace.go:171","msg":"trace[1974457704] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1096; }","duration":"337.082698ms","start":"2023-10-11T03:55:21.392Z","end":"2023-10-11T03:55:21.729Z","steps":["trace[1974457704] 'agreement among raft nodes before linearized reading'  (duration: 337.029318ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:21.729Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:21.392Z","time spent":"337.121073ms","remote":"127.0.0.1:34820","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2023-10-11T03:55:35.825Z","caller":"traceutil/trace.go:171","msg":"trace[306034812] transaction","detail":"{read_only:false; response_revision:1107; number_of_response:1; }","duration":"139.933787ms","start":"2023-10-11T03:55:35.685Z","end":"2023-10-11T03:55:35.825Z","steps":["trace[306034812] 'process raft request'  (duration: 60.189635ms)","trace[306034812] 'compare'  (duration: 79.60744ms)"],"step_count":2}
{"level":"warn","ts":"2023-10-11T03:55:36.345Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"391.107876ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128024383551025439 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1106 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2023-10-11T03:55:36.345Z","caller":"traceutil/trace.go:171","msg":"trace[1855192826] linearizableReadLoop","detail":"{readStateIndex:1276; appliedIndex:1275; }","duration":"518.211313ms","start":"2023-10-11T03:55:35.827Z","end":"2023-10-11T03:55:36.345Z","steps":["trace[1855192826] 'read index received'  (duration: 126.953141ms)","trace[1855192826] 'applied index is now lower than readState.Index'  (duration: 391.257167ms)"],"step_count":2}
{"level":"info","ts":"2023-10-11T03:55:36.345Z","caller":"traceutil/trace.go:171","msg":"trace[1863759874] transaction","detail":"{read_only:false; response_revision:1108; number_of_response:1; }","duration":"652.590417ms","start":"2023-10-11T03:55:35.692Z","end":"2023-10-11T03:55:36.345Z","steps":["trace[1863759874] 'process raft request'  (duration: 261.22596ms)","trace[1863759874] 'compare'  (duration: 390.890853ms)"],"step_count":2}
{"level":"warn","ts":"2023-10-11T03:55:36.345Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"518.330363ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2023-10-11T03:55:36.345Z","caller":"traceutil/trace.go:171","msg":"trace[1882204204] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:1108; }","duration":"518.368772ms","start":"2023-10-11T03:55:35.827Z","end":"2023-10-11T03:55:36.345Z","steps":["trace[1882204204] 'agreement among raft nodes before linearized reading'  (duration: 518.260295ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:36.345Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:35.827Z","time spent":"518.415653ms","remote":"127.0.0.1:34786","response type":"/etcdserverpb.KV/Range","request count":0,"request size":49,"response count":1,"response size":444,"request content":"key:\"/registry/services/endpoints/default/kubernetes\" "}
{"level":"warn","ts":"2023-10-11T03:55:36.345Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:35.692Z","time spent":"652.711723ms","remote":"127.0.0.1:34786","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1106 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-10-11T03:55:38.816Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"126.248898ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:55:38.816Z","caller":"traceutil/trace.go:171","msg":"trace[2051854246] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1109; }","duration":"126.383605ms","start":"2023-10-11T03:55:38.690Z","end":"2023-10-11T03:55:38.816Z","steps":["trace[2051854246] 'range keys from in-memory index tree'  (duration: 126.063781ms)"],"step_count":1}
{"level":"info","ts":"2023-10-11T03:55:40.821Z","caller":"traceutil/trace.go:171","msg":"trace[1064123388] linearizableReadLoop","detail":"{readStateIndex:1280; appliedIndex:1279; }","duration":"132.586582ms","start":"2023-10-11T03:55:40.689Z","end":"2023-10-11T03:55:40.821Z","steps":["trace[1064123388] 'read index received'  (duration: 132.335769ms)","trace[1064123388] 'applied index is now lower than readState.Index'  (duration: 249.107¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-10-11T03:55:40.822Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"132.747695ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2023-10-11T03:55:40.822Z","caller":"traceutil/trace.go:171","msg":"trace[270656049] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:1111; }","duration":"132.813834ms","start":"2023-10-11T03:55:40.689Z","end":"2023-10-11T03:55:40.822Z","steps":["trace[270656049] 'agreement among raft nodes before linearized reading'  (duration: 132.662236ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:41.400Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128024383551025456,"retry-timeout":"500ms"}
{"level":"info","ts":"2023-10-11T03:55:41.633Z","caller":"traceutil/trace.go:171","msg":"trace[596755146] linearizableReadLoop","detail":"{readStateIndex:1281; appliedIndex:1280; }","duration":"733.126792ms","start":"2023-10-11T03:55:40.900Z","end":"2023-10-11T03:55:41.633Z","steps":["trace[596755146] 'read index received'  (duration: 683.494487ms)","trace[596755146] 'applied index is now lower than readState.Index'  (duration: 49.628848ms)"],"step_count":2}
{"level":"info","ts":"2023-10-11T03:55:41.633Z","caller":"traceutil/trace.go:171","msg":"trace[820024599] transaction","detail":"{read_only:false; response_revision:1112; number_of_response:1; }","duration":"805.820528ms","start":"2023-10-11T03:55:40.827Z","end":"2023-10-11T03:55:41.633Z","steps":["trace[820024599] 'process raft request'  (duration: 756.098375ms)","trace[820024599] 'compare'  (duration: 49.282372ms)"],"step_count":2}
{"level":"warn","ts":"2023-10-11T03:55:41.633Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:40.827Z","time spent":"805.895202ms","remote":"127.0.0.1:34786","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1109 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-10-11T03:55:41.633Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.264298ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiregistration.k8s.io/apiservices/\" range_end:\"/registry/apiregistration.k8s.io/apiservices0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-10-11T03:55:41.633Z","caller":"traceutil/trace.go:171","msg":"trace[1798808675] range","detail":"{range_begin:/registry/apiregistration.k8s.io/apiservices/; range_end:/registry/apiregistration.k8s.io/apiservices0; response_count:0; response_revision:1112; }","duration":"101.321187ms","start":"2023-10-11T03:55:41.532Z","end":"2023-10-11T03:55:41.633Z","steps":["trace[1798808675] 'agreement among raft nodes before linearized reading'  (duration: 101.201349ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:41.636Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"736.438413ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/\" range_end:\"/registry/persistentvolumes0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:55:41.636Z","caller":"traceutil/trace.go:171","msg":"trace[1333073656] range","detail":"{range_begin:/registry/persistentvolumes/; range_end:/registry/persistentvolumes0; response_count:0; response_revision:1112; }","duration":"736.577629ms","start":"2023-10-11T03:55:40.900Z","end":"2023-10-11T03:55:41.636Z","steps":["trace[1333073656] 'agreement among raft nodes before linearized reading'  (duration: 733.821335ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:41.636Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:40.900Z","time spent":"736.654316ms","remote":"127.0.0.1:34778","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":0,"response size":29,"request content":"key:\"/registry/persistentvolumes/\" range_end:\"/registry/persistentvolumes0\" count_only:true "}
{"level":"warn","ts":"2023-10-11T03:55:43.900Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.991601ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges/\" range_end:\"/registry/limitranges0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:55:43.900Z","caller":"traceutil/trace.go:171","msg":"trace[1078310772] range","detail":"{range_begin:/registry/limitranges/; range_end:/registry/limitranges0; response_count:0; response_revision:1113; }","duration":"108.109201ms","start":"2023-10-11T03:55:43.792Z","end":"2023-10-11T03:55:43.900Z","steps":["trace[1078310772] 'count revisions from in-memory index tree'  (duration: 107.883667ms)"],"step_count":1}
{"level":"info","ts":"2023-10-11T03:55:45.771Z","caller":"traceutil/trace.go:171","msg":"trace[601761205] linearizableReadLoop","detail":"{readStateIndex:1283; appliedIndex:1282; }","duration":"284.660878ms","start":"2023-10-11T03:55:45.487Z","end":"2023-10-11T03:55:45.771Z","steps":["trace[601761205] 'read index received'  (duration: 284.384319ms)","trace[601761205] 'applied index is now lower than readState.Index'  (duration: 275.32¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-10-11T03:55:45.771Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:45.389Z","time spent":"381.990918ms","remote":"127.0.0.1:34762","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2023-10-11T03:55:45.772Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"284.816449ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:55:45.772Z","caller":"traceutil/trace.go:171","msg":"trace[1468310509] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1113; }","duration":"284.901498ms","start":"2023-10-11T03:55:45.487Z","end":"2023-10-11T03:55:45.772Z","steps":["trace[1468310509] 'agreement among raft nodes before linearized reading'  (duration: 284.758ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:46.532Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128024383551025475,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-10-11T03:55:47.646Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"663.709374ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:55:47.646Z","caller":"traceutil/trace.go:171","msg":"trace[1621884143] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1113; }","duration":"663.801305ms","start":"2023-10-11T03:55:46.983Z","end":"2023-10-11T03:55:47.646Z","steps":["trace[1621884143] 'range keys from in-memory index tree'  (duration: 663.668588ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:47.647Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.642351714s","expected-duration":"100ms","prefix":"","request":"header:<ID:8128024383551025476 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1107 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128024383551025471 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2023-10-11T03:55:47.647Z","caller":"traceutil/trace.go:171","msg":"trace[891379632] linearizableReadLoop","detail":"{readStateIndex:1285; appliedIndex:1283; }","duration":"1.616309117s","start":"2023-10-11T03:55:46.031Z","end":"2023-10-11T03:55:47.647Z","steps":["trace[891379632] 'read index received'  (duration: 586.177852ms)","trace[891379632] 'applied index is now lower than readState.Index'  (duration: 1.030130546s)"],"step_count":2}
{"level":"info","ts":"2023-10-11T03:55:47.647Z","caller":"traceutil/trace.go:171","msg":"trace[267603758] transaction","detail":"{read_only:false; response_revision:1115; number_of_response:1; }","duration":"1.862590507s","start":"2023-10-11T03:55:45.785Z","end":"2023-10-11T03:55:47.647Z","steps":["trace[267603758] 'process raft request'  (duration: 1.862377154s)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:47.647Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:45.785Z","time spent":"1.862646775s","remote":"127.0.0.1:34786","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1113 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-10-11T03:55:47.647Z","caller":"traceutil/trace.go:171","msg":"trace[1882050779] transaction","detail":"{read_only:false; response_revision:1114; number_of_response:1; }","duration":"1.866325461s","start":"2023-10-11T03:55:45.781Z","end":"2023-10-11T03:55:47.647Z","steps":["trace[1882050779] 'process raft request'  (duration: 223.399733ms)","trace[1882050779] 'compare'  (duration: 1.641802795s)"],"step_count":2}
{"level":"warn","ts":"2023-10-11T03:55:47.647Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.616616572s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"warn","ts":"2023-10-11T03:55:47.647Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:45.781Z","time spent":"1.86639319s","remote":"127.0.0.1:34762","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1107 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128024383551025471 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"info","ts":"2023-10-11T03:55:47.647Z","caller":"traceutil/trace.go:171","msg":"trace[656705600] range","detail":"{range_begin:/registry/configmaps/; range_end:/registry/configmaps0; response_count:0; response_revision:1115; }","duration":"1.616657789s","start":"2023-10-11T03:55:46.031Z","end":"2023-10-11T03:55:47.647Z","steps":["trace[656705600] 'agreement among raft nodes before linearized reading'  (duration: 1.616552693s)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:47.648Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:46.031Z","time spent":"1.6167019s","remote":"127.0.0.1:34782","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":13,"response size":31,"request content":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" count_only:true "}
{"level":"warn","ts":"2023-10-11T03:55:47.648Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.418615621s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-10-11T03:55:47.648Z","caller":"traceutil/trace.go:171","msg":"trace[182003862] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:1115; }","duration":"1.41865636s","start":"2023-10-11T03:55:46.229Z","end":"2023-10-11T03:55:47.648Z","steps":["trace[182003862] 'agreement among raft nodes before linearized reading'  (duration: 1.418577169s)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:47.648Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:46.229Z","time spent":"1.418692801s","remote":"127.0.0.1:34860","response type":"/etcdserverpb.KV/Range","request count":0,"request size":82,"response count":8,"response size":31,"request content":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true "}
{"level":"warn","ts":"2023-10-11T03:55:47.648Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.160583253s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-11T03:55:47.648Z","caller":"traceutil/trace.go:171","msg":"trace[1501034040] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1115; }","duration":"1.160613461s","start":"2023-10-11T03:55:46.487Z","end":"2023-10-11T03:55:47.648Z","steps":["trace[1501034040] 'agreement among raft nodes before linearized reading'  (duration: 1.160508857s)"],"step_count":1}
{"level":"warn","ts":"2023-10-11T03:55:47.648Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-11T03:55:46.487Z","time spent":"1.160660774s","remote":"127.0.0.1:34820","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-10-11T03:55:57.591Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"175.981417ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" limit:500 ","response":"range_response_count:110 size:72714"}
{"level":"info","ts":"2023-10-11T03:55:57.591Z","caller":"traceutil/trace.go:171","msg":"trace[1982138798] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:110; response_revision:1122; }","duration":"176.182762ms","start":"2023-10-11T03:55:57.415Z","end":"2023-10-11T03:55:57.591Z","steps":["trace[1982138798] 'range keys from bolt db'  (duration: 175.661372ms)"],"step_count":1}

* 
* ==> kernel <==
*  03:55:58 up  4:25,  0 users,  load average: 5.79, 4.52, 3.40
Linux minikube 5.10.102.1-microsoft-standard-WSL2 #1 SMP Wed Mar 2 00:30:59 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [11ed290a628c] <==
* I1011 03:39:15.186130       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1011 03:39:15.186140       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I1011 03:39:15.186615       1 controller.go:85] Starting OpenAPI controller
I1011 03:39:15.186633       1 controller.go:121] Starting legacy_token_tracking_controller
I1011 03:39:15.186645       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I1011 03:39:15.293256       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I1011 03:39:15.375019       1 shared_informer.go:318] Caches are synced for crd-autoregister
I1011 03:39:15.375095       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1011 03:39:15.375181       1 aggregator.go:152] initial CRD sync complete...
I1011 03:39:15.375186       1 shared_informer.go:318] Caches are synced for configmaps
I1011 03:39:15.380984       1 shared_informer.go:318] Caches are synced for node_authorizer
I1011 03:39:15.382871       1 controller.go:624] quota admission added evaluator for: namespaces
I1011 03:39:15.384963       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1011 03:39:15.387317       1 apf_controller.go:366] Running API Priority and Fairness config worker
I1011 03:39:15.387345       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I1011 03:39:15.375196       1 autoregister_controller.go:141] Starting autoregister controller
I1011 03:39:15.400493       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1011 03:39:15.400645       1 cache.go:39] Caches are synced for autoregister controller
E1011 03:39:15.579646       1 controller.go:150] while syncing ConfigMap "kube-system/kube-apiserver-legacy-service-account-token-tracking", err: namespaces "kube-system" not found
I1011 03:39:15.585678       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
E1011 03:39:15.600886       1 controller.go:146] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I1011 03:39:15.882114       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I1011 03:39:16.199013       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1011 03:39:16.209379       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1011 03:39:16.209469       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1011 03:39:17.616252       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1011 03:39:17.705786       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1011 03:39:17.827828       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs=map[IPv4:10.96.0.1]
W1011 03:39:17.841830       1 lease.go:251] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1011 03:39:17.843424       1 controller.go:624] quota admission added evaluator for: endpoints
I1011 03:39:17.852688       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1011 03:39:18.508970       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I1011 03:39:19.489209       1 controller.go:624] quota admission added evaluator for: deployments.apps
I1011 03:39:19.513334       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs=map[IPv4:10.96.0.10]
I1011 03:39:19.584415       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I1011 03:39:32.254098       1 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I1011 03:39:32.267656       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I1011 03:39:37.840715       1 trace.go:219] Trace[1126180504]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:89eb6ff0-c041-464e-8322-eb7e42c2e896,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:POST (11-Oct-2023 03:39:37.321) (total time: 519ms):
Trace[1126180504]: ["Create etcd3" audit-id:89eb6ff0-c041-464e-8322-eb7e42c2e896,key:/events/kube-system/storage-provisioner.178cf07c8b897b92,type:*core.Event,resource:events 519ms (03:39:37.321)
Trace[1126180504]:  ---"Txn call succeeded" 518ms (03:39:37.840)]
Trace[1126180504]: [519.658509ms] [519.658509ms] END
I1011 03:40:16.212981       1 trace.go:219] Trace[1530360893]: "Create" accept:application/json,audit-id:4086a24e-2778-4e4e-a102-436a5669abe5,client:192.168.49.1,protocol:HTTP/2.0,resource:secrets,scope:resource,url:/api/v1/namespaces/default/secrets,user-agent:kubectl.exe/v1.27.2 (windows/amd64) kubernetes/7f6f68f,verb:POST (11-Oct-2023 03:40:14.277) (total time: 1934ms):
Trace[1530360893]: ["Create etcd3" audit-id:4086a24e-2778-4e4e-a102-436a5669abe5,key:/secrets/default/mongo-secrets,type:*core.Secret,resource:secrets 1931ms (03:40:14.281)
Trace[1530360893]:  ---"Txn call succeeded" 1930ms (03:40:16.212)]
Trace[1530360893]: [1.93494283s] [1.93494283s] END
I1011 03:40:16.214775       1 trace.go:219] Trace[1813568731]: "Get" accept:application/json, */*,audit-id:3ae54fd9-f85f-4921-8c52-64de90a0151b,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (11-Oct-2023 03:40:14.392) (total time: 1822ms):
Trace[1813568731]: ---"About to write a response" 1822ms (03:40:16.214)
Trace[1813568731]: [1.822147801s] [1.822147801s] END
I1011 03:40:27.212474       1 trace.go:219] Trace[1874156642]: "Update" accept:application/json, */*,audit-id:a97b856e-bd1b-4864-ab57-f06a0bbd6d55,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-Oct-2023 03:40:26.536) (total time: 676ms):
Trace[1874156642]: ["GuaranteedUpdate etcd3" audit-id:a97b856e-bd1b-4864-ab57-f06a0bbd6d55,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 676ms (03:40:26.536)
Trace[1874156642]:  ---"Txn call completed" 675ms (03:40:27.212)]
Trace[1874156642]: [676.303834ms] [676.303834ms] END
I1011 03:40:28.739226       1 trace.go:219] Trace[1490610672]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (11-Oct-2023 03:40:28.180) (total time: 558ms):
Trace[1490610672]: ---"Transaction prepared" 534ms (03:40:28.716)
Trace[1490610672]: [558.411019ms] [558.411019ms] END
I1011 03:40:34.412609       1 alloc.go:330] "allocated clusterIPs" service="default/mongodb-service" clusterIPs=map[IPv4:10.110.217.133]
I1011 03:40:50.145261       1 trace.go:219] Trace[521757724]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:1b95becb-5dda-4d40-b069-aa0a26ebd031,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (11-Oct-2023 03:40:48.794) (total time: 1350ms):
Trace[521757724]: ["GuaranteedUpdate etcd3" audit-id:1b95becb-5dda-4d40-b069-aa0a26ebd031,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 1350ms (03:40:48.794)
Trace[521757724]:  ---"Txn call completed" 1348ms (03:40:50.144)]
Trace[521757724]: [1.350873333s] [1.350873333s] END

* 
* ==> kube-apiserver [88545e8c1972] <==
* Trace[561237135]: ---"Transaction prepared" 570ms (03:50:35.881)
Trace[561237135]: ---"Txn call completed" 1800ms (03:50:37.682)
Trace[561237135]: [2.371865549s] [2.371865549s] END
I1011 03:50:37.695587       1 trace.go:219] Trace[199158052]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:88d20696-46f6-4508-9e9b-c5d6305670dc,client:192.168.49.2,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/kube-system/serviceaccounts/node-controller/token,user-agent:kube-controller-manager/v1.27.4 (linux/amd64) kubernetes/fa3d799/kube-controller-manager,verb:POST (11-Oct-2023 03:50:35.884) (total time: 1810ms):
Trace[199158052]: ---"Write to database call succeeded" len:81 1810ms (03:50:37.695)
Trace[199158052]: [1.810771986s] [1.810771986s] END
I1011 03:50:38.700941       1 trace.go:219] Trace[498578416]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ea8e2763-dd23-46ab-a1c3-aa0a28805494,client:192.168.49.2,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/kube-system/serviceaccounts/persistent-volume-binder,user-agent:kube-controller-manager/v1.27.4 (linux/amd64) kubernetes/fa3d799/kube-controller-manager,verb:GET (11-Oct-2023 03:50:37.752) (total time: 948ms):
Trace[498578416]: ---"About to write a response" 947ms (03:50:38.700)
Trace[498578416]: [948.01254ms] [948.01254ms] END
I1011 03:50:38.740454       1 controller.go:624] quota admission added evaluator for: endpoints
I1011 03:50:39.583167       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1011 03:52:52.367618       1 alloc.go:330] "allocated clusterIPs" service="default/backend-service" clusterIPs=map[IPv4:10.105.21.164]
I1011 03:53:21.460782       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I1011 03:54:16.072887       1 trace.go:219] Trace[792348780]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5715a2df-460c-4c3c-8330-0d7a7302e612,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:resource,url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:GET (11-Oct-2023 03:54:15.314) (total time: 724ms):
Trace[792348780]: ---"About to write a response" 724ms (03:54:16.039)
Trace[792348780]: [724.996449ms] [724.996449ms] END
I1011 03:54:22.373563       1 trace.go:219] Trace[2093169939]: "Update" accept:application/json, */*,audit-id:5177d98a-1420-4983-b082-a00c91d0fe00,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-Oct-2023 03:54:18.896) (total time: 3476ms):
Trace[2093169939]: ["GuaranteedUpdate etcd3" audit-id:5177d98a-1420-4983-b082-a00c91d0fe00,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 3472ms (03:54:18.901)
Trace[2093169939]:  ---"Txn call completed" 3457ms (03:54:22.373)]
Trace[2093169939]: [3.476875159s] [3.476875159s] END
I1011 03:54:26.686485       1 trace.go:219] Trace[424349289]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:6ea4aa71-7b65-4d7b-877b-7e7828ffa344,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:resource,url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:GET (11-Oct-2023 03:54:25.315) (total time: 1371ms):
Trace[424349289]: ---"About to write a response" 1371ms (03:54:26.686)
Trace[424349289]: [1.371430185s] [1.371430185s] END
I1011 03:54:26.686981       1 trace.go:219] Trace[1894937551]: "Get" accept:application/json, */*,audit-id:27919a76-cfb1-46d7-874a-4ad7d54fa464,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (11-Oct-2023 03:54:24.378) (total time: 2308ms):
Trace[1894937551]: ---"About to write a response" 2308ms (03:54:26.686)
Trace[1894937551]: [2.308747621s] [2.308747621s] END
I1011 03:54:26.707652       1 trace.go:219] Trace[405523685]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a6f619f5-6c6d-4208-a59c-d2be1db01bc1,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:POST (11-Oct-2023 03:54:23.397) (total time: 3309ms):
Trace[405523685]: ["Create etcd3" audit-id:a6f619f5-6c6d-4208-a59c-d2be1db01bc1,key:/events/kube-system/kube-apiserver-minikube.178cf14ad9c9d258,type:*core.Event,resource:events 3308ms (03:54:23.398)
Trace[405523685]:  ---"TransformToStorage succeeded" 3287ms (03:54:26.686)]
Trace[405523685]: [3.309824645s] [3.309824645s] END
I1011 03:54:45.429897       1 trace.go:219] Trace[1137633365]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:701899ef-61cf-4679-a54a-dfb33772e34e,client:192.168.49.2,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/default/deployments/mongodb/status,user-agent:kube-controller-manager/v1.27.4 (linux/amd64) kubernetes/fa3d799/system:serviceaccount:kube-system:deployment-controller,verb:PUT (11-Oct-2023 03:54:40.712) (total time: 4716ms):
Trace[1137633365]: ["GuaranteedUpdate etcd3" audit-id:701899ef-61cf-4679-a54a-dfb33772e34e,key:/deployments/default/mongodb,type:*apps.Deployment,resource:deployments.apps 4716ms (03:54:40.713)
Trace[1137633365]:  ---"Txn call completed" 4711ms (03:54:45.429)]
Trace[1137633365]: [4.716841015s] [4.716841015s] END
I1011 03:54:45.439004       1 trace.go:219] Trace[225193202]: "Get" accept:application/json, */*,audit-id:73ce86bb-a210-403a-90e8-a2f5e7c5fd53,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (11-Oct-2023 03:54:41.421) (total time: 4017ms):
Trace[225193202]: ---"About to write a response" 4017ms (03:54:45.438)
Trace[225193202]: [4.017667089s] [4.017667089s] END
I1011 03:55:21.254151       1 trace.go:219] Trace[856847542]: "Update" accept:application/json, */*,audit-id:77e7feae-6497-4aee-8452-107ec53ba363,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-Oct-2023 03:55:20.296) (total time: 957ms):
Trace[856847542]: ["GuaranteedUpdate etcd3" audit-id:77e7feae-6497-4aee-8452-107ec53ba363,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 957ms (03:55:20.297)
Trace[856847542]:  ---"Txn call completed" 955ms (03:55:21.253)]
Trace[856847542]: [957.364487ms] [957.364487ms] END
I1011 03:55:36.346519       1 trace.go:219] Trace[1425959730]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:89c9b6a1-3b1d-442b-8374-ae33035eb543,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:GET (11-Oct-2023 03:55:35.826) (total time: 519ms):
Trace[1425959730]: ---"About to write a response" 519ms (03:55:36.346)
Trace[1425959730]: [519.880815ms] [519.880815ms] END
I1011 03:55:36.346546       1 trace.go:219] Trace[531086842]: "Update" accept:application/json, */*,audit-id:144b3f55-e3bc-4ed3-bb4d-9e097729bd1e,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-Oct-2023 03:55:35.691) (total time: 655ms):
Trace[531086842]: ["GuaranteedUpdate etcd3" audit-id:144b3f55-e3bc-4ed3-bb4d-9e097729bd1e,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 655ms (03:55:35.691)
Trace[531086842]:  ---"Txn call completed" 654ms (03:55:36.346)]
Trace[531086842]: [655.425489ms] [655.425489ms] END
I1011 03:55:41.635508       1 trace.go:219] Trace[2009947073]: "Update" accept:application/json, */*,audit-id:62c7b2e1-78cb-4d71-ab2d-dce428517dc5,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-Oct-2023 03:55:40.825) (total time: 810ms):
Trace[2009947073]: ["GuaranteedUpdate etcd3" audit-id:62c7b2e1-78cb-4d71-ab2d-dce428517dc5,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 809ms (03:55:40.825)
Trace[2009947073]:  ---"Txn call completed" 808ms (03:55:41.635)]
Trace[2009947073]: [810.070685ms] [810.070685ms] END
I1011 03:55:47.648623       1 trace.go:219] Trace[446965195]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (11-Oct-2023 03:55:45.388) (total time: 2260ms):
Trace[446965195]: ---"Transaction prepared" 383ms (03:55:45.772)
Trace[446965195]: ---"Txn call completed" 1875ms (03:55:47.648)
Trace[446965195]: [2.260338286s] [2.260338286s] END
I1011 03:55:47.648851       1 trace.go:219] Trace[1332154759]: "Update" accept:application/json, */*,audit-id:01314d39-59ba-489c-a33e-0ff8767b8773,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-Oct-2023 03:55:45.781) (total time: 1867ms):
Trace[1332154759]: ["GuaranteedUpdate etcd3" audit-id:01314d39-59ba-489c-a33e-0ff8767b8773,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1867ms (03:55:45.781)
Trace[1332154759]:  ---"Txn call completed" 1864ms (03:55:47.648)]
Trace[1332154759]: [1.867701367s] [1.867701367s] END

* 
* ==> kube-controller-manager [528bef7d2777] <==
* I1011 03:39:31.234277       1 controllermanager.go:638] "Started controller" controller="clusterrole-aggregation"
I1011 03:39:31.234334       1 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I1011 03:39:31.234367       1 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
I1011 03:39:31.247847       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I1011 03:39:31.253796       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I1011 03:39:31.262258       1 shared_informer.go:318] Caches are synced for namespace
I1011 03:39:31.262577       1 shared_informer.go:318] Caches are synced for service account
I1011 03:39:31.262972       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1011 03:39:31.265078       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I1011 03:39:31.288542       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1011 03:39:31.346331       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I1011 03:39:31.351865       1 shared_informer.go:318] Caches are synced for cronjob
I1011 03:39:31.351911       1 shared_informer.go:318] Caches are synced for job
I1011 03:39:31.351958       1 shared_informer.go:318] Caches are synced for ephemeral
I1011 03:39:31.351983       1 shared_informer.go:318] Caches are synced for PV protection
I1011 03:39:31.352001       1 shared_informer.go:318] Caches are synced for ReplicationController
I1011 03:39:31.354216       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I1011 03:39:31.355794       1 shared_informer.go:318] Caches are synced for node
I1011 03:39:31.355864       1 range_allocator.go:174] "Sending events to api server"
I1011 03:39:31.355900       1 range_allocator.go:178] "Starting range CIDR allocator"
I1011 03:39:31.355907       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1011 03:39:31.355916       1 shared_informer.go:318] Caches are synced for cidrallocator
I1011 03:39:31.357877       1 shared_informer.go:318] Caches are synced for persistent volume
I1011 03:39:31.445327       1 shared_informer.go:318] Caches are synced for TTL after finished
I1011 03:39:31.445398       1 shared_informer.go:318] Caches are synced for taint
I1011 03:39:31.445441       1 shared_informer.go:318] Caches are synced for daemon sets
I1011 03:39:31.445611       1 shared_informer.go:318] Caches are synced for TTL
I1011 03:39:31.445799       1 shared_informer.go:318] Caches are synced for attach detach
I1011 03:39:31.445343       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1011 03:39:31.445975       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1011 03:39:31.446095       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I1011 03:39:31.446382       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1011 03:39:31.446517       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I1011 03:39:31.447212       1 shared_informer.go:318] Caches are synced for GC
I1011 03:39:31.447360       1 shared_informer.go:318] Caches are synced for PVC protection
I1011 03:39:31.447443       1 shared_informer.go:318] Caches are synced for crt configmap
I1011 03:39:31.449934       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1011 03:39:31.450047       1 taint_manager.go:211] "Sending events to api server"
I1011 03:39:31.450454       1 shared_informer.go:318] Caches are synced for endpoint
I1011 03:39:31.455471       1 shared_informer.go:318] Caches are synced for stateful set
I1011 03:39:31.455651       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1011 03:39:31.456170       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1011 03:39:31.456333       1 shared_informer.go:318] Caches are synced for HPA
I1011 03:39:31.456542       1 shared_informer.go:318] Caches are synced for expand
I1011 03:39:31.460691       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I1011 03:39:31.462448       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1011 03:39:31.472698       1 shared_informer.go:318] Caches are synced for resource quota
I1011 03:39:31.482405       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1011 03:39:31.545427       1 shared_informer.go:318] Caches are synced for deployment
I1011 03:39:31.545549       1 shared_informer.go:318] Caches are synced for disruption
I1011 03:39:31.548070       1 shared_informer.go:318] Caches are synced for resource quota
I1011 03:39:31.568843       1 range_allocator.go:380] "Set node PodCIDR" node="minikube" podCIDRs=[10.244.0.0/24]
I1011 03:39:31.871825       1 shared_informer.go:318] Caches are synced for garbage collector
I1011 03:39:31.945578       1 shared_informer.go:318] Caches are synced for garbage collector
I1011 03:39:31.945617       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1011 03:39:32.285797       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5d78c9869d to 1"
I1011 03:39:32.291188       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-mkmvg"
I1011 03:39:32.453297       1 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5d78c9869d-rgt2n"
I1011 03:39:58.209694       1 event.go:307] "Event occurred" object="default/mongodb" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mongodb-7776888f9 to 1"
I1011 03:39:58.280663       1 event.go:307] "Event occurred" object="default/mongodb-7776888f9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mongodb-7776888f9-mff54"

* 
* ==> kube-controller-manager [7fa051f66a96] <==
* I1011 03:50:39.315602       1 controllermanager.go:638] "Started controller" controller="root-ca-cert-publisher"
I1011 03:50:39.315841       1 publisher.go:101] Starting root CA certificate configmap publisher
I1011 03:50:39.315884       1 shared_informer.go:311] Waiting for caches to sync for crt configmap
I1011 03:50:39.321592       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I1011 03:50:39.337563       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1011 03:50:39.339477       1 shared_informer.go:318] Caches are synced for TTL
I1011 03:50:39.342377       1 shared_informer.go:318] Caches are synced for node
I1011 03:50:39.343367       1 shared_informer.go:318] Caches are synced for expand
I1011 03:50:39.343616       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1011 03:50:39.379617       1 shared_informer.go:318] Caches are synced for PV protection
I1011 03:50:39.379877       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1011 03:50:39.379897       1 range_allocator.go:174] "Sending events to api server"
I1011 03:50:39.380255       1 range_allocator.go:178] "Starting range CIDR allocator"
I1011 03:50:39.380274       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1011 03:50:39.380286       1 shared_informer.go:318] Caches are synced for cidrallocator
I1011 03:50:39.382622       1 shared_informer.go:318] Caches are synced for stateful set
I1011 03:50:39.383343       1 shared_informer.go:318] Caches are synced for taint
I1011 03:50:39.383688       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1011 03:50:39.383911       1 taint_manager.go:211] "Sending events to api server"
I1011 03:50:39.384138       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1011 03:50:39.384449       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I1011 03:50:39.385997       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1011 03:50:39.386152       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I1011 03:50:39.391037       1 shared_informer.go:318] Caches are synced for namespace
I1011 03:50:39.398725       1 shared_informer.go:318] Caches are synced for cronjob
I1011 03:50:39.399569       1 shared_informer.go:318] Caches are synced for PVC protection
I1011 03:50:39.400238       1 shared_informer.go:318] Caches are synced for attach detach
I1011 03:50:39.401384       1 shared_informer.go:318] Caches are synced for service account
I1011 03:50:39.401932       1 shared_informer.go:318] Caches are synced for ephemeral
I1011 03:50:39.403860       1 shared_informer.go:318] Caches are synced for TTL after finished
I1011 03:50:39.404408       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I1011 03:50:39.416173       1 shared_informer.go:318] Caches are synced for job
I1011 03:50:39.416226       1 shared_informer.go:318] Caches are synced for crt configmap
I1011 03:50:39.420739       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I1011 03:50:39.420794       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I1011 03:50:39.421812       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1011 03:50:39.421991       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I1011 03:50:39.451955       1 shared_informer.go:318] Caches are synced for ReplicationController
I1011 03:50:39.479747       1 shared_informer.go:318] Caches are synced for GC
I1011 03:50:39.479822       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I1011 03:50:39.479747       1 shared_informer.go:318] Caches are synced for daemon sets
I1011 03:50:39.479873       1 shared_informer.go:318] Caches are synced for persistent volume
I1011 03:50:39.479759       1 shared_informer.go:318] Caches are synced for endpoint
I1011 03:50:39.480191       1 shared_informer.go:318] Caches are synced for HPA
I1011 03:50:39.496069       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1011 03:50:39.509225       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1011 03:50:39.585044       1 shared_informer.go:318] Caches are synced for resource quota
I1011 03:50:39.611508       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1011 03:50:39.621951       1 shared_informer.go:318] Caches are synced for resource quota
I1011 03:50:39.637073       1 shared_informer.go:318] Caches are synced for disruption
I1011 03:50:39.679624       1 shared_informer.go:318] Caches are synced for deployment
I1011 03:50:39.994240       1 shared_informer.go:318] Caches are synced for garbage collector
I1011 03:50:39.994322       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1011 03:50:40.004660       1 shared_informer.go:318] Caches are synced for garbage collector
I1011 03:53:21.470517       1 event.go:307] "Event occurred" object="default/backend-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set backend-deployment-6d4d68f5b5 to 1"
I1011 03:53:21.514760       1 event.go:307] "Event occurred" object="default/backend-deployment-6d4d68f5b5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-deployment-6d4d68f5b5-s6szv"
I1011 03:54:39.432014       1 event.go:307] "Event occurred" object="default/backend-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set backend-deployment-6fcbc7759c to 1"
I1011 03:54:39.724574       1 event.go:307] "Event occurred" object="default/backend-deployment-6fcbc7759c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-deployment-6fcbc7759c-8hrhp"
I1011 03:54:39.908751       1 event.go:307] "Event occurred" object="default/mongodb" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mongodb-565b4ccdb4 to 1"
I1011 03:54:39.918263       1 event.go:307] "Event occurred" object="default/mongodb-565b4ccdb4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mongodb-565b4ccdb4-v22kk"

* 
* ==> kube-proxy [41f5666077bc] <==
* I1011 03:50:27.486584       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1011 03:50:27.486716       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1011 03:50:27.487245       1 server_others.go:554] "Using iptables proxy"
I1011 03:50:27.809158       1 server_others.go:192] "Using iptables Proxier"
I1011 03:50:27.809277       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1011 03:50:27.809290       1 server_others.go:200] "Creating dualStackProxier for iptables"
I1011 03:50:27.809341       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1011 03:50:27.815088       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1011 03:50:27.880608       1 server.go:658] "Version info" version="v1.27.4"
I1011 03:50:27.880668       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1011 03:50:27.890911       1 config.go:188] "Starting service config controller"
I1011 03:50:27.891410       1 config.go:315] "Starting node config controller"
I1011 03:50:27.895554       1 shared_informer.go:311] Waiting for caches to sync for service config
I1011 03:50:27.895795       1 shared_informer.go:311] Waiting for caches to sync for node config
I1011 03:50:27.891803       1 config.go:97] "Starting endpoint slice config controller"
I1011 03:50:27.896899       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1011 03:50:27.997120       1 shared_informer.go:318] Caches are synced for service config
I1011 03:50:27.997258       1 shared_informer.go:318] Caches are synced for node config
I1011 03:50:27.997298       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-proxy [4afca4381570] <==
* I1011 03:39:35.454935       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1011 03:39:35.455070       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1011 03:39:35.455221       1 server_others.go:554] "Using iptables proxy"
I1011 03:39:35.571272       1 server_others.go:192] "Using iptables Proxier"
I1011 03:39:35.571380       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1011 03:39:35.571399       1 server_others.go:200] "Creating dualStackProxier for iptables"
I1011 03:39:35.571420       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1011 03:39:35.571452       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1011 03:39:35.572663       1 server.go:658] "Version info" version="v1.27.4"
I1011 03:39:35.572731       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1011 03:39:35.573561       1 config.go:188] "Starting service config controller"
I1011 03:39:35.573626       1 shared_informer.go:311] Waiting for caches to sync for service config
I1011 03:39:35.573693       1 config.go:315] "Starting node config controller"
I1011 03:39:35.573744       1 shared_informer.go:311] Waiting for caches to sync for node config
I1011 03:39:35.574133       1 config.go:97] "Starting endpoint slice config controller"
I1011 03:39:35.574184       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1011 03:39:35.674235       1 shared_informer.go:318] Caches are synced for node config
I1011 03:39:35.674271       1 shared_informer.go:318] Caches are synced for service config
I1011 03:39:35.674370       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [07c7a87e577f] <==
* E1011 03:39:15.603442       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1011 03:39:15.603518       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1011 03:39:15.603539       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1011 03:39:15.603546       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1011 03:39:15.603558       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1011 03:39:15.603639       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1011 03:39:15.603662       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1011 03:39:15.603688       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1011 03:39:15.603710       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1011 03:39:15.603727       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1011 03:39:15.603749       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1011 03:39:15.603821       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1011 03:39:15.603871       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1011 03:39:15.603947       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1011 03:39:15.603967       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1011 03:39:15.604031       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1011 03:39:15.604050       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1011 03:39:15.604380       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1011 03:39:15.604406       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1011 03:39:15.605952       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1011 03:39:15.606006       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1011 03:39:15.675178       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1011 03:39:15.675219       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1011 03:39:15.676503       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1011 03:39:15.676540       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1011 03:39:15.677876       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1011 03:39:15.677945       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1011 03:39:15.678268       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1011 03:39:15.678292       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1011 03:39:16.577874       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1011 03:39:16.577958       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1011 03:39:16.578148       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1011 03:39:16.578223       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1011 03:39:16.623557       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1011 03:39:16.623650       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1011 03:39:16.677014       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1011 03:39:16.677102       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1011 03:39:16.677214       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1011 03:39:16.677236       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1011 03:39:16.734487       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1011 03:39:16.734567       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1011 03:39:16.842917       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1011 03:39:16.842987       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1011 03:39:16.876730       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1011 03:39:16.876835       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1011 03:39:16.892720       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1011 03:39:16.892799       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1011 03:39:16.977089       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1011 03:39:16.977166       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1011 03:39:16.999998       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1011 03:39:17.002893       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1011 03:39:17.020593       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1011 03:39:17.020667       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1011 03:39:17.093947       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1011 03:39:17.094022       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1011 03:39:17.097120       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1011 03:39:17.097198       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1011 03:39:17.176471       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1011 03:39:17.176542       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I1011 03:39:19.495106       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [180b4f8bcef7] <==
* I1011 03:50:15.502178       1 serving.go:348] Generated self-signed cert in-memory
W1011 03:50:21.584019       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1011 03:50:21.584174       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1011 03:50:21.584203       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1011 03:50:21.584217       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1011 03:50:21.799188       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I1011 03:50:21.799644       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1011 03:50:21.895250       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1011 03:50:21.895591       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1011 03:50:21.896229       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1011 03:50:21.897400       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1011 03:50:22.091877       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Oct 11 03:50:14 minikube kubelet[1625]: E1011 03:50:14.582119    1625 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="3.2s"
Oct 11 03:50:14 minikube kubelet[1625]: W1011 03:50:14.779299    1625 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Oct 11 03:50:14 minikube kubelet[1625]: E1011 03:50:14.779368    1625 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Oct 11 03:50:14 minikube kubelet[1625]: W1011 03:50:14.878698    1625 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Oct 11 03:50:14 minikube kubelet[1625]: E1011 03:50:14.878966    1625 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Oct 11 03:50:14 minikube kubelet[1625]: I1011 03:50:14.882098    1625 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Oct 11 03:50:14 minikube kubelet[1625]: E1011 03:50:14.883967    1625 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Oct 11 03:50:14 minikube kubelet[1625]: W1011 03:50:14.978626    1625 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Oct 11 03:50:14 minikube kubelet[1625]: E1011 03:50:14.978766    1625 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Oct 11 03:50:18 minikube kubelet[1625]: I1011 03:50:18.145233    1625 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.579348    1625 apiserver.go:52] "Watching apiserver"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.605657    1625 topology_manager.go:212] "Topology Admit Handler"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.715979    1625 topology_manager.go:212] "Topology Admit Handler"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.718009    1625 topology_manager.go:212] "Topology Admit Handler"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.719719    1625 topology_manager.go:212] "Topology Admit Handler"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.804935    1625 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/f509436d-32fe-4c91-8f5a-58fab04f4055-tmp\") pod \"storage-provisioner\" (UID: \"f509436d-32fe-4c91-8f5a-58fab04f4055\") " pod="kube-system/storage-provisioner"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.805104    1625 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/709ec71d-6020-49b1-82e7-be053892893a-lib-modules\") pod \"kube-proxy-mkmvg\" (UID: \"709ec71d-6020-49b1-82e7-be053892893a\") " pod="kube-system/kube-proxy-mkmvg"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.805155    1625 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/709ec71d-6020-49b1-82e7-be053892893a-kube-proxy\") pod \"kube-proxy-mkmvg\" (UID: \"709ec71d-6020-49b1-82e7-be053892893a\") " pod="kube-system/kube-proxy-mkmvg"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.805196    1625 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bsw5c\" (UniqueName: \"kubernetes.io/projected/709ec71d-6020-49b1-82e7-be053892893a-kube-api-access-bsw5c\") pod \"kube-proxy-mkmvg\" (UID: \"709ec71d-6020-49b1-82e7-be053892893a\") " pod="kube-system/kube-proxy-mkmvg"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.805237    1625 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rv4n4\" (UniqueName: \"kubernetes.io/projected/f509436d-32fe-4c91-8f5a-58fab04f4055-kube-api-access-rv4n4\") pod \"storage-provisioner\" (UID: \"f509436d-32fe-4c91-8f5a-58fab04f4055\") " pod="kube-system/storage-provisioner"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.807092    1625 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/709ec71d-6020-49b1-82e7-be053892893a-xtables-lock\") pod \"kube-proxy-mkmvg\" (UID: \"709ec71d-6020-49b1-82e7-be053892893a\") " pod="kube-system/kube-proxy-mkmvg"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.810208    1625 kubelet_node_status.go:108] "Node was previously registered" node="minikube"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.811432    1625 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.845347    1625 desired_state_of_world_populator.go:153] "Finished populating initial desired state of world"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.884004    1625 kuberuntime_manager.go:1460] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.885118    1625 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.978678    1625 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/8e77c0eb-fb1f-465d-b114-962e7a21dcf5-config-volume\") pod \"coredns-5d78c9869d-rgt2n\" (UID: \"8e77c0eb-fb1f-465d-b114-962e7a21dcf5\") " pod="kube-system/coredns-5d78c9869d-rgt2n"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.979101    1625 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tk8sv\" (UniqueName: \"kubernetes.io/projected/8e77c0eb-fb1f-465d-b114-962e7a21dcf5-kube-api-access-tk8sv\") pod \"coredns-5d78c9869d-rgt2n\" (UID: \"8e77c0eb-fb1f-465d-b114-962e7a21dcf5\") " pod="kube-system/coredns-5d78c9869d-rgt2n"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.979410    1625 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qk4kw\" (UniqueName: \"kubernetes.io/projected/a23ea0a7-cb38-4c86-a54f-b2074c2f0c9a-kube-api-access-qk4kw\") pod \"mongodb-7776888f9-mff54\" (UID: \"a23ea0a7-cb38-4c86-a54f-b2074c2f0c9a\") " pod="default/mongodb-7776888f9-mff54"
Oct 11 03:50:21 minikube kubelet[1625]: I1011 03:50:21.979658    1625 reconciler.go:41] "Reconciler: start to sync state"
Oct 11 03:50:24 minikube kubelet[1625]: E1011 03:50:24.404550    1625 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Oct 11 03:50:24 minikube kubelet[1625]: E1011 03:50:24.405374    1625 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Oct 11 03:50:24 minikube kubelet[1625]: I1011 03:50:24.985773    1625 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8f64f577b8389f8356355e225c9a4a197326cd06bd3d61f7274f216ef14bbf52"
Oct 11 03:50:25 minikube kubelet[1625]: I1011 03:50:25.012800    1625 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1d07e65b8fdeacb9c71c725d0af847f3dd23cb3863f4ecdda33d3ae293828c24"
Oct 11 03:50:26 minikube kubelet[1625]: I1011 03:50:26.408798    1625 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="eb29107dbf034c0bd602f30b96d5a7a6fba2447e265bb59ce7df45171b0d5f1d"
Oct 11 03:50:26 minikube kubelet[1625]: I1011 03:50:26.644183    1625 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="445d9ed09ccdd31705b7ed6f8d26766c36460207d5347dac7529b5423ddea94a"
Oct 11 03:50:27 minikube kubelet[1625]: I1011 03:50:27.990756    1625 scope.go:115] "RemoveContainer" containerID="b6fe8ea8d5682e241dd34828cd10c93ab043be70f6f73fcc5baee4ab14505477"
Oct 11 03:50:27 minikube kubelet[1625]: I1011 03:50:27.992385    1625 scope.go:115] "RemoveContainer" containerID="62a15ab473415f8fe425afc137a5d0887b4e24cd9fa64bd6147eee781737d46a"
Oct 11 03:50:27 minikube kubelet[1625]: E1011 03:50:27.992836    1625 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f509436d-32fe-4c91-8f5a-58fab04f4055)\"" pod="kube-system/storage-provisioner" podUID=f509436d-32fe-4c91-8f5a-58fab04f4055
Oct 11 03:50:29 minikube kubelet[1625]: I1011 03:50:29.147175    1625 scope.go:115] "RemoveContainer" containerID="62a15ab473415f8fe425afc137a5d0887b4e24cd9fa64bd6147eee781737d46a"
Oct 11 03:50:29 minikube kubelet[1625]: E1011 03:50:29.147425    1625 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f509436d-32fe-4c91-8f5a-58fab04f4055)\"" pod="kube-system/storage-provisioner" podUID=f509436d-32fe-4c91-8f5a-58fab04f4055
Oct 11 03:50:36 minikube kubelet[1625]: E1011 03:50:36.904279    1625 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Oct 11 03:50:36 minikube kubelet[1625]: E1011 03:50:36.904347    1625 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Oct 11 03:50:43 minikube kubelet[1625]: I1011 03:50:43.612588    1625 scope.go:115] "RemoveContainer" containerID="62a15ab473415f8fe425afc137a5d0887b4e24cd9fa64bd6147eee781737d46a"
Oct 11 03:50:48 minikube kubelet[1625]: E1011 03:50:48.416373    1625 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Oct 11 03:50:48 minikube kubelet[1625]: E1011 03:50:48.416473    1625 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Oct 11 03:50:59 minikube kubelet[1625]: E1011 03:50:59.515284    1625 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Oct 11 03:50:59 minikube kubelet[1625]: E1011 03:50:59.515371    1625 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Oct 11 03:51:10 minikube kubelet[1625]: E1011 03:51:10.787206    1625 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Oct 11 03:51:10 minikube kubelet[1625]: E1011 03:51:10.787302    1625 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Oct 11 03:53:21 minikube kubelet[1625]: I1011 03:53:21.745529    1625 topology_manager.go:212] "Topology Admit Handler"
Oct 11 03:53:21 minikube kubelet[1625]: I1011 03:53:21.894946    1625 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-l7jpr\" (UniqueName: \"kubernetes.io/projected/eb4a4b30-b697-404c-8f84-979b498c45e9-kube-api-access-l7jpr\") pod \"backend-deployment-6d4d68f5b5-s6szv\" (UID: \"eb4a4b30-b697-404c-8f84-979b498c45e9\") " pod="default/backend-deployment-6d4d68f5b5-s6szv"
Oct 11 03:53:22 minikube kubelet[1625]: I1011 03:53:22.890789    1625 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3349947db51f2a6c240a8998f9c352196bc147fea0727ce5ea736458eb49f88c"
Oct 11 03:54:39 minikube kubelet[1625]: I1011 03:54:39.798633    1625 topology_manager.go:212] "Topology Admit Handler"
Oct 11 03:54:39 minikube kubelet[1625]: I1011 03:54:39.995393    1625 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-z5wl9\" (UniqueName: \"kubernetes.io/projected/c69cc843-fd8a-47d3-817a-3ea726193212-kube-api-access-z5wl9\") pod \"backend-deployment-6fcbc7759c-8hrhp\" (UID: \"c69cc843-fd8a-47d3-817a-3ea726193212\") " pod="default/backend-deployment-6fcbc7759c-8hrhp"
Oct 11 03:54:40 minikube kubelet[1625]: I1011 03:54:40.191293    1625 topology_manager.go:212] "Topology Admit Handler"
Oct 11 03:54:40 minikube kubelet[1625]: I1011 03:54:40.304451    1625 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kq8ls\" (UniqueName: \"kubernetes.io/projected/737f2372-40f4-4c83-ad58-92682e0181ae-kube-api-access-kq8ls\") pod \"mongodb-565b4ccdb4-v22kk\" (UID: \"737f2372-40f4-4c83-ad58-92682e0181ae\") " pod="default/mongodb-565b4ccdb4-v22kk"
Oct 11 03:54:49 minikube kubelet[1625]: I1011 03:54:49.826874    1625 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2457fac334334c3694b9d5c2dd0e0f09a570ae4b73b87e87dfb2299ea455abb5"
Oct 11 03:54:49 minikube kubelet[1625]: I1011 03:54:49.892120    1625 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="9bd042075e5b1db407e83086a3bad895fff1960997cbf2e1797d9748e8bfdc87"
Oct 11 03:55:11 minikube kubelet[1625]: W1011 03:55:11.708370    1625 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> storage-provisioner [62a15ab47341] <==
* I1011 03:50:26.785866       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1011 03:50:26.917327       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

* 
* ==> storage-provisioner [f5aff4c12057] <==
* I1011 03:50:44.500155       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1011 03:50:44.713635       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1011 03:50:44.716820       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1011 03:51:02.153135       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1011 03:51:02.154814       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_b65d0a49-3cf9-4a36-85b3-8aa0d8b98b7a!
I1011 03:51:02.154835       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"56cfe8d1-31fc-48bf-8b07-24eac1dec7ed", APIVersion:"v1", ResourceVersion:"844", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_b65d0a49-3cf9-4a36-85b3-8aa0d8b98b7a became leader
I1011 03:51:02.258966       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_b65d0a49-3cf9-4a36-85b3-8aa0d8b98b7a!

